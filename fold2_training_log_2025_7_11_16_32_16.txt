
#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################
 
2025-07-11 16:32:19.739007: Using torch.compile... 
2025-07-11 16:32:20.916771: do_dummy_2d_data_aug: False 
2025-07-11 16:32:20.923434: Using splits from existing split file: /data/nnUNet_preprocessed/Dataset010_PTB_all_energies_1mm_no_background_alldata/splits_final.json 
2025-07-11 16:32:20.933754: The split file contains 5 splits. 
2025-07-11 16:32:20.934915: Desired fold for training: 1 
2025-07-11 16:32:20.936028: This split has 1440 training and 360 validation cases. 

This is the configuration used by this training:
Configuration name: 2d
 {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 35, 'patch_size': [512, 512], 'median_image_size_in_voxels': [504.0, 504.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.ResidualEncoderUNet', 'arch_kwargs': {'n_stages': 8, 'features_per_stage': [32, 64, 128, 256, 512, 512, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_blocks_per_stage': [1, 3, 4, 6, 6, 6, 6, 6], 'n_conv_per_stage_decoder': [1, 1, 1, 1, 1, 1, 1], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} 
 
These are the global plan.json settings:
 {'dataset_name': 'Dataset010_PTB_all_energies_1mm_no_background_alldata', 'plans_name': 'nnUNetResEncUNetPlans_24G', 'original_median_spacing_after_transp': [999.0, 1.0, 1.0], 'original_median_shape_after_transp': [1, 504, 504], 'image_reader_writer': 'NaturalImage2DIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'nnUNetPlannerResEncL', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 4282.0, 'mean': 1593.7022300557526, 'median': 1568.0, 'min': 0.0, 'percentile_00_5': 982.0, 'percentile_99_5': 2808.0, 'std': 337.91142407822184}}} 
 
2025-07-11 16:32:26.407318: Unable to plot network architecture: nnUNet_compile is enabled! 
2025-07-11 16:32:26.482888:  
2025-07-11 16:32:26.484065: Epoch 0 
2025-07-11 16:32:26.485432: Current learning rate: 0.01 
2025-07-11 16:33:53.436352: train_loss -0.0541 
2025-07-11 16:33:53.437820: val_loss -0.3882 
2025-07-11 16:33:53.439087: Pseudo dice [np.float32(0.6406)] 
2025-07-11 16:33:53.440359: Epoch time: 86.96 s 
2025-07-11 16:33:53.441301: Yayy! New best EMA pseudo Dice: 0.6406000256538391 
2025-07-11 16:33:55.357703:  
2025-07-11 16:33:55.359616: Epoch 1 
2025-07-11 16:33:55.360819: Current learning rate: 0.00999 
2025-07-11 16:35:02.392927: train_loss -0.5913 
2025-07-11 16:35:02.394402: val_loss -0.7666 
2025-07-11 16:35:02.395315: Pseudo dice [np.float32(0.8547)] 
2025-07-11 16:35:02.396765: Epoch time: 67.04 s 
2025-07-11 16:35:02.397876: Yayy! New best EMA pseudo Dice: 0.6620000004768372 
2025-07-11 16:35:04.472621:  
2025-07-11 16:35:04.474611: Epoch 2 
2025-07-11 16:35:04.475738: Current learning rate: 0.00998 
2025-07-11 16:36:11.933786: train_loss -0.8237 
2025-07-11 16:36:11.934913: val_loss -0.8566 
2025-07-11 16:36:11.935971: Pseudo dice [np.float32(0.8881)] 
2025-07-11 16:36:11.937164: Epoch time: 67.46 s 
2025-07-11 16:36:11.938041: Yayy! New best EMA pseudo Dice: 0.6845999956130981 
2025-07-11 16:36:14.190348:  
2025-07-11 16:36:14.191896: Epoch 3 
2025-07-11 16:36:14.193109: Current learning rate: 0.00997 
2025-07-11 16:37:21.592211: train_loss -0.8676 
2025-07-11 16:37:21.598942: val_loss -0.8813 
2025-07-11 16:37:21.600274: Pseudo dice [np.float32(0.9095)] 
2025-07-11 16:37:21.601378: Epoch time: 67.41 s 
2025-07-11 16:37:21.602340: Yayy! New best EMA pseudo Dice: 0.707099974155426 
2025-07-11 16:37:23.568783:  
2025-07-11 16:37:23.570106: Epoch 4 
2025-07-11 16:37:23.571170: Current learning rate: 0.00996 
2025-07-11 16:38:31.050506: train_loss -0.8846 
2025-07-11 16:38:31.051753: val_loss -0.8991 
2025-07-11 16:38:31.052984: Pseudo dice [np.float32(0.9222)] 
2025-07-11 16:38:31.053993: Epoch time: 67.49 s 
2025-07-11 16:38:31.054951: Yayy! New best EMA pseudo Dice: 0.728600025177002 
2025-07-11 16:38:33.150251:  
2025-07-11 16:38:33.151822: Epoch 5 
2025-07-11 16:38:33.152960: Current learning rate: 0.00995 
2025-07-11 16:39:40.776256: train_loss -0.8966 
2025-07-11 16:39:40.777403: val_loss -0.9114 
2025-07-11 16:39:40.778337: Pseudo dice [np.float32(0.9321)] 
2025-07-11 16:39:40.779346: Epoch time: 67.63 s 
2025-07-11 16:39:40.780263: Yayy! New best EMA pseudo Dice: 0.7488999962806702 
2025-07-11 16:39:42.862051:  
2025-07-11 16:39:42.863795: Epoch 6 
2025-07-11 16:39:42.865236: Current learning rate: 0.00995 
2025-07-11 16:40:50.424244: train_loss -0.9051 
2025-07-11 16:40:50.425429: val_loss -0.9142 
2025-07-11 16:40:50.426295: Pseudo dice [np.float32(0.9341)] 
2025-07-11 16:40:50.427197: Epoch time: 67.57 s 
2025-07-11 16:40:50.428115: Yayy! New best EMA pseudo Dice: 0.7674999833106995 
2025-07-11 16:40:52.505036:  
2025-07-11 16:40:52.506707: Epoch 7 
2025-07-11 16:40:52.507813: Current learning rate: 0.00994 
2025-07-11 16:41:59.974499: train_loss -0.9106 
2025-07-11 16:41:59.976008: val_loss -0.9191 
2025-07-11 16:41:59.976902: Pseudo dice [np.float32(0.9367)] 
2025-07-11 16:41:59.977922: Epoch time: 67.47 s 
2025-07-11 16:41:59.978844: Yayy! New best EMA pseudo Dice: 0.7843999862670898 
2025-07-11 16:42:02.076051:  
2025-07-11 16:42:02.077606: Epoch 8 
2025-07-11 16:42:02.078619: Current learning rate: 0.00993 
2025-07-11 16:43:09.978941: train_loss -0.9145 
2025-07-11 16:43:09.980157: val_loss -0.9206 
2025-07-11 16:43:09.981237: Pseudo dice [np.float32(0.9365)] 
2025-07-11 16:43:09.982307: Epoch time: 67.91 s 
2025-07-11 16:43:09.983430: Yayy! New best EMA pseudo Dice: 0.7996000051498413 
2025-07-11 16:43:12.208021:  
2025-07-11 16:43:12.209610: Epoch 9 
2025-07-11 16:43:12.210722: Current learning rate: 0.00992 
2025-07-11 16:44:19.864087: train_loss -0.9172 
2025-07-11 16:44:19.865430: val_loss -0.9288 
2025-07-11 16:44:19.866422: Pseudo dice [np.float32(0.9459)] 
2025-07-11 16:44:19.867348: Epoch time: 67.66 s 
2025-07-11 16:44:19.868187: Yayy! New best EMA pseudo Dice: 0.8141999840736389 
2025-07-11 16:44:21.946815:  
2025-07-11 16:44:21.948595: Epoch 10 
2025-07-11 16:44:21.949763: Current learning rate: 0.00991 
2025-07-11 16:45:29.812866: train_loss -0.9205 
2025-07-11 16:45:29.814024: val_loss -0.9219 
2025-07-11 16:45:29.815166: Pseudo dice [np.float32(0.9374)] 
2025-07-11 16:45:29.816317: Epoch time: 67.87 s 
2025-07-11 16:45:29.817206: Yayy! New best EMA pseudo Dice: 0.8264999985694885 
2025-07-11 16:45:31.888959:  
2025-07-11 16:45:31.890881: Epoch 11 
2025-07-11 16:45:31.892010: Current learning rate: 0.0099 
2025-07-11 16:46:39.641859: train_loss -0.9235 
2025-07-11 16:46:39.643292: val_loss -0.9276 
2025-07-11 16:46:39.644151: Pseudo dice [np.float32(0.9426)] 
2025-07-11 16:46:39.645033: Epoch time: 67.76 s 
2025-07-11 16:46:39.646004: Yayy! New best EMA pseudo Dice: 0.838100016117096 
2025-07-11 16:46:41.750362:  
2025-07-11 16:46:41.751897: Epoch 12 
2025-07-11 16:46:41.753103: Current learning rate: 0.00989 
2025-07-11 16:47:49.497630: train_loss -0.9248 
2025-07-11 16:47:49.498760: val_loss -0.9308 
2025-07-11 16:47:49.499717: Pseudo dice [np.float32(0.9455)] 
2025-07-11 16:47:49.500780: Epoch time: 67.75 s 
2025-07-11 16:47:49.501671: Yayy! New best EMA pseudo Dice: 0.8489000201225281 
2025-07-11 16:47:51.699303:  
2025-07-11 16:47:51.701033: Epoch 13 
2025-07-11 16:47:51.702557: Current learning rate: 0.00988 
2025-07-11 16:48:59.470087: train_loss -0.9272 
2025-07-11 16:48:59.471412: val_loss -0.9329 
2025-07-11 16:48:59.472301: Pseudo dice [np.float32(0.9462)] 
2025-07-11 16:48:59.473327: Epoch time: 67.77 s 
2025-07-11 16:48:59.474388: Yayy! New best EMA pseudo Dice: 0.8586000204086304 
2025-07-11 16:49:01.585058:  
2025-07-11 16:49:01.586744: Epoch 14 
2025-07-11 16:49:01.587877: Current learning rate: 0.00987 
2025-07-11 16:50:09.439897: train_loss -0.9289 
2025-07-11 16:50:09.441273: val_loss -0.934 
2025-07-11 16:50:09.442252: Pseudo dice [np.float32(0.9463)] 
2025-07-11 16:50:09.443157: Epoch time: 67.86 s 
2025-07-11 16:50:09.444152: Yayy! New best EMA pseudo Dice: 0.8673999905586243 
2025-07-11 16:50:11.548490:  
2025-07-11 16:50:11.550024: Epoch 15 
2025-07-11 16:50:11.551151: Current learning rate: 0.00986 
2025-07-11 16:51:19.292912: train_loss -0.9304 
2025-07-11 16:51:19.294059: val_loss -0.9336 
2025-07-11 16:51:19.295108: Pseudo dice [np.float32(0.9465)] 
2025-07-11 16:51:19.296280: Epoch time: 67.75 s 
2025-07-11 16:51:19.297474: Yayy! New best EMA pseudo Dice: 0.8752999901771545 
2025-07-11 16:51:21.418289:  
2025-07-11 16:51:21.419886: Epoch 16 
2025-07-11 16:51:21.421024: Current learning rate: 0.00986 
2025-07-11 16:52:29.333469: train_loss -0.9331 
2025-07-11 16:52:29.334661: val_loss -0.9356 
2025-07-11 16:52:29.335591: Pseudo dice [np.float32(0.9481)] 
2025-07-11 16:52:29.336807: Epoch time: 67.92 s 
2025-07-11 16:52:29.337866: Yayy! New best EMA pseudo Dice: 0.8826000094413757 
2025-07-11 16:52:31.465302:  
2025-07-11 16:52:31.467184: Epoch 17 
2025-07-11 16:52:31.468599: Current learning rate: 0.00985 
2025-07-11 16:53:39.435637: train_loss -0.9351 
2025-07-11 16:53:39.436787: val_loss -0.9346 
2025-07-11 16:53:39.437922: Pseudo dice [np.float32(0.946)] 
2025-07-11 16:53:39.439238: Epoch time: 67.97 s 
2025-07-11 16:53:39.440268: Yayy! New best EMA pseudo Dice: 0.8888999819755554 
2025-07-11 16:53:41.578404:  
2025-07-11 16:53:41.579972: Epoch 18 
2025-07-11 16:53:41.581216: Current learning rate: 0.00984 
2025-07-11 16:54:49.422172: train_loss -0.9361 
2025-07-11 16:54:49.423763: val_loss -0.9341 
2025-07-11 16:54:49.424795: Pseudo dice [np.float32(0.9466)] 
2025-07-11 16:54:49.425926: Epoch time: 67.85 s 
2025-07-11 16:54:49.426748: Yayy! New best EMA pseudo Dice: 0.8946999907493591 
2025-07-11 16:54:51.653264:  
2025-07-11 16:54:51.654944: Epoch 19 
2025-07-11 16:54:51.656076: Current learning rate: 0.00983 
2025-07-11 16:55:59.483517: train_loss -0.9373 
2025-07-11 16:55:59.484673: val_loss -0.9353 
2025-07-11 16:55:59.485835: Pseudo dice [np.float32(0.9469)] 
2025-07-11 16:55:59.487200: Epoch time: 67.83 s 
2025-07-11 16:55:59.488222: Yayy! New best EMA pseudo Dice: 0.8999000191688538 
2025-07-11 16:56:01.641065:  
2025-07-11 16:56:01.642606: Epoch 20 
2025-07-11 16:56:01.643637: Current learning rate: 0.00982 
2025-07-11 16:57:09.620199: train_loss -0.9371 
2025-07-11 16:57:09.621479: val_loss -0.9328 
2025-07-11 16:57:09.622455: Pseudo dice [np.float32(0.9432)] 
2025-07-11 16:57:09.623814: Epoch time: 67.98 s 
2025-07-11 16:57:09.624698: Yayy! New best EMA pseudo Dice: 0.90420001745224 
2025-07-11 16:57:11.809918:  
2025-07-11 16:57:11.811646: Epoch 21 
2025-07-11 16:57:11.812861: Current learning rate: 0.00981 
2025-07-11 16:58:19.722619: train_loss -0.938 
2025-07-11 16:58:19.723927: val_loss -0.9357 
2025-07-11 16:58:19.724877: Pseudo dice [np.float32(0.9459)] 
2025-07-11 16:58:19.725717: Epoch time: 67.92 s 
2025-07-11 16:58:19.726702: Yayy! New best EMA pseudo Dice: 0.9083999991416931 
2025-07-11 16:58:21.928011:  
2025-07-11 16:58:21.929558: Epoch 22 
2025-07-11 16:58:21.930646: Current learning rate: 0.0098 
2025-07-11 16:59:29.797325: train_loss -0.9397 
2025-07-11 16:59:29.798535: val_loss -0.9408 
2025-07-11 16:59:29.799512: Pseudo dice [np.float32(0.9506)] 
2025-07-11 16:59:29.800525: Epoch time: 67.87 s 
2025-07-11 16:59:29.801475: Yayy! New best EMA pseudo Dice: 0.9125999808311462 
2025-07-11 16:59:31.973040:  
2025-07-11 16:59:31.974813: Epoch 23 
2025-07-11 16:59:31.975983: Current learning rate: 0.00979 
2025-07-11 17:00:39.973608: train_loss -0.9414 
2025-07-11 17:00:39.974812: val_loss -0.9371 
2025-07-11 17:00:39.975731: Pseudo dice [np.float32(0.9465)] 
2025-07-11 17:00:39.976886: Epoch time: 68.0 s 
2025-07-11 17:00:39.977917: Yayy! New best EMA pseudo Dice: 0.9160000085830688 
2025-07-11 17:00:42.079428:  
2025-07-11 17:00:42.080948: Epoch 24 
2025-07-11 17:00:42.082003: Current learning rate: 0.00978 
2025-07-11 17:01:50.016273: train_loss -0.9414 
2025-07-11 17:01:50.017601: val_loss -0.9417 
2025-07-11 17:01:50.018605: Pseudo dice [np.float32(0.9504)] 
2025-07-11 17:01:50.019816: Epoch time: 67.94 s 
2025-07-11 17:01:50.020779: Yayy! New best EMA pseudo Dice: 0.9193999767303467 
2025-07-11 17:01:52.090483:  
2025-07-11 17:01:52.092256: Epoch 25 
2025-07-11 17:01:52.093406: Current learning rate: 0.00977 
2025-07-11 17:03:00.099892: train_loss -0.942 
2025-07-11 17:03:00.101362: val_loss -0.9391 
2025-07-11 17:03:00.102497: Pseudo dice [np.float32(0.9483)] 
2025-07-11 17:03:00.103505: Epoch time: 68.01 s 
2025-07-11 17:03:00.104692: Yayy! New best EMA pseudo Dice: 0.9222999811172485 
2025-07-11 17:03:02.183084:  
2025-07-11 17:03:02.184818: Epoch 26 
2025-07-11 17:03:02.185993: Current learning rate: 0.00977 
2025-07-11 17:04:10.219157: train_loss -0.943 
2025-07-11 17:04:10.220328: val_loss -0.9408 
2025-07-11 17:04:10.221445: Pseudo dice [np.float32(0.9488)] 
2025-07-11 17:04:10.222254: Epoch time: 68.04 s 
2025-07-11 17:04:10.223152: Yayy! New best EMA pseudo Dice: 0.925000011920929 
2025-07-11 17:04:12.299540:  
2025-07-11 17:04:12.300998: Epoch 27 
2025-07-11 17:04:12.302188: Current learning rate: 0.00976 
2025-07-11 17:05:20.193728: train_loss -0.9454 
2025-07-11 17:05:20.195074: val_loss -0.9402 
2025-07-11 17:05:20.195946: Pseudo dice [np.float32(0.9496)] 
2025-07-11 17:05:20.197101: Epoch time: 67.9 s 
2025-07-11 17:05:20.198027: Yayy! New best EMA pseudo Dice: 0.9273999929428101 
2025-07-11 17:05:22.296335:  
2025-07-11 17:05:22.297794: Epoch 28 
2025-07-11 17:05:22.298888: Current learning rate: 0.00975 
2025-07-11 17:06:30.248805: train_loss -0.9452 
2025-07-11 17:06:30.250024: val_loss -0.9401 
2025-07-11 17:06:30.251171: Pseudo dice [np.float32(0.9481)] 
2025-07-11 17:06:30.252284: Epoch time: 67.96 s 
2025-07-11 17:06:30.253398: Yayy! New best EMA pseudo Dice: 0.9294999837875366 
2025-07-11 17:06:32.350181:  
2025-07-11 17:06:32.351796: Epoch 29 
2025-07-11 17:06:32.352956: Current learning rate: 0.00974 
2025-07-11 17:07:40.522290: train_loss -0.9453 
2025-07-11 17:07:40.523394: val_loss -0.9408 
2025-07-11 17:07:40.524571: Pseudo dice [np.float32(0.9486)] 
2025-07-11 17:07:40.525584: Epoch time: 68.18 s 
2025-07-11 17:07:40.526643: Yayy! New best EMA pseudo Dice: 0.9314000010490417 
2025-07-11 17:07:42.856576:  
2025-07-11 17:07:42.858145: Epoch 30 
2025-07-11 17:07:42.859260: Current learning rate: 0.00973 
2025-07-11 17:08:51.022044: train_loss -0.9461 
2025-07-11 17:08:51.023271: val_loss -0.9429 
2025-07-11 17:08:51.024274: Pseudo dice [np.float32(0.9507)] 
2025-07-11 17:08:51.025151: Epoch time: 68.17 s 
2025-07-11 17:08:51.026042: Yayy! New best EMA pseudo Dice: 0.9333000183105469 
2025-07-11 17:08:53.123519:  
2025-07-11 17:08:53.124768: Epoch 31 
2025-07-11 17:08:53.125756: Current learning rate: 0.00972 
2025-07-11 17:10:01.196007: train_loss -0.9474 
2025-07-11 17:10:01.197088: val_loss -0.9421 
2025-07-11 17:10:01.198224: Pseudo dice [np.float32(0.9494)] 
2025-07-11 17:10:01.199120: Epoch time: 68.08 s 
2025-07-11 17:10:01.200076: Yayy! New best EMA pseudo Dice: 0.9350000023841858 
2025-07-11 17:10:03.342754:  
2025-07-11 17:10:03.344401: Epoch 32 
2025-07-11 17:10:03.345799: Current learning rate: 0.00971 
2025-07-11 17:11:11.253101: train_loss -0.9468 
2025-07-11 17:11:11.254481: val_loss -0.9387 
2025-07-11 17:11:11.255655: Pseudo dice [np.float32(0.9459)] 
2025-07-11 17:11:11.256935: Epoch time: 67.91 s 
2025-07-11 17:11:11.258039: Yayy! New best EMA pseudo Dice: 0.9361000061035156 
2025-07-11 17:11:13.600308:  
2025-07-11 17:11:13.601765: Epoch 33 
2025-07-11 17:11:13.602950: Current learning rate: 0.0097 
2025-07-11 17:12:21.520356: train_loss -0.9471 
2025-07-11 17:12:21.521572: val_loss -0.9392 
2025-07-11 17:12:21.522684: Pseudo dice [np.float32(0.9469)] 
2025-07-11 17:12:21.523922: Epoch time: 67.92 s 
2025-07-11 17:12:21.524715: Yayy! New best EMA pseudo Dice: 0.9370999932289124 
2025-07-11 17:12:23.712659:  
2025-07-11 17:12:23.714201: Epoch 34 
2025-07-11 17:12:23.715282: Current learning rate: 0.00969 
2025-07-11 17:13:31.654416: train_loss -0.9481 
2025-07-11 17:13:31.655763: val_loss -0.9422 
2025-07-11 17:13:31.656838: Pseudo dice [np.float32(0.95)] 
2025-07-11 17:13:31.657839: Epoch time: 67.95 s 
2025-07-11 17:13:31.658791: Yayy! New best EMA pseudo Dice: 0.9383999705314636 
2025-07-11 17:13:33.764631:  
2025-07-11 17:13:33.765968: Epoch 35 
2025-07-11 17:13:33.767089: Current learning rate: 0.00968 
2025-07-11 17:14:41.722461: train_loss -0.9486 
2025-07-11 17:14:41.723732: val_loss -0.9433 
2025-07-11 17:14:41.724630: Pseudo dice [np.float32(0.9509)] 
2025-07-11 17:14:41.725541: Epoch time: 67.96 s 
2025-07-11 17:14:41.726458: Yayy! New best EMA pseudo Dice: 0.9397000074386597 
2025-07-11 17:14:44.077272:  
2025-07-11 17:14:44.078755: Epoch 36 
2025-07-11 17:14:44.079811: Current learning rate: 0.00968 
2025-07-11 17:15:52.104988: train_loss -0.9503 
2025-07-11 17:15:52.106349: val_loss -0.9412 
2025-07-11 17:15:52.107420: Pseudo dice [np.float32(0.9482)] 
2025-07-11 17:15:52.108678: Epoch time: 68.03 s 
2025-07-11 17:15:52.109697: Yayy! New best EMA pseudo Dice: 0.940500020980835 
2025-07-11 17:15:54.232103:  
2025-07-11 17:15:54.233787: Epoch 37 
2025-07-11 17:15:54.234984: Current learning rate: 0.00967 
2025-07-11 17:17:02.410659: train_loss -0.9515 
2025-07-11 17:17:02.411822: val_loss -0.9398 
2025-07-11 17:17:02.412704: Pseudo dice [np.float32(0.9483)] 
2025-07-11 17:17:02.413936: Epoch time: 68.18 s 
2025-07-11 17:17:02.415076: Yayy! New best EMA pseudo Dice: 0.9412999749183655 
2025-07-11 17:17:04.536122:  
2025-07-11 17:17:04.537692: Epoch 38 
2025-07-11 17:17:04.538866: Current learning rate: 0.00966 
2025-07-11 17:18:12.392541: train_loss -0.9518 
2025-07-11 17:18:12.393840: val_loss -0.9413 
2025-07-11 17:18:12.394696: Pseudo dice [np.float32(0.9476)] 
2025-07-11 17:18:12.395563: Epoch time: 67.86 s 
2025-07-11 17:18:12.396524: Yayy! New best EMA pseudo Dice: 0.9419000148773193 
2025-07-11 17:18:14.623948:  
2025-07-11 17:18:14.625730: Epoch 39 
2025-07-11 17:18:14.626824: Current learning rate: 0.00965 
2025-07-11 17:19:22.618991: train_loss -0.9511 
2025-07-11 17:19:22.620463: val_loss -0.9416 
2025-07-11 17:19:22.621754: Pseudo dice [np.float32(0.9482)] 
2025-07-11 17:19:22.622746: Epoch time: 68.0 s 
2025-07-11 17:19:22.623829: Yayy! New best EMA pseudo Dice: 0.9426000118255615 
2025-07-11 17:19:24.876373:  
2025-07-11 17:19:24.877892: Epoch 40 
2025-07-11 17:19:24.878929: Current learning rate: 0.00964 
2025-07-11 17:20:33.032413: train_loss -0.9513 
2025-07-11 17:20:33.033766: val_loss -0.945 
2025-07-11 17:20:33.034812: Pseudo dice [np.float32(0.9513)] 
2025-07-11 17:20:33.035965: Epoch time: 68.16 s 
2025-07-11 17:20:33.036842: Yayy! New best EMA pseudo Dice: 0.9434000253677368 
2025-07-11 17:20:35.177032:  
2025-07-11 17:20:35.178770: Epoch 41 
2025-07-11 17:20:35.180007: Current learning rate: 0.00963 
2025-07-11 17:21:43.322748: train_loss -0.952 
2025-07-11 17:21:43.324049: val_loss -0.9443 
2025-07-11 17:21:43.325112: Pseudo dice [np.float32(0.9516)] 
2025-07-11 17:21:43.326168: Epoch time: 68.15 s 
2025-07-11 17:21:43.327292: Yayy! New best EMA pseudo Dice: 0.9442999958992004 
2025-07-11 17:21:45.420599:  
2025-07-11 17:21:45.422114: Epoch 42 
2025-07-11 17:21:45.423283: Current learning rate: 0.00962 
2025-07-11 17:22:53.778864: train_loss -0.9513 
2025-07-11 17:22:53.780117: val_loss -0.9409 
2025-07-11 17:22:53.781196: Pseudo dice [np.float32(0.947)] 
2025-07-11 17:22:53.782572: Epoch time: 68.36 s 
2025-07-11 17:22:53.783787: Yayy! New best EMA pseudo Dice: 0.9445000290870667 
2025-07-11 17:22:55.995910:  
2025-07-11 17:22:55.997256: Epoch 43 
2025-07-11 17:22:55.998405: Current learning rate: 0.00961 
2025-07-11 17:24:04.199125: train_loss -0.9521 
2025-07-11 17:24:04.200496: val_loss -0.9429 
2025-07-11 17:24:04.201343: Pseudo dice [np.float32(0.9487)] 
2025-07-11 17:24:04.202430: Epoch time: 68.21 s 
2025-07-11 17:24:04.203421: Yayy! New best EMA pseudo Dice: 0.9449999928474426 
2025-07-11 17:24:06.317261:  
2025-07-11 17:24:06.318958: Epoch 44 
2025-07-11 17:24:06.320108: Current learning rate: 0.0096 
2025-07-11 17:25:14.471327: train_loss -0.9522 
2025-07-11 17:25:14.472543: val_loss -0.9448 
2025-07-11 17:25:14.473727: Pseudo dice [np.float32(0.9506)] 
2025-07-11 17:25:14.474792: Epoch time: 68.16 s 
2025-07-11 17:25:14.475813: Yayy! New best EMA pseudo Dice: 0.9455000162124634 
2025-07-11 17:25:16.572842:  
2025-07-11 17:25:16.574670: Epoch 45 
2025-07-11 17:25:16.575863: Current learning rate: 0.00959 
2025-07-11 17:26:24.702560: train_loss -0.9536 
2025-07-11 17:26:24.703924: val_loss -0.9463 
2025-07-11 17:26:24.705266: Pseudo dice [np.float32(0.9525)] 
2025-07-11 17:26:24.706297: Epoch time: 68.13 s 
2025-07-11 17:26:24.707088: Yayy! New best EMA pseudo Dice: 0.9462000131607056 
2025-07-11 17:26:26.926533:  
2025-07-11 17:26:26.928174: Epoch 46 
2025-07-11 17:26:26.929224: Current learning rate: 0.00959 
2025-07-11 17:27:34.839451: train_loss -0.9531 
2025-07-11 17:27:34.840734: val_loss -0.9403 
2025-07-11 17:27:34.841648: Pseudo dice [np.float32(0.9459)] 
2025-07-11 17:27:34.842604: Epoch time: 67.92 s 
2025-07-11 17:27:35.715431:  
2025-07-11 17:27:35.717344: Epoch 47 
2025-07-11 17:27:35.718560: Current learning rate: 0.00958 
2025-07-11 17:28:43.746809: train_loss -0.9532 
2025-07-11 17:28:43.748149: val_loss -0.9434 
2025-07-11 17:28:43.749231: Pseudo dice [np.float32(0.9489)] 
2025-07-11 17:28:43.750173: Epoch time: 68.03 s 
2025-07-11 17:28:43.751327: Yayy! New best EMA pseudo Dice: 0.9465000033378601 
2025-07-11 17:28:45.842171:  
2025-07-11 17:28:45.843957: Epoch 48 
2025-07-11 17:28:45.845139: Current learning rate: 0.00957 
2025-07-11 17:29:54.083544: train_loss -0.9553 
2025-07-11 17:29:54.084940: val_loss -0.9457 
2025-07-11 17:29:54.085908: Pseudo dice [np.float32(0.9519)] 
2025-07-11 17:29:54.086918: Epoch time: 68.24 s 
2025-07-11 17:29:54.087887: Yayy! New best EMA pseudo Dice: 0.9470000267028809 
2025-07-11 17:29:56.192884:  
2025-07-11 17:29:56.194376: Epoch 49 
2025-07-11 17:29:56.195540: Current learning rate: 0.00956 
2025-07-11 17:31:04.219894: train_loss -0.9558 
2025-07-11 17:31:04.221196: val_loss -0.9454 
2025-07-11 17:31:04.222105: Pseudo dice [np.float32(0.9512)] 
2025-07-11 17:31:04.223037: Epoch time: 68.03 s 
2025-07-11 17:31:05.283363: Yayy! New best EMA pseudo Dice: 0.9473999738693237 
2025-07-11 17:31:07.405519:  
2025-07-11 17:31:07.407223: Epoch 50 
2025-07-11 17:31:07.408227: Current learning rate: 0.00955 
2025-07-11 17:32:15.432055: train_loss -0.9551 
2025-07-11 17:32:15.433285: val_loss -0.9453 
2025-07-11 17:32:15.434458: Pseudo dice [np.float32(0.9511)] 
2025-07-11 17:32:15.435359: Epoch time: 68.03 s 
2025-07-11 17:32:15.436492: Yayy! New best EMA pseudo Dice: 0.9477999806404114 
2025-07-11 17:32:17.548883:  
2025-07-11 17:32:17.550351: Epoch 51 
2025-07-11 17:32:17.551487: Current learning rate: 0.00954 
2025-07-11 17:33:25.814541: train_loss -0.9571 
2025-07-11 17:33:25.815809: val_loss -0.9452 
2025-07-11 17:33:25.816762: Pseudo dice [np.float32(0.951)] 
2025-07-11 17:33:25.817693: Epoch time: 68.27 s 
2025-07-11 17:33:25.818636: Yayy! New best EMA pseudo Dice: 0.9480999708175659 
2025-07-11 17:33:27.944197:  
2025-07-11 17:33:27.945760: Epoch 52 
2025-07-11 17:33:27.946867: Current learning rate: 0.00953 
2025-07-11 17:34:36.197844: train_loss -0.9558 
2025-07-11 17:34:36.199389: val_loss -0.9475 
2025-07-11 17:34:36.200356: Pseudo dice [np.float32(0.9527)] 
2025-07-11 17:34:36.201365: Epoch time: 68.26 s 
2025-07-11 17:34:36.202243: Yayy! New best EMA pseudo Dice: 0.9485999941825867 
2025-07-11 17:34:38.327097:  
2025-07-11 17:34:38.328515: Epoch 53 
2025-07-11 17:34:38.329689: Current learning rate: 0.00952 
2025-07-11 17:35:46.768269: train_loss -0.9557 
2025-07-11 17:35:46.769691: val_loss -0.9457 
2025-07-11 17:35:46.770658: Pseudo dice [np.float32(0.9507)] 
2025-07-11 17:35:46.771639: Epoch time: 68.44 s 
2025-07-11 17:35:46.772675: Yayy! New best EMA pseudo Dice: 0.9488000273704529 
2025-07-11 17:35:48.906418:  
2025-07-11 17:35:48.908004: Epoch 54 
2025-07-11 17:35:48.909479: Current learning rate: 0.00951 
2025-07-11 17:36:57.117227: train_loss -0.9566 
2025-07-11 17:36:57.118494: val_loss -0.9467 
2025-07-11 17:36:57.119579: Pseudo dice [np.float32(0.9517)] 
2025-07-11 17:36:57.120482: Epoch time: 68.21 s 
2025-07-11 17:36:57.121382: Yayy! New best EMA pseudo Dice: 0.9491000175476074 
2025-07-11 17:36:59.469775:  
2025-07-11 17:36:59.471184: Epoch 55 
2025-07-11 17:36:59.472249: Current learning rate: 0.0095 
2025-07-11 17:38:07.677082: train_loss -0.9568 
2025-07-11 17:38:07.679316: val_loss -0.9474 
2025-07-11 17:38:07.680341: Pseudo dice [np.float32(0.9529)] 
2025-07-11 17:38:07.681513: Epoch time: 68.21 s 
2025-07-11 17:38:07.682874: Yayy! New best EMA pseudo Dice: 0.9495000243186951 
2025-07-11 17:38:09.817937:  
2025-07-11 17:38:09.819524: Epoch 56 
2025-07-11 17:38:09.820675: Current learning rate: 0.00949 
2025-07-11 17:39:17.965004: train_loss -0.9573 
2025-07-11 17:39:17.966268: val_loss -0.9438 
2025-07-11 17:39:17.967144: Pseudo dice [np.float32(0.9489)] 
2025-07-11 17:39:17.968538: Epoch time: 68.15 s 
2025-07-11 17:39:18.851264:  
2025-07-11 17:39:18.852959: Epoch 57 
2025-07-11 17:39:18.854614: Current learning rate: 0.00949 
2025-07-11 17:40:27.284928: train_loss -0.9559 
2025-07-11 17:40:27.286209: val_loss -0.9498 
2025-07-11 17:40:27.287194: Pseudo dice [np.float32(0.9553)] 
2025-07-11 17:40:27.288081: Epoch time: 68.44 s 
2025-07-11 17:40:27.289105: Yayy! New best EMA pseudo Dice: 0.949999988079071 
2025-07-11 17:40:29.642562:  
2025-07-11 17:40:29.644129: Epoch 58 
2025-07-11 17:40:29.645244: Current learning rate: 0.00948 
2025-07-11 17:41:37.769289: train_loss -0.9598 
2025-07-11 17:41:37.770639: val_loss -0.9466 
2025-07-11 17:41:37.771778: Pseudo dice [np.float32(0.9527)] 
2025-07-11 17:41:37.773021: Epoch time: 68.13 s 
2025-07-11 17:41:37.774201: Yayy! New best EMA pseudo Dice: 0.9502999782562256 
2025-07-11 17:41:39.916418:  
2025-07-11 17:41:39.918156: Epoch 59 
2025-07-11 17:41:39.919264: Current learning rate: 0.00947 
2025-07-11 17:42:48.073278: train_loss -0.9582 
2025-07-11 17:42:48.074586: val_loss -0.9465 
2025-07-11 17:42:48.075645: Pseudo dice [np.float32(0.9518)] 
2025-07-11 17:42:48.076773: Epoch time: 68.16 s 
2025-07-11 17:42:48.077688: Yayy! New best EMA pseudo Dice: 0.9503999948501587 
2025-07-11 17:42:50.279465:  
2025-07-11 17:42:50.280702: Epoch 60 
2025-07-11 17:42:50.281847: Current learning rate: 0.00946 
2025-07-11 17:43:58.567931: train_loss -0.9584 
2025-07-11 17:43:58.569306: val_loss -0.9462 
2025-07-11 17:43:58.570200: Pseudo dice [np.float32(0.9524)] 
2025-07-11 17:43:58.571115: Epoch time: 68.29 s 
2025-07-11 17:43:58.572029: Yayy! New best EMA pseudo Dice: 0.9506000280380249 
2025-07-11 17:44:00.934951:  
2025-07-11 17:44:00.936885: Epoch 61 
2025-07-11 17:44:00.937977: Current learning rate: 0.00945 
2025-07-11 17:45:09.116244: train_loss -0.958 
2025-07-11 17:45:09.117526: val_loss -0.9477 
2025-07-11 17:45:09.118592: Pseudo dice [np.float32(0.9528)] 
2025-07-11 17:45:09.119825: Epoch time: 68.18 s 
2025-07-11 17:45:09.120945: Yayy! New best EMA pseudo Dice: 0.9508000016212463 
2025-07-11 17:45:11.273448:  
2025-07-11 17:45:11.275038: Epoch 62 
2025-07-11 17:45:11.276107: Current learning rate: 0.00944 
2025-07-11 17:46:19.909240: train_loss -0.9582 
2025-07-11 17:46:19.910623: val_loss -0.9481 
2025-07-11 17:46:19.911578: Pseudo dice [np.float32(0.9537)] 
2025-07-11 17:46:19.912659: Epoch time: 68.64 s 
2025-07-11 17:46:19.913677: Yayy! New best EMA pseudo Dice: 0.9510999917984009 
2025-07-11 17:46:22.036649:  
2025-07-11 17:46:22.038140: Epoch 63 
2025-07-11 17:46:22.039239: Current learning rate: 0.00943 
2025-07-11 17:47:30.297840: train_loss -0.9596 
2025-07-11 17:47:30.299186: val_loss -0.947 
2025-07-11 17:47:30.300322: Pseudo dice [np.float32(0.9524)] 
2025-07-11 17:47:30.301439: Epoch time: 68.26 s 
2025-07-11 17:47:30.302348: Yayy! New best EMA pseudo Dice: 0.951200008392334 
2025-07-11 17:47:32.623480:  
2025-07-11 17:47:32.624998: Epoch 64 
2025-07-11 17:47:32.626153: Current learning rate: 0.00942 
2025-07-11 17:48:40.817856: train_loss -0.9592 
2025-07-11 17:48:40.819119: val_loss -0.9452 
2025-07-11 17:48:40.820170: Pseudo dice [np.float32(0.95)] 
2025-07-11 17:48:40.821211: Epoch time: 68.2 s 
2025-07-11 17:48:41.718057:  
2025-07-11 17:48:41.719933: Epoch 65 
2025-07-11 17:48:41.721120: Current learning rate: 0.00941 
2025-07-11 17:49:49.676037: train_loss -0.9592 
2025-07-11 17:49:49.677873: val_loss -0.9463 
2025-07-11 17:49:49.678852: Pseudo dice [np.float32(0.9519)] 
2025-07-11 17:49:49.679867: Epoch time: 67.96 s 
2025-07-11 17:49:50.573160:  
2025-07-11 17:49:50.574920: Epoch 66 
2025-07-11 17:49:50.576048: Current learning rate: 0.0094 
2025-07-11 17:50:58.593326: train_loss -0.9585 
2025-07-11 17:50:58.594455: val_loss -0.9497 
2025-07-11 17:50:58.595463: Pseudo dice [np.float32(0.954)] 
2025-07-11 17:50:58.596650: Epoch time: 68.02 s 
2025-07-11 17:50:58.597802: Yayy! New best EMA pseudo Dice: 0.9514999985694885 
2025-07-11 17:51:00.730719:  
2025-07-11 17:51:00.732098: Epoch 67 
2025-07-11 17:51:00.733380: Current learning rate: 0.00939 
2025-07-11 17:52:09.050383: train_loss -0.958 
2025-07-11 17:52:09.051448: val_loss -0.9485 
2025-07-11 17:52:09.052583: Pseudo dice [np.float32(0.9526)] 
2025-07-11 17:52:09.053582: Epoch time: 68.32 s 
2025-07-11 17:52:09.054571: Yayy! New best EMA pseudo Dice: 0.9516000151634216 
2025-07-11 17:52:11.198696:  
2025-07-11 17:52:11.200189: Epoch 68 
2025-07-11 17:52:11.201632: Current learning rate: 0.00939 
2025-07-11 17:53:19.110636: train_loss -0.9572 
2025-07-11 17:53:19.112080: val_loss -0.9448 
2025-07-11 17:53:19.113107: Pseudo dice [np.float32(0.9504)] 
2025-07-11 17:53:19.114474: Epoch time: 67.92 s 
2025-07-11 17:53:20.011189:  
2025-07-11 17:53:20.013016: Epoch 69 
2025-07-11 17:53:20.014612: Current learning rate: 0.00938 
2025-07-11 17:54:28.042182: train_loss -0.9591 
2025-07-11 17:54:28.043451: val_loss -0.9463 
2025-07-11 17:54:28.044626: Pseudo dice [np.float32(0.9511)] 
2025-07-11 17:54:28.045900: Epoch time: 68.03 s 
2025-07-11 17:54:28.960203:  
2025-07-11 17:54:28.962042: Epoch 70 
2025-07-11 17:54:28.963167: Current learning rate: 0.00937 
2025-07-11 17:55:37.197556: train_loss -0.9599 
2025-07-11 17:55:37.198920: val_loss -0.9489 
2025-07-11 17:55:37.199984: Pseudo dice [np.float32(0.9544)] 
2025-07-11 17:55:37.201101: Epoch time: 68.24 s 
2025-07-11 17:55:37.202165: Yayy! New best EMA pseudo Dice: 0.95169997215271 
2025-07-11 17:55:39.331367:  
2025-07-11 17:55:39.332969: Epoch 71 
2025-07-11 17:55:39.334115: Current learning rate: 0.00936 
2025-07-11 17:56:47.344082: train_loss -0.9607 
2025-07-11 17:56:47.345426: val_loss -0.9491 
2025-07-11 17:56:47.346383: Pseudo dice [np.float32(0.954)] 
2025-07-11 17:56:47.347353: Epoch time: 68.02 s 
2025-07-11 17:56:47.348266: Yayy! New best EMA pseudo Dice: 0.9520000219345093 
2025-07-11 17:56:49.484361:  
2025-07-11 17:56:49.485984: Epoch 72 
2025-07-11 17:56:49.487167: Current learning rate: 0.00935 
2025-07-11 17:57:57.535045: train_loss -0.9606 
2025-07-11 17:57:57.536669: val_loss -0.9464 
2025-07-11 17:57:57.537597: Pseudo dice [np.float32(0.9513)] 
2025-07-11 17:57:57.538627: Epoch time: 68.05 s 
2025-07-11 17:57:58.444876:  
2025-07-11 17:57:58.446538: Epoch 73 
2025-07-11 17:57:58.447645: Current learning rate: 0.00934 
2025-07-11 17:59:06.382411: train_loss -0.9604 
2025-07-11 17:59:06.383779: val_loss -0.9469 
2025-07-11 17:59:06.384826: Pseudo dice [np.float32(0.9527)] 
2025-07-11 17:59:06.385716: Epoch time: 67.94 s 
2025-07-11 17:59:06.386765: Yayy! New best EMA pseudo Dice: 0.9520000219345093 
2025-07-11 17:59:08.770147:  
2025-07-11 17:59:08.771818: Epoch 74 
2025-07-11 17:59:08.772983: Current learning rate: 0.00933 
2025-07-11 18:00:16.751202: train_loss -0.9606 
2025-07-11 18:00:16.752576: val_loss -0.9465 
2025-07-11 18:00:16.753864: Pseudo dice [np.float32(0.951)] 
2025-07-11 18:00:16.755044: Epoch time: 67.98 s 
2025-07-11 18:00:17.668540:  
2025-07-11 18:00:17.670180: Epoch 75 
2025-07-11 18:00:17.671364: Current learning rate: 0.00932 
2025-07-11 18:01:25.608364: train_loss -0.9613 
2025-07-11 18:01:25.609626: val_loss -0.9498 
2025-07-11 18:01:25.610685: Pseudo dice [np.float32(0.9542)] 
2025-07-11 18:01:25.611580: Epoch time: 67.94 s 
2025-07-11 18:01:25.612491: Yayy! New best EMA pseudo Dice: 0.9520999789237976 
2025-07-11 18:01:27.750059:  
2025-07-11 18:01:27.751639: Epoch 76 
2025-07-11 18:01:27.752806: Current learning rate: 0.00931 
2025-07-11 18:02:36.015754: train_loss -0.9608 
2025-07-11 18:02:36.016985: val_loss -0.9451 
2025-07-11 18:02:36.018171: Pseudo dice [np.float32(0.951)] 
2025-07-11 18:02:36.019494: Epoch time: 68.27 s 
2025-07-11 18:02:37.165652:  
2025-07-11 18:02:37.167577: Epoch 77 
2025-07-11 18:02:37.168797: Current learning rate: 0.0093 
2025-07-11 18:03:45.317452: train_loss -0.9596 
2025-07-11 18:03:45.318717: val_loss -0.9455 
2025-07-11 18:03:45.319866: Pseudo dice [np.float32(0.9509)] 
2025-07-11 18:03:45.321041: Epoch time: 68.16 s 
2025-07-11 18:03:46.233907:  
2025-07-11 18:03:46.235644: Epoch 78 
2025-07-11 18:03:46.236795: Current learning rate: 0.0093 
2025-07-11 18:04:54.229338: train_loss -0.9592 
2025-07-11 18:04:54.230698: val_loss -0.947 
2025-07-11 18:04:54.231622: Pseudo dice [np.float32(0.9521)] 
2025-07-11 18:04:54.232715: Epoch time: 68.0 s 
2025-07-11 18:04:55.148491:  
2025-07-11 18:04:55.150029: Epoch 79 
2025-07-11 18:04:55.151196: Current learning rate: 0.00929 
2025-07-11 18:06:03.137011: train_loss -0.9592 
2025-07-11 18:06:03.138207: val_loss -0.9472 
2025-07-11 18:06:03.139275: Pseudo dice [np.float32(0.9521)] 
2025-07-11 18:06:03.140134: Epoch time: 67.99 s 
2025-07-11 18:06:04.051718:  
2025-07-11 18:06:04.053556: Epoch 80 
2025-07-11 18:06:04.054749: Current learning rate: 0.00928 
2025-07-11 18:07:12.189674: train_loss -0.96 
2025-07-11 18:07:12.191007: val_loss -0.9438 
2025-07-11 18:07:12.192034: Pseudo dice [np.float32(0.9489)] 
2025-07-11 18:07:12.192966: Epoch time: 68.14 s 
2025-07-11 18:07:13.104229:  
2025-07-11 18:07:13.105862: Epoch 81 
2025-07-11 18:07:13.106948: Current learning rate: 0.00927 
2025-07-11 18:08:21.013779: train_loss -0.9615 
2025-07-11 18:08:21.015063: val_loss -0.9481 
2025-07-11 18:08:21.016150: Pseudo dice [np.float32(0.9533)] 
2025-07-11 18:08:21.017117: Epoch time: 67.91 s 
2025-07-11 18:08:21.937651:  
2025-07-11 18:08:21.938927: Epoch 82 
2025-07-11 18:08:21.939997: Current learning rate: 0.00926 
2025-07-11 18:09:30.106498: train_loss -0.96 
2025-07-11 18:09:30.108009: val_loss -0.9455 
2025-07-11 18:09:30.109057: Pseudo dice [np.float32(0.9511)] 
2025-07-11 18:09:30.110367: Epoch time: 68.17 s 
2025-07-11 18:09:30.989503:  
2025-07-11 18:09:30.991366: Epoch 83 
2025-07-11 18:09:30.992568: Current learning rate: 0.00925 
2025-07-11 18:10:39.256118: train_loss -0.9617 
2025-07-11 18:10:39.257750: val_loss -0.9483 
2025-07-11 18:10:39.258849: Pseudo dice [np.float32(0.9532)] 
2025-07-11 18:10:39.259954: Epoch time: 68.27 s 
2025-07-11 18:10:40.376080:  
2025-07-11 18:10:40.378154: Epoch 84 
2025-07-11 18:10:40.379331: Current learning rate: 0.00924 
2025-07-11 18:11:48.553866: train_loss -0.9606 
2025-07-11 18:11:48.555619: val_loss -0.9434 
2025-07-11 18:11:48.556694: Pseudo dice [np.float32(0.9487)] 
2025-07-11 18:11:48.557833: Epoch time: 68.18 s 
2025-07-11 18:11:49.441715:  
2025-07-11 18:11:49.443232: Epoch 85 
2025-07-11 18:11:49.444583: Current learning rate: 0.00923 
2025-07-11 18:12:57.586334: train_loss -0.961 
2025-07-11 18:12:57.587797: val_loss -0.9472 
2025-07-11 18:12:57.588781: Pseudo dice [np.float32(0.9518)] 
2025-07-11 18:12:57.589667: Epoch time: 68.15 s 
2025-07-11 18:12:58.467471:  
2025-07-11 18:12:58.469253: Epoch 86 
2025-07-11 18:12:58.470386: Current learning rate: 0.00922 
2025-07-11 18:14:06.647255: train_loss -0.9595 
2025-07-11 18:14:06.648448: val_loss -0.9474 
2025-07-11 18:14:06.649436: Pseudo dice [np.float32(0.9527)] 
2025-07-11 18:14:06.650542: Epoch time: 68.18 s 
2025-07-11 18:14:07.549517:  
2025-07-11 18:14:07.551298: Epoch 87 
2025-07-11 18:14:07.552456: Current learning rate: 0.00921 
2025-07-11 18:15:15.833219: train_loss -0.9597 
2025-07-11 18:15:15.834413: val_loss -0.9489 
2025-07-11 18:15:15.835759: Pseudo dice [np.float32(0.9534)] 
2025-07-11 18:15:15.836809: Epoch time: 68.29 s 
2025-07-11 18:15:16.727356:  
2025-07-11 18:15:16.729117: Epoch 88 
2025-07-11 18:15:16.730345: Current learning rate: 0.0092 
2025-07-11 18:16:24.877812: train_loss -0.9609 
2025-07-11 18:16:24.879032: val_loss -0.946 
2025-07-11 18:16:24.880157: Pseudo dice [np.float32(0.9512)] 
2025-07-11 18:16:24.881140: Epoch time: 68.15 s 
2025-07-11 18:16:25.764975:  
2025-07-11 18:16:25.766909: Epoch 89 
2025-07-11 18:16:25.768617: Current learning rate: 0.0092 
2025-07-11 18:17:33.916043: train_loss -0.9613 
2025-07-11 18:17:33.917413: val_loss -0.9496 
2025-07-11 18:17:33.918549: Pseudo dice [np.float32(0.9545)] 
2025-07-11 18:17:33.919660: Epoch time: 68.15 s 
2025-07-11 18:17:34.797454:  
2025-07-11 18:17:34.799114: Epoch 90 
2025-07-11 18:17:34.800353: Current learning rate: 0.00919 
2025-07-11 18:18:42.900680: train_loss -0.961 
2025-07-11 18:18:42.902149: val_loss -0.9448 
2025-07-11 18:18:42.903740: Pseudo dice [np.float32(0.9504)] 
2025-07-11 18:18:42.904840: Epoch time: 68.11 s 
2025-07-11 18:18:44.019995:  
2025-07-11 18:18:44.021580: Epoch 91 
2025-07-11 18:18:44.022823: Current learning rate: 0.00918 
2025-07-11 18:19:52.120436: train_loss -0.9611 
2025-07-11 18:19:52.121622: val_loss -0.9483 
2025-07-11 18:19:52.122661: Pseudo dice [np.float32(0.9539)] 
2025-07-11 18:19:52.123610: Epoch time: 68.1 s 
2025-07-11 18:19:52.992578:  
2025-07-11 18:19:52.994229: Epoch 92 
2025-07-11 18:19:52.995427: Current learning rate: 0.00917 
2025-07-11 18:21:01.105232: train_loss -0.9627 
2025-07-11 18:21:01.106782: val_loss -0.9482 
2025-07-11 18:21:01.107996: Pseudo dice [np.float32(0.9526)] 
2025-07-11 18:21:01.109076: Epoch time: 68.12 s 
2025-07-11 18:21:01.110270: Yayy! New best EMA pseudo Dice: 0.9521999955177307 
2025-07-11 18:21:03.428996:  
2025-07-11 18:21:03.430415: Epoch 93 
2025-07-11 18:21:03.431397: Current learning rate: 0.00916 
2025-07-11 18:22:11.523901: train_loss -0.9618 
2025-07-11 18:22:11.525176: val_loss -0.9499 
2025-07-11 18:22:11.526224: Pseudo dice [np.float32(0.9555)] 
2025-07-11 18:22:11.527112: Epoch time: 68.1 s 
2025-07-11 18:22:11.528081: Yayy! New best EMA pseudo Dice: 0.9524999856948853 
2025-07-11 18:22:13.619062:  
2025-07-11 18:22:13.620755: Epoch 94 
2025-07-11 18:22:13.622042: Current learning rate: 0.00915 
2025-07-11 18:23:21.750044: train_loss -0.9617 
2025-07-11 18:23:21.751362: val_loss -0.9486 
2025-07-11 18:23:21.752276: Pseudo dice [np.float32(0.9539)] 
2025-07-11 18:23:21.753381: Epoch time: 68.13 s 
2025-07-11 18:23:21.754262: Yayy! New best EMA pseudo Dice: 0.9526000022888184 
2025-07-11 18:23:24.050987:  
2025-07-11 18:23:24.052778: Epoch 95 
2025-07-11 18:23:24.054073: Current learning rate: 0.00914 
2025-07-11 18:24:32.054343: train_loss -0.9621 
2025-07-11 18:24:32.055723: val_loss -0.951 
2025-07-11 18:24:32.056758: Pseudo dice [np.float32(0.9555)] 
2025-07-11 18:24:32.057922: Epoch time: 68.01 s 
2025-07-11 18:24:32.059083: Yayy! New best EMA pseudo Dice: 0.9528999924659729 
2025-07-11 18:24:34.173420:  
2025-07-11 18:24:34.175007: Epoch 96 
2025-07-11 18:24:34.176130: Current learning rate: 0.00913 
2025-07-11 18:25:42.347783: train_loss -0.9612 
2025-07-11 18:25:42.349075: val_loss -0.9458 
2025-07-11 18:25:42.350278: Pseudo dice [np.float32(0.9506)] 
2025-07-11 18:25:42.351205: Epoch time: 68.18 s 
2025-07-11 18:25:43.207869:  
2025-07-11 18:25:43.209526: Epoch 97 
2025-07-11 18:25:43.210615: Current learning rate: 0.00912 
2025-07-11 18:26:51.453965: train_loss -0.9621 
2025-07-11 18:26:51.455296: val_loss -0.9488 
2025-07-11 18:26:51.456335: Pseudo dice [np.float32(0.9541)] 
2025-07-11 18:26:51.457672: Epoch time: 68.25 s 
2025-07-11 18:26:52.333196:  
2025-07-11 18:26:52.334955: Epoch 98 
2025-07-11 18:26:52.336120: Current learning rate: 0.00911 
2025-07-11 18:28:00.634833: train_loss -0.962 
2025-07-11 18:28:00.636078: val_loss -0.9464 
2025-07-11 18:28:00.637142: Pseudo dice [np.float32(0.9508)] 
2025-07-11 18:28:00.638377: Epoch time: 68.31 s 
2025-07-11 18:28:01.529001:  
2025-07-11 18:28:01.531076: Epoch 99 
2025-07-11 18:28:01.532268: Current learning rate: 0.0091 
2025-07-11 18:29:09.965385: train_loss -0.963 
2025-07-11 18:29:09.966842: val_loss -0.9479 
2025-07-11 18:29:09.967908: Pseudo dice [np.float32(0.9519)] 
2025-07-11 18:29:09.969129: Epoch time: 68.44 s 
2025-07-11 18:29:11.906568:  
2025-07-11 18:29:11.908142: Epoch 100 
2025-07-11 18:29:11.909304: Current learning rate: 0.0091 
2025-07-11 18:30:20.214038: train_loss -0.9623 
2025-07-11 18:30:20.215571: val_loss -0.9453 
2025-07-11 18:30:20.216735: Pseudo dice [np.float32(0.9503)] 
2025-07-11 18:30:20.218542: Epoch time: 68.31 s 
2025-07-11 18:30:21.349134:  
2025-07-11 18:30:21.350775: Epoch 101 
2025-07-11 18:30:21.351866: Current learning rate: 0.00909 
2025-07-11 18:31:29.543088: train_loss -0.9639 
2025-07-11 18:31:29.544357: val_loss -0.9516 
2025-07-11 18:31:29.545337: Pseudo dice [np.float32(0.956)] 
2025-07-11 18:31:29.546328: Epoch time: 68.2 s 
2025-07-11 18:31:30.416069:  
2025-07-11 18:31:30.417431: Epoch 102 
2025-07-11 18:31:30.418566: Current learning rate: 0.00908 
2025-07-11 18:32:38.503434: train_loss -0.9636 
2025-07-11 18:32:38.504882: val_loss -0.9486 
2025-07-11 18:32:38.506029: Pseudo dice [np.float32(0.9543)] 
2025-07-11 18:32:38.507119: Epoch time: 68.09 s 
2025-07-11 18:32:39.389426:  
2025-07-11 18:32:39.391167: Epoch 103 
2025-07-11 18:32:39.392275: Current learning rate: 0.00907 
2025-07-11 18:33:47.623692: train_loss -0.9644 
2025-07-11 18:33:47.625057: val_loss -0.9465 
2025-07-11 18:33:47.626066: Pseudo dice [np.float32(0.9509)] 
2025-07-11 18:33:47.626955: Epoch time: 68.24 s 
2025-07-11 18:33:48.508767:  
2025-07-11 18:33:48.510726: Epoch 104 
2025-07-11 18:33:48.511962: Current learning rate: 0.00906 
2025-07-11 18:34:56.888255: train_loss -0.9617 
2025-07-11 18:34:56.889893: val_loss -0.9476 
2025-07-11 18:34:56.891083: Pseudo dice [np.float32(0.9528)] 
2025-07-11 18:34:56.892035: Epoch time: 68.38 s 
2025-07-11 18:34:57.777519:  
2025-07-11 18:34:57.779459: Epoch 105 
2025-07-11 18:34:57.780538: Current learning rate: 0.00905 
2025-07-11 18:36:05.959324: train_loss -0.9636 
2025-07-11 18:36:05.960898: val_loss -0.9484 
2025-07-11 18:36:05.962188: Pseudo dice [np.float32(0.9527)] 
2025-07-11 18:36:05.963707: Epoch time: 68.19 s 
2025-07-11 18:36:06.841753:  
2025-07-11 18:36:06.843149: Epoch 106 
2025-07-11 18:36:06.844211: Current learning rate: 0.00904 
2025-07-11 18:37:14.943869: train_loss -0.9635 
2025-07-11 18:37:14.945156: val_loss -0.9493 
2025-07-11 18:37:14.946210: Pseudo dice [np.float32(0.9539)] 
2025-07-11 18:37:14.947285: Epoch time: 68.11 s 
2025-07-11 18:37:15.820952:  
2025-07-11 18:37:15.822789: Epoch 107 
2025-07-11 18:37:15.823956: Current learning rate: 0.00903 
2025-07-11 18:38:23.957941: train_loss -0.9649 
2025-07-11 18:38:23.959209: val_loss -0.946 
2025-07-11 18:38:23.960406: Pseudo dice [np.float32(0.9513)] 
2025-07-11 18:38:23.961586: Epoch time: 68.14 s 
2025-07-11 18:38:24.836277:  
2025-07-11 18:38:24.838027: Epoch 108 
2025-07-11 18:38:24.839804: Current learning rate: 0.00902 
2025-07-11 18:39:33.512190: train_loss -0.9636 
2025-07-11 18:39:33.513309: val_loss -0.9501 
2025-07-11 18:39:33.514234: Pseudo dice [np.float32(0.9556)] 
2025-07-11 18:39:33.515501: Epoch time: 68.68 s 
2025-07-11 18:39:33.516499: Yayy! New best EMA pseudo Dice: 0.9528999924659729 
2025-07-11 18:39:35.836118:  
2025-07-11 18:39:35.838001: Epoch 109 
2025-07-11 18:39:35.839260: Current learning rate: 0.00901 
2025-07-11 18:40:44.059261: train_loss -0.9621 
2025-07-11 18:40:44.060474: val_loss -0.9485 
2025-07-11 18:40:44.061689: Pseudo dice [np.float32(0.953)] 
2025-07-11 18:40:44.062980: Epoch time: 68.23 s 
2025-07-11 18:40:44.064113: Yayy! New best EMA pseudo Dice: 0.9528999924659729 
2025-07-11 18:40:46.195669:  
2025-07-11 18:40:46.196870: Epoch 110 
2025-07-11 18:40:46.198035: Current learning rate: 0.009 
2025-07-11 18:41:54.496612: train_loss -0.9628 
2025-07-11 18:41:54.497795: val_loss -0.9497 
2025-07-11 18:41:54.498896: Pseudo dice [np.float32(0.955)] 
2025-07-11 18:41:54.500345: Epoch time: 68.3 s 
2025-07-11 18:41:54.501589: Yayy! New best EMA pseudo Dice: 0.9531999826431274 
2025-07-11 18:41:56.631742:  
2025-07-11 18:41:56.633484: Epoch 111 
2025-07-11 18:41:56.634605: Current learning rate: 0.009 
2025-07-11 18:43:05.293784: train_loss -0.9633 
2025-07-11 18:43:05.295115: val_loss -0.9489 
2025-07-11 18:43:05.296162: Pseudo dice [np.float32(0.9544)] 
2025-07-11 18:43:05.297306: Epoch time: 68.67 s 
2025-07-11 18:43:05.298134: Yayy! New best EMA pseudo Dice: 0.9532999992370605 
2025-07-11 18:43:07.636397:  
2025-07-11 18:43:07.639131: Epoch 112 
2025-07-11 18:43:07.640254: Current learning rate: 0.00899 
2025-07-11 18:44:16.044642: train_loss -0.9635 
2025-07-11 18:44:16.046192: val_loss -0.9476 
2025-07-11 18:44:16.047082: Pseudo dice [np.float32(0.9528)] 
2025-07-11 18:44:16.048155: Epoch time: 68.41 s 
2025-07-11 18:44:16.928682:  
2025-07-11 18:44:16.930395: Epoch 113 
2025-07-11 18:44:16.931480: Current learning rate: 0.00898 
2025-07-11 18:45:25.188587: train_loss -0.9633 
2025-07-11 18:45:25.189905: val_loss -0.9496 
2025-07-11 18:45:25.190943: Pseudo dice [np.float32(0.9551)] 
2025-07-11 18:45:25.192374: Epoch time: 68.26 s 
2025-07-11 18:45:25.193378: Yayy! New best EMA pseudo Dice: 0.9534000158309937 
2025-07-11 18:45:27.544455:  
2025-07-11 18:45:27.546226: Epoch 114 
2025-07-11 18:45:27.547346: Current learning rate: 0.00897 
2025-07-11 18:46:36.029098: train_loss -0.9651 
2025-07-11 18:46:36.030292: val_loss -0.9484 
2025-07-11 18:46:36.031312: Pseudo dice [np.float32(0.9535)] 
2025-07-11 18:46:36.032432: Epoch time: 68.49 s 
2025-07-11 18:46:36.033383: Yayy! New best EMA pseudo Dice: 0.9534000158309937 
2025-07-11 18:46:38.337866:  
2025-07-11 18:46:38.339344: Epoch 115 
2025-07-11 18:46:38.340408: Current learning rate: 0.00896 
2025-07-11 18:47:46.482898: train_loss -0.9653 
2025-07-11 18:47:46.484164: val_loss -0.9507 
2025-07-11 18:47:46.485127: Pseudo dice [np.float32(0.9556)] 
2025-07-11 18:47:46.486215: Epoch time: 68.15 s 
2025-07-11 18:47:46.487149: Yayy! New best EMA pseudo Dice: 0.9535999894142151 
2025-07-11 18:47:48.590950:  
2025-07-11 18:47:48.592702: Epoch 116 
2025-07-11 18:47:48.593714: Current learning rate: 0.00895 
2025-07-11 18:48:56.816067: train_loss -0.9648 
2025-07-11 18:48:56.817475: val_loss -0.9486 
2025-07-11 18:48:56.818377: Pseudo dice [np.float32(0.9528)] 
2025-07-11 18:48:56.819711: Epoch time: 68.23 s 
2025-07-11 18:48:57.716210:  
2025-07-11 18:48:57.717820: Epoch 117 
2025-07-11 18:48:57.719017: Current learning rate: 0.00894 
2025-07-11 18:50:05.971658: train_loss -0.9643 
2025-07-11 18:50:05.972848: val_loss -0.9486 
2025-07-11 18:50:05.973881: Pseudo dice [np.float32(0.9534)] 
2025-07-11 18:50:05.975110: Epoch time: 68.26 s 
2025-07-11 18:50:06.867695:  
2025-07-11 18:50:06.869712: Epoch 118 
2025-07-11 18:50:06.870861: Current learning rate: 0.00893 
2025-07-11 18:51:15.010097: train_loss -0.965 
2025-07-11 18:51:15.011762: val_loss -0.9477 
2025-07-11 18:51:15.012653: Pseudo dice [np.float32(0.9525)] 
2025-07-11 18:51:15.013722: Epoch time: 68.15 s 
2025-07-11 18:51:15.902143:  
2025-07-11 18:51:15.903976: Epoch 119 
2025-07-11 18:51:15.905104: Current learning rate: 0.00892 
2025-07-11 18:52:24.000268: train_loss -0.9644 
2025-07-11 18:52:24.001593: val_loss -0.9488 
2025-07-11 18:52:24.002508: Pseudo dice [np.float32(0.9537)] 
2025-07-11 18:52:24.003551: Epoch time: 68.1 s 
2025-07-11 18:52:24.899224:  
2025-07-11 18:52:24.901152: Epoch 120 
2025-07-11 18:52:24.902154: Current learning rate: 0.00891 
2025-07-11 18:53:33.237823: train_loss -0.9646 
2025-07-11 18:53:33.239115: val_loss -0.9448 
2025-07-11 18:53:33.240124: Pseudo dice [np.float32(0.9504)] 
2025-07-11 18:53:33.241159: Epoch time: 68.34 s 
2025-07-11 18:53:34.138137:  
2025-07-11 18:53:34.139794: Epoch 121 
2025-07-11 18:53:34.140909: Current learning rate: 0.0089 
2025-07-11 18:54:42.394432: train_loss -0.9631 
2025-07-11 18:54:42.395623: val_loss -0.9485 
2025-07-11 18:54:42.396850: Pseudo dice [np.float32(0.9531)] 
2025-07-11 18:54:42.397873: Epoch time: 68.26 s 
2025-07-11 18:54:43.277471:  
2025-07-11 18:54:43.279137: Epoch 122 
2025-07-11 18:54:43.280113: Current learning rate: 0.00889 
2025-07-11 18:55:51.591695: train_loss -0.9629 
2025-07-11 18:55:51.592984: val_loss -0.9498 
2025-07-11 18:55:51.594253: Pseudo dice [np.float32(0.9547)] 
2025-07-11 18:55:51.595217: Epoch time: 68.32 s 
2025-07-11 18:55:52.492854:  
2025-07-11 18:55:52.494031: Epoch 123 
2025-07-11 18:55:52.495141: Current learning rate: 0.00889 
2025-07-11 18:57:00.873143: train_loss -0.9635 
2025-07-11 18:57:00.874396: val_loss -0.9482 
2025-07-11 18:57:00.875306: Pseudo dice [np.float32(0.952)] 
2025-07-11 18:57:00.876325: Epoch time: 68.38 s 
2025-07-11 18:57:01.756371:  
2025-07-11 18:57:01.758158: Epoch 124 
2025-07-11 18:57:01.759530: Current learning rate: 0.00888 
2025-07-11 18:58:10.599047: train_loss -0.9651 
2025-07-11 18:58:10.600416: val_loss -0.9508 
2025-07-11 18:58:10.601493: Pseudo dice [np.float32(0.9551)] 
2025-07-11 18:58:10.602544: Epoch time: 68.85 s 
2025-07-11 18:58:11.490807:  
2025-07-11 18:58:11.492567: Epoch 125 
2025-07-11 18:58:11.493706: Current learning rate: 0.00887 
2025-07-11 18:59:19.768153: train_loss -0.9648 
2025-07-11 18:59:19.769439: val_loss -0.9491 
2025-07-11 18:59:19.770636: Pseudo dice [np.float32(0.9538)] 
2025-07-11 18:59:19.771569: Epoch time: 68.28 s 
2025-07-11 18:59:20.672138:  
2025-07-11 18:59:20.673876: Epoch 126 
2025-07-11 18:59:20.675032: Current learning rate: 0.00886 
2025-07-11 19:00:28.833069: train_loss -0.9649 
2025-07-11 19:00:28.834444: val_loss -0.9486 
2025-07-11 19:00:28.835594: Pseudo dice [np.float32(0.9536)] 
2025-07-11 19:00:28.836843: Epoch time: 68.16 s 
2025-07-11 19:00:29.725255:  
2025-07-11 19:00:29.726667: Epoch 127 
2025-07-11 19:00:29.727884: Current learning rate: 0.00885 
2025-07-11 19:01:38.144460: train_loss -0.9635 
2025-07-11 19:01:38.145782: val_loss -0.9498 
2025-07-11 19:01:38.146745: Pseudo dice [np.float32(0.9543)] 
2025-07-11 19:01:38.147759: Epoch time: 68.42 s 
2025-07-11 19:01:39.046090:  
2025-07-11 19:01:39.047829: Epoch 128 
2025-07-11 19:01:39.048835: Current learning rate: 0.00884 
2025-07-11 19:02:47.624904: train_loss -0.9641 
2025-07-11 19:02:47.626086: val_loss -0.9491 
2025-07-11 19:02:47.627258: Pseudo dice [np.float32(0.9535)] 
2025-07-11 19:02:47.628233: Epoch time: 68.58 s 
2025-07-11 19:02:48.532154:  
2025-07-11 19:02:48.533858: Epoch 129 
2025-07-11 19:02:48.535042: Current learning rate: 0.00883 
2025-07-11 19:03:56.746114: train_loss -0.9641 
2025-07-11 19:03:56.747639: val_loss -0.9471 
2025-07-11 19:03:56.748924: Pseudo dice [np.float32(0.9521)] 
2025-07-11 19:03:56.749905: Epoch time: 68.22 s 
2025-07-11 19:03:57.646765:  
2025-07-11 19:03:57.648725: Epoch 130 
2025-07-11 19:03:57.649861: Current learning rate: 0.00882 
2025-07-11 19:05:05.684761: train_loss -0.963 
2025-07-11 19:05:05.686860: val_loss -0.9495 
2025-07-11 19:05:05.688151: Pseudo dice [np.float32(0.9537)] 
2025-07-11 19:05:05.689653: Epoch time: 68.04 s 
2025-07-11 19:05:06.597340:  
2025-07-11 19:05:06.598931: Epoch 131 
2025-07-11 19:05:06.600143: Current learning rate: 0.00881 
2025-07-11 19:06:14.923774: train_loss -0.9645 
2025-07-11 19:06:14.924977: val_loss -0.9493 
2025-07-11 19:06:14.926035: Pseudo dice [np.float32(0.9545)] 
2025-07-11 19:06:14.926951: Epoch time: 68.33 s 
2025-07-11 19:06:15.843687:  
2025-07-11 19:06:15.845112: Epoch 132 
2025-07-11 19:06:15.846153: Current learning rate: 0.0088 
2025-07-11 19:07:23.874399: train_loss -0.965 
2025-07-11 19:07:23.875689: val_loss -0.9504 
2025-07-11 19:07:23.876743: Pseudo dice [np.float32(0.9551)] 
2025-07-11 19:07:23.877697: Epoch time: 68.03 s 
2025-07-11 19:07:23.878712: Yayy! New best EMA pseudo Dice: 0.9537000060081482 
2025-07-11 19:07:26.179978:  
2025-07-11 19:07:26.181792: Epoch 133 
2025-07-11 19:07:26.182891: Current learning rate: 0.00879 
2025-07-11 19:08:34.130122: train_loss -0.9645 
2025-07-11 19:08:34.131258: val_loss -0.9499 
2025-07-11 19:08:34.132221: Pseudo dice [np.float32(0.9555)] 
2025-07-11 19:08:34.133430: Epoch time: 67.95 s 
2025-07-11 19:08:34.134398: Yayy! New best EMA pseudo Dice: 0.9538999795913696 
2025-07-11 19:08:36.266824:  
2025-07-11 19:08:36.268302: Epoch 134 
2025-07-11 19:08:36.269295: Current learning rate: 0.00879 
2025-07-11 19:09:44.334893: train_loss -0.9635 
2025-07-11 19:09:44.336280: val_loss -0.9483 
2025-07-11 19:09:44.337312: Pseudo dice [np.float32(0.9531)] 
2025-07-11 19:09:44.338479: Epoch time: 68.07 s 
2025-07-11 19:09:45.481710:  
2025-07-11 19:09:45.483315: Epoch 135 
2025-07-11 19:09:45.484541: Current learning rate: 0.00878 
2025-07-11 19:10:53.516908: train_loss -0.9646 
2025-07-11 19:10:53.518354: val_loss -0.9504 
2025-07-11 19:10:53.519318: Pseudo dice [np.float32(0.9558)] 
2025-07-11 19:10:53.520275: Epoch time: 68.04 s 
2025-07-11 19:10:53.521300: Yayy! New best EMA pseudo Dice: 0.9539999961853027 
2025-07-11 19:10:55.900317:  
2025-07-11 19:10:55.901909: Epoch 136 
2025-07-11 19:10:55.903209: Current learning rate: 0.00877 
2025-07-11 19:12:03.985476: train_loss -0.9631 
2025-07-11 19:12:03.986858: val_loss -0.9483 
2025-07-11 19:12:03.988091: Pseudo dice [np.float32(0.9528)] 
2025-07-11 19:12:03.989103: Epoch time: 68.09 s 
2025-07-11 19:12:04.898031:  
2025-07-11 19:12:04.899338: Epoch 137 
2025-07-11 19:12:04.900363: Current learning rate: 0.00876 
2025-07-11 19:13:12.998692: train_loss -0.9656 
2025-07-11 19:13:12.999938: val_loss -0.9484 
2025-07-11 19:13:13.000912: Pseudo dice [np.float32(0.9537)] 
2025-07-11 19:13:13.001989: Epoch time: 68.1 s 
2025-07-11 19:13:13.906610:  
2025-07-11 19:13:13.908208: Epoch 138 
2025-07-11 19:13:13.909214: Current learning rate: 0.00875 
2025-07-11 19:14:22.195997: train_loss -0.9639 
2025-07-11 19:14:22.197409: val_loss -0.9469 
2025-07-11 19:14:22.198676: Pseudo dice [np.float32(0.9525)] 
2025-07-11 19:14:22.199778: Epoch time: 68.29 s 
2025-07-11 19:14:23.107076:  
2025-07-11 19:14:23.108901: Epoch 139 
2025-07-11 19:14:23.109968: Current learning rate: 0.00874 
2025-07-11 19:15:31.270281: train_loss -0.9652 
2025-07-11 19:15:31.271578: val_loss -0.9518 
2025-07-11 19:15:31.272542: Pseudo dice [np.float32(0.9553)] 
2025-07-11 19:15:31.273418: Epoch time: 68.17 s 
2025-07-11 19:15:32.181191:  
2025-07-11 19:15:32.182923: Epoch 140 
2025-07-11 19:15:32.183911: Current learning rate: 0.00873 
2025-07-11 19:16:40.312336: train_loss -0.9643 
2025-07-11 19:16:40.313463: val_loss -0.9488 
2025-07-11 19:16:40.314474: Pseudo dice [np.float32(0.9539)] 
2025-07-11 19:16:40.315405: Epoch time: 68.13 s 
2025-07-11 19:16:41.221499:  
2025-07-11 19:16:41.223488: Epoch 141 
2025-07-11 19:16:41.224593: Current learning rate: 0.00872 
2025-07-11 19:17:49.554407: train_loss -0.9672 
2025-07-11 19:17:49.555767: val_loss -0.9511 
2025-07-11 19:17:49.556830: Pseudo dice [np.float32(0.9554)] 
2025-07-11 19:17:49.558034: Epoch time: 68.34 s 
2025-07-11 19:17:49.559172: Yayy! New best EMA pseudo Dice: 0.9539999961853027 
2025-07-11 19:17:51.875925:  
2025-07-11 19:17:51.877568: Epoch 142 
2025-07-11 19:17:51.878708: Current learning rate: 0.00871 
2025-07-11 19:18:59.970775: train_loss -0.9656 
2025-07-11 19:18:59.972009: val_loss -0.9542 
2025-07-11 19:18:59.973232: Pseudo dice [np.float32(0.9589)] 
2025-07-11 19:18:59.974351: Epoch time: 68.1 s 
2025-07-11 19:18:59.975563: Yayy! New best EMA pseudo Dice: 0.9545000195503235 
2025-07-11 19:19:02.120511:  
2025-07-11 19:19:02.122029: Epoch 143 
2025-07-11 19:19:02.123085: Current learning rate: 0.0087 
2025-07-11 19:20:10.404326: train_loss -0.9659 
2025-07-11 19:20:10.405519: val_loss -0.9495 
2025-07-11 19:20:10.406667: Pseudo dice [np.float32(0.9536)] 
2025-07-11 19:20:10.407733: Epoch time: 68.29 s 
2025-07-11 19:20:11.308662:  
2025-07-11 19:20:11.310437: Epoch 144 
2025-07-11 19:20:11.311635: Current learning rate: 0.00869 
2025-07-11 19:21:19.462496: train_loss -0.9662 
2025-07-11 19:21:19.463809: val_loss -0.948 
2025-07-11 19:21:19.464880: Pseudo dice [np.float32(0.9528)] 
2025-07-11 19:21:19.466075: Epoch time: 68.16 s 
2025-07-11 19:21:20.374802:  
2025-07-11 19:21:20.376544: Epoch 145 
2025-07-11 19:21:20.377718: Current learning rate: 0.00868 
2025-07-11 19:22:28.615883: train_loss -0.9655 
2025-07-11 19:22:28.617355: val_loss -0.9483 
2025-07-11 19:22:28.618601: Pseudo dice [np.float32(0.9529)] 
2025-07-11 19:22:28.619604: Epoch time: 68.24 s 
2025-07-11 19:22:29.521376:  
2025-07-11 19:22:29.522948: Epoch 146 
2025-07-11 19:22:29.524107: Current learning rate: 0.00868 
2025-07-11 19:23:37.834264: train_loss -0.964 
2025-07-11 19:23:37.835668: val_loss -0.9467 
2025-07-11 19:23:37.836669: Pseudo dice [np.float32(0.9515)] 
2025-07-11 19:23:37.837714: Epoch time: 68.32 s 
2025-07-11 19:23:38.734354:  
2025-07-11 19:23:38.736236: Epoch 147 
2025-07-11 19:23:38.737548: Current learning rate: 0.00867 
2025-07-11 19:24:46.940901: train_loss -0.964 
2025-07-11 19:24:46.942201: val_loss -0.9515 
2025-07-11 19:24:46.943230: Pseudo dice [np.float32(0.9552)] 
2025-07-11 19:24:46.944183: Epoch time: 68.21 s 
2025-07-11 19:24:47.853022:  
2025-07-11 19:24:47.854684: Epoch 148 
2025-07-11 19:24:47.855900: Current learning rate: 0.00866 
2025-07-11 19:25:56.180305: train_loss -0.9662 
2025-07-11 19:25:56.181492: val_loss -0.9505 
2025-07-11 19:25:56.182319: Pseudo dice [np.float32(0.9555)] 
2025-07-11 19:25:56.183306: Epoch time: 68.33 s 
2025-07-11 19:25:57.095348:  
2025-07-11 19:25:57.096991: Epoch 149 
2025-07-11 19:25:57.098085: Current learning rate: 0.00865 
2025-07-11 19:27:05.257180: train_loss -0.9673 
2025-07-11 19:27:05.258835: val_loss -0.9488 
2025-07-11 19:27:05.259850: Pseudo dice [np.float32(0.9533)] 
2025-07-11 19:27:05.260950: Epoch time: 68.17 s 
2025-07-11 19:27:07.432343:  
2025-07-11 19:27:07.433691: Epoch 150 
2025-07-11 19:27:07.434857: Current learning rate: 0.00864 
2025-07-11 19:28:15.616440: train_loss -0.9654 
2025-07-11 19:28:15.617754: val_loss -0.9499 
2025-07-11 19:28:15.618908: Pseudo dice [np.float32(0.9536)] 
2025-07-11 19:28:15.620131: Epoch time: 68.19 s 
2025-07-11 19:28:16.531241:  
2025-07-11 19:28:16.532871: Epoch 151 
2025-07-11 19:28:16.533963: Current learning rate: 0.00863 
2025-07-11 19:29:24.476424: train_loss -0.9657 
2025-07-11 19:29:24.477628: val_loss -0.9493 
2025-07-11 19:29:24.479128: Pseudo dice [np.float32(0.9538)] 
2025-07-11 19:29:24.480555: Epoch time: 67.95 s 
2025-07-11 19:29:25.638752:  
2025-07-11 19:29:25.640707: Epoch 152 
2025-07-11 19:29:25.641868: Current learning rate: 0.00862 
2025-07-11 19:30:33.822095: train_loss -0.9662 
2025-07-11 19:30:33.823466: val_loss -0.9505 
2025-07-11 19:30:33.824498: Pseudo dice [np.float32(0.9549)] 
2025-07-11 19:30:33.825482: Epoch time: 68.19 s 
2025-07-11 19:30:34.724736:  
2025-07-11 19:30:34.726717: Epoch 153 
2025-07-11 19:30:34.727831: Current learning rate: 0.00861 
2025-07-11 19:31:43.249447: train_loss -0.9651 
2025-07-11 19:31:43.250736: val_loss -0.9473 
2025-07-11 19:31:43.251753: Pseudo dice [np.float32(0.9521)] 
2025-07-11 19:31:43.252654: Epoch time: 68.53 s 
2025-07-11 19:31:44.166389:  
2025-07-11 19:31:44.167746: Epoch 154 
2025-07-11 19:31:44.168858: Current learning rate: 0.0086 
2025-07-11 19:32:52.254901: train_loss -0.9654 
2025-07-11 19:32:52.256463: val_loss -0.9495 
2025-07-11 19:32:52.257586: Pseudo dice [np.float32(0.9547)] 
2025-07-11 19:32:52.258518: Epoch time: 68.09 s 
2025-07-11 19:32:53.169650:  
2025-07-11 19:32:53.171362: Epoch 155 
2025-07-11 19:32:53.172480: Current learning rate: 0.00859 
2025-07-11 19:34:01.411960: train_loss -0.9652 
2025-07-11 19:34:01.413217: val_loss -0.9473 
2025-07-11 19:34:01.414403: Pseudo dice [np.float32(0.952)] 
2025-07-11 19:34:01.415330: Epoch time: 68.25 s 
2025-07-11 19:34:02.332798:  
2025-07-11 19:34:02.334687: Epoch 156 
2025-07-11 19:34:02.335853: Current learning rate: 0.00858 
2025-07-11 19:35:10.729893: train_loss -0.9643 
2025-07-11 19:35:10.731386: val_loss -0.9467 
2025-07-11 19:35:10.732360: Pseudo dice [np.float32(0.9508)] 
2025-07-11 19:35:10.733380: Epoch time: 68.4 s 
2025-07-11 19:35:11.650128:  
2025-07-11 19:35:11.651766: Epoch 157 
2025-07-11 19:35:11.652857: Current learning rate: 0.00858 
2025-07-11 19:36:19.547893: train_loss -0.9659 
2025-07-11 19:36:19.549097: val_loss -0.9492 
2025-07-11 19:36:19.550126: Pseudo dice [np.float32(0.9534)] 
2025-07-11 19:36:19.551052: Epoch time: 67.9 s 
2025-07-11 19:36:20.469930:  
2025-07-11 19:36:20.471789: Epoch 158 
2025-07-11 19:36:20.472982: Current learning rate: 0.00857 
2025-07-11 19:37:28.614773: train_loss -0.9669 
2025-07-11 19:37:28.616088: val_loss -0.9491 
2025-07-11 19:37:28.617103: Pseudo dice [np.float32(0.9532)] 
2025-07-11 19:37:28.618222: Epoch time: 68.15 s 
2025-07-11 19:37:29.531409:  
2025-07-11 19:37:29.532676: Epoch 159 
2025-07-11 19:37:29.533739: Current learning rate: 0.00856 
2025-07-11 19:38:37.679205: train_loss -0.9653 
2025-07-11 19:38:37.680345: val_loss -0.9494 
2025-07-11 19:38:37.681627: Pseudo dice [np.float32(0.9537)] 
2025-07-11 19:38:37.682588: Epoch time: 68.15 s 
2025-07-11 19:38:38.592858:  
2025-07-11 19:38:38.594589: Epoch 160 
2025-07-11 19:38:38.595688: Current learning rate: 0.00855 
2025-07-11 19:39:46.598312: train_loss -0.9664 
2025-07-11 19:39:46.599714: val_loss -0.9499 
2025-07-11 19:39:46.600908: Pseudo dice [np.float32(0.9544)] 
2025-07-11 19:39:46.601905: Epoch time: 68.01 s 
2025-07-11 19:39:47.494934:  
2025-07-11 19:39:47.496828: Epoch 161 
2025-07-11 19:39:47.497937: Current learning rate: 0.00854 
2025-07-11 19:40:55.561795: train_loss -0.9663 
2025-07-11 19:40:55.563268: val_loss -0.9504 
2025-07-11 19:40:55.564386: Pseudo dice [np.float32(0.9548)] 
2025-07-11 19:40:55.565418: Epoch time: 68.07 s 
2025-07-11 19:40:56.487283:  
2025-07-11 19:40:56.489137: Epoch 162 
2025-07-11 19:40:56.490300: Current learning rate: 0.00853 
2025-07-11 19:42:05.015403: train_loss -0.9661 
2025-07-11 19:42:05.016527: val_loss -0.9494 
2025-07-11 19:42:05.017695: Pseudo dice [np.float32(0.9529)] 
2025-07-11 19:42:05.018815: Epoch time: 68.53 s 
2025-07-11 19:42:05.998252:  
2025-07-11 19:42:06.000070: Epoch 163 
2025-07-11 19:42:06.001242: Current learning rate: 0.00852 
2025-07-11 19:43:14.234471: train_loss -0.9672 
2025-07-11 19:43:14.235845: val_loss -0.9496 
2025-07-11 19:43:14.236812: Pseudo dice [np.float32(0.954)] 
2025-07-11 19:43:14.237820: Epoch time: 68.24 s 
2025-07-11 19:43:15.163528:  
2025-07-11 19:43:15.165542: Epoch 164 
2025-07-11 19:43:15.166695: Current learning rate: 0.00851 
2025-07-11 19:44:23.468450: train_loss -0.9661 
2025-07-11 19:44:23.469665: val_loss -0.9525 
2025-07-11 19:44:23.471033: Pseudo dice [np.float32(0.9571)] 
2025-07-11 19:44:23.472624: Epoch time: 68.31 s 
2025-07-11 19:44:24.366188:  
2025-07-11 19:44:24.367989: Epoch 165 
2025-07-11 19:44:24.369115: Current learning rate: 0.0085 
2025-07-11 19:45:32.739800: train_loss -0.9676 
2025-07-11 19:45:32.741045: val_loss -0.95 
2025-07-11 19:45:32.742370: Pseudo dice [np.float32(0.9544)] 
2025-07-11 19:45:32.743309: Epoch time: 68.38 s 
2025-07-11 19:45:33.641102:  
2025-07-11 19:45:33.642825: Epoch 166 
2025-07-11 19:45:33.643860: Current learning rate: 0.00849 
2025-07-11 19:46:41.985873: train_loss -0.9669 
2025-07-11 19:46:41.987510: val_loss -0.9523 
2025-07-11 19:46:41.988698: Pseudo dice [np.float32(0.9568)] 
2025-07-11 19:46:41.989913: Epoch time: 68.35 s 
2025-07-11 19:46:42.872780:  
2025-07-11 19:46:42.874602: Epoch 167 
2025-07-11 19:46:42.875738: Current learning rate: 0.00848 
2025-07-11 19:47:51.093724: train_loss -0.9661 
2025-07-11 19:47:51.095339: val_loss -0.9488 
2025-07-11 19:47:51.096605: Pseudo dice [np.float32(0.9542)] 
2025-07-11 19:47:51.097801: Epoch time: 68.22 s 
2025-07-11 19:47:52.014459:  
2025-07-11 19:47:52.016706: Epoch 168 
2025-07-11 19:47:52.017777: Current learning rate: 0.00847 
2025-07-11 19:49:00.328007: train_loss -0.9664 
2025-07-11 19:49:00.330650: val_loss -0.9491 
2025-07-11 19:49:00.332180: Pseudo dice [np.float32(0.9543)] 
2025-07-11 19:49:00.333624: Epoch time: 68.32 s 
2025-07-11 19:49:01.247652:  
2025-07-11 19:49:01.249023: Epoch 169 
2025-07-11 19:49:01.250091: Current learning rate: 0.00847 
2025-07-11 19:50:09.635647: train_loss -0.9673 
2025-07-11 19:50:09.636757: val_loss -0.9501 
2025-07-11 19:50:09.637758: Pseudo dice [np.float32(0.9544)] 
2025-07-11 19:50:09.638776: Epoch time: 68.39 s 
2025-07-11 19:50:10.544089:  
2025-07-11 19:50:10.545863: Epoch 170 
2025-07-11 19:50:10.547018: Current learning rate: 0.00846 
2025-07-11 19:51:18.784673: train_loss -0.9681 
2025-07-11 19:51:18.786227: val_loss -0.9514 
2025-07-11 19:51:18.787438: Pseudo dice [np.float32(0.9555)] 
2025-07-11 19:51:18.788733: Epoch time: 68.24 s 
2025-07-11 19:51:19.703313:  
2025-07-11 19:51:19.705152: Epoch 171 
2025-07-11 19:51:19.706262: Current learning rate: 0.00845 
2025-07-11 19:52:27.853224: train_loss -0.9675 
2025-07-11 19:52:27.854578: val_loss -0.9501 
2025-07-11 19:52:27.855758: Pseudo dice [np.float32(0.9546)] 
2025-07-11 19:52:27.856635: Epoch time: 68.15 s 
2025-07-11 19:52:28.771641:  
2025-07-11 19:52:28.773460: Epoch 172 
2025-07-11 19:52:28.774549: Current learning rate: 0.00844 
2025-07-11 19:53:37.012564: train_loss -0.9666 
2025-07-11 19:53:37.013863: val_loss -0.9505 
2025-07-11 19:53:37.015165: Pseudo dice [np.float32(0.9548)] 
2025-07-11 19:53:37.016302: Epoch time: 68.24 s 
2025-07-11 19:53:37.928356:  
2025-07-11 19:53:37.930031: Epoch 173 
2025-07-11 19:53:37.931101: Current learning rate: 0.00843 
2025-07-11 19:54:46.218946: train_loss -0.9654 
2025-07-11 19:54:46.220220: val_loss -0.9508 
2025-07-11 19:54:46.221376: Pseudo dice [np.float32(0.9553)] 
2025-07-11 19:54:46.222323: Epoch time: 68.29 s 
2025-07-11 19:54:46.223229: Yayy! New best EMA pseudo Dice: 0.9545999765396118 
2025-07-11 19:54:48.387832:  
2025-07-11 19:54:48.389617: Epoch 174 
2025-07-11 19:54:48.390889: Current learning rate: 0.00842 
2025-07-11 19:55:56.435413: train_loss -0.9668 
2025-07-11 19:55:56.436680: val_loss -0.9485 
2025-07-11 19:55:56.437728: Pseudo dice [np.float32(0.9529)] 
2025-07-11 19:55:56.438646: Epoch time: 68.05 s 
2025-07-11 19:55:57.340251:  
2025-07-11 19:55:57.341822: Epoch 175 
2025-07-11 19:55:57.342983: Current learning rate: 0.00841 
2025-07-11 19:57:05.638510: train_loss -0.9668 
2025-07-11 19:57:05.639663: val_loss -0.9517 
2025-07-11 19:57:05.640550: Pseudo dice [np.float32(0.9563)] 
2025-07-11 19:57:05.641650: Epoch time: 68.3 s 
2025-07-11 19:57:05.642872: Yayy! New best EMA pseudo Dice: 0.9545999765396118 
2025-07-11 19:57:07.984633:  
2025-07-11 19:57:07.986209: Epoch 176 
2025-07-11 19:57:07.987262: Current learning rate: 0.0084 
2025-07-11 19:58:16.132474: train_loss -0.966 
2025-07-11 19:58:16.133787: val_loss -0.951 
2025-07-11 19:58:16.135010: Pseudo dice [np.float32(0.9559)] 
2025-07-11 19:58:16.135936: Epoch time: 68.15 s 
2025-07-11 19:58:16.137070: Yayy! New best EMA pseudo Dice: 0.9546999931335449 
2025-07-11 19:58:18.295497:  
2025-07-11 19:58:18.296584: Epoch 177 
2025-07-11 19:58:18.297513: Current learning rate: 0.00839 
2025-07-11 19:59:26.382351: train_loss -0.9672 
2025-07-11 19:59:26.383636: val_loss -0.9533 
2025-07-11 19:59:26.384803: Pseudo dice [np.float32(0.9571)] 
2025-07-11 19:59:26.385893: Epoch time: 68.09 s 
2025-07-11 19:59:26.387060: Yayy! New best EMA pseudo Dice: 0.9549999833106995 
2025-07-11 19:59:28.520637:  
2025-07-11 19:59:28.522040: Epoch 178 
2025-07-11 19:59:28.523197: Current learning rate: 0.00838 
2025-07-11 20:00:36.437540: train_loss -0.968 
2025-07-11 20:00:36.439037: val_loss -0.9521 
2025-07-11 20:00:36.440008: Pseudo dice [np.float32(0.9566)] 
2025-07-11 20:00:36.441086: Epoch time: 67.92 s 
2025-07-11 20:00:36.441965: Yayy! New best EMA pseudo Dice: 0.9550999999046326 
2025-07-11 20:00:38.828702:  
2025-07-11 20:00:38.830163: Epoch 179 
2025-07-11 20:00:38.831313: Current learning rate: 0.00837 
2025-07-11 20:01:47.105393: train_loss -0.9669 
2025-07-11 20:01:47.106573: val_loss -0.949 
2025-07-11 20:01:47.107511: Pseudo dice [np.float32(0.9535)] 
2025-07-11 20:01:47.108411: Epoch time: 68.28 s 
2025-07-11 20:01:48.015820:  
2025-07-11 20:01:48.017434: Epoch 180 
2025-07-11 20:01:48.018601: Current learning rate: 0.00836 
2025-07-11 20:02:56.300898: train_loss -0.9665 
2025-07-11 20:02:56.302332: val_loss -0.9498 
2025-07-11 20:02:56.303635: Pseudo dice [np.float32(0.9549)] 
2025-07-11 20:02:56.304687: Epoch time: 68.29 s 
2025-07-11 20:02:57.208535:  
2025-07-11 20:02:57.209961: Epoch 181 
2025-07-11 20:02:57.210894: Current learning rate: 0.00836 
2025-07-11 20:04:05.503255: train_loss -0.9652 
2025-07-11 20:04:05.504390: val_loss -0.9519 
2025-07-11 20:04:05.505406: Pseudo dice [np.float32(0.9557)] 
2025-07-11 20:04:05.506588: Epoch time: 68.3 s 
2025-07-11 20:04:06.421468:  
2025-07-11 20:04:06.423337: Epoch 182 
2025-07-11 20:04:06.424529: Current learning rate: 0.00835 
2025-07-11 20:05:14.754404: train_loss -0.967 
2025-07-11 20:05:14.755721: val_loss -0.9503 
2025-07-11 20:05:14.756837: Pseudo dice [np.float32(0.954)] 
2025-07-11 20:05:14.757680: Epoch time: 68.34 s 
2025-07-11 20:05:15.648860:  
2025-07-11 20:05:15.650229: Epoch 183 
2025-07-11 20:05:15.651405: Current learning rate: 0.00834 
2025-07-11 20:06:23.699313: train_loss -0.9666 
2025-07-11 20:06:23.700658: val_loss -0.9508 
2025-07-11 20:06:23.701638: Pseudo dice [np.float32(0.9552)] 
2025-07-11 20:06:23.702732: Epoch time: 68.05 s 
2025-07-11 20:06:24.610011:  
2025-07-11 20:06:24.611622: Epoch 184 
2025-07-11 20:06:24.612670: Current learning rate: 0.00833 
2025-07-11 20:07:32.611323: train_loss -0.9679 
2025-07-11 20:07:32.612576: val_loss -0.9473 
2025-07-11 20:07:32.613700: Pseudo dice [np.float32(0.9523)] 
2025-07-11 20:07:32.614853: Epoch time: 68.0 s 
2025-07-11 20:07:33.518493:  
2025-07-11 20:07:33.519702: Epoch 185 
2025-07-11 20:07:33.520683: Current learning rate: 0.00832 
2025-07-11 20:08:41.738827: train_loss -0.9676 
2025-07-11 20:08:41.739960: val_loss -0.9498 
2025-07-11 20:08:41.741206: Pseudo dice [np.float32(0.9546)] 
2025-07-11 20:08:41.742073: Epoch time: 68.22 s 
2025-07-11 20:08:42.652073:  
2025-07-11 20:08:42.653422: Epoch 186 
2025-07-11 20:08:42.654398: Current learning rate: 0.00831 
2025-07-11 20:09:50.693331: train_loss -0.9676 
2025-07-11 20:09:50.694752: val_loss -0.9491 
2025-07-11 20:09:50.695718: Pseudo dice [np.float32(0.9531)] 
2025-07-11 20:09:50.696632: Epoch time: 68.04 s 
2025-07-11 20:09:51.597842:  
2025-07-11 20:09:51.599618: Epoch 187 
2025-07-11 20:09:51.600687: Current learning rate: 0.0083 
2025-07-11 20:10:59.886084: train_loss -0.965 
2025-07-11 20:10:59.887739: val_loss -0.9512 
2025-07-11 20:10:59.888715: Pseudo dice [np.float32(0.9552)] 
2025-07-11 20:10:59.889891: Epoch time: 68.29 s 
2025-07-11 20:11:00.788518:  
2025-07-11 20:11:00.790077: Epoch 188 
2025-07-11 20:11:00.791238: Current learning rate: 0.00829 
2025-07-11 20:12:08.882597: train_loss -0.9677 
2025-07-11 20:12:08.883894: val_loss -0.9519 
2025-07-11 20:12:08.884923: Pseudo dice [np.float32(0.9563)] 
2025-07-11 20:12:08.886215: Epoch time: 68.1 s 
2025-07-11 20:12:09.789602:  
2025-07-11 20:12:09.791073: Epoch 189 
2025-07-11 20:12:09.792198: Current learning rate: 0.00828 
2025-07-11 20:13:18.020474: train_loss -0.968 
2025-07-11 20:13:18.021683: val_loss -0.9521 
2025-07-11 20:13:18.022953: Pseudo dice [np.float32(0.956)] 
2025-07-11 20:13:18.024048: Epoch time: 68.23 s 
2025-07-11 20:13:18.935537:  
2025-07-11 20:13:18.937181: Epoch 190 
2025-07-11 20:13:18.938391: Current learning rate: 0.00827 
2025-07-11 20:14:27.010911: train_loss -0.9667 
2025-07-11 20:14:27.012063: val_loss -0.9477 
2025-07-11 20:14:27.012994: Pseudo dice [np.float32(0.952)] 
2025-07-11 20:14:27.013936: Epoch time: 68.08 s 
2025-07-11 20:14:27.926222:  
2025-07-11 20:14:27.927843: Epoch 191 
2025-07-11 20:14:27.928857: Current learning rate: 0.00826 
2025-07-11 20:15:36.041517: train_loss -0.9674 
2025-07-11 20:15:36.042898: val_loss -0.951 
2025-07-11 20:15:36.044527: Pseudo dice [np.float32(0.9544)] 
2025-07-11 20:15:36.045586: Epoch time: 68.12 s 
2025-07-11 20:15:36.965992:  
2025-07-11 20:15:36.967646: Epoch 192 
2025-07-11 20:15:36.968828: Current learning rate: 0.00825 
2025-07-11 20:16:45.529216: train_loss -0.9678 
2025-07-11 20:16:45.530660: val_loss -0.9497 
2025-07-11 20:16:45.531695: Pseudo dice [np.float32(0.9543)] 
2025-07-11 20:16:45.532567: Epoch time: 68.57 s 
2025-07-11 20:16:46.441502:  
2025-07-11 20:16:46.442861: Epoch 193 
2025-07-11 20:16:46.444040: Current learning rate: 0.00824 
2025-07-11 20:17:54.597761: train_loss -0.968 
2025-07-11 20:17:54.599076: val_loss -0.9515 
2025-07-11 20:17:54.600063: Pseudo dice [np.float32(0.9565)] 
2025-07-11 20:17:54.601030: Epoch time: 68.16 s 
2025-07-11 20:17:55.518196:  
2025-07-11 20:17:55.519988: Epoch 194 
2025-07-11 20:17:55.521305: Current learning rate: 0.00824 
2025-07-11 20:19:03.678679: train_loss -0.9669 
2025-07-11 20:19:03.679845: val_loss -0.9526 
2025-07-11 20:19:03.680900: Pseudo dice [np.float32(0.957)] 
2025-07-11 20:19:03.682101: Epoch time: 68.16 s 
2025-07-11 20:19:04.603941:  
2025-07-11 20:19:04.605765: Epoch 195 
2025-07-11 20:19:04.606789: Current learning rate: 0.00823 
2025-07-11 20:20:13.035404: train_loss -0.968 
2025-07-11 20:20:13.036705: val_loss -0.9515 
2025-07-11 20:20:13.037848: Pseudo dice [np.float32(0.9563)] 
2025-07-11 20:20:13.038891: Epoch time: 68.43 s 
2025-07-11 20:20:14.189312:  
2025-07-11 20:20:14.191036: Epoch 196 
2025-07-11 20:20:14.192172: Current learning rate: 0.00822 
2025-07-11 20:21:22.561877: train_loss -0.9667 
2025-07-11 20:21:22.563124: val_loss -0.95 
2025-07-11 20:21:22.564201: Pseudo dice [np.float32(0.9548)] 
2025-07-11 20:21:22.565320: Epoch time: 68.38 s 
2025-07-11 20:21:23.483266:  
2025-07-11 20:21:23.485140: Epoch 197 
2025-07-11 20:21:23.486402: Current learning rate: 0.00821 
2025-07-11 20:22:31.807390: train_loss -0.9683 
2025-07-11 20:22:31.808540: val_loss -0.9514 
2025-07-11 20:22:31.809520: Pseudo dice [np.float32(0.9561)] 
2025-07-11 20:22:31.810798: Epoch time: 68.33 s 
2025-07-11 20:22:31.811913: Yayy! New best EMA pseudo Dice: 0.9552000164985657 
2025-07-11 20:22:33.974931:  
2025-07-11 20:22:33.976682: Epoch 198 
2025-07-11 20:22:33.977864: Current learning rate: 0.0082 
2025-07-11 20:23:42.330839: train_loss -0.9662 
2025-07-11 20:23:42.332006: val_loss -0.9519 
2025-07-11 20:23:42.333182: Pseudo dice [np.float32(0.9563)] 
2025-07-11 20:23:42.334338: Epoch time: 68.36 s 
2025-07-11 20:23:42.335430: Yayy! New best EMA pseudo Dice: 0.955299973487854 
2025-07-11 20:23:44.489225:  
2025-07-11 20:23:44.490916: Epoch 199 
2025-07-11 20:23:44.492120: Current learning rate: 0.00819 
2025-07-11 20:24:52.997484: train_loss -0.9671 
2025-07-11 20:24:52.998707: val_loss -0.9487 
2025-07-11 20:24:52.999664: Pseudo dice [np.float32(0.9541)] 
2025-07-11 20:24:53.000849: Epoch time: 68.51 s 
2025-07-11 20:24:55.153376:  
2025-07-11 20:24:55.155077: Epoch 200 
2025-07-11 20:24:55.156286: Current learning rate: 0.00818 
2025-07-11 20:26:03.506417: train_loss -0.9648 
2025-07-11 20:26:03.507825: val_loss -0.9502 
2025-07-11 20:26:03.508890: Pseudo dice [np.float32(0.9561)] 
2025-07-11 20:26:03.510029: Epoch time: 68.36 s 
2025-07-11 20:26:04.420986:  
2025-07-11 20:26:04.422891: Epoch 201 
2025-07-11 20:26:04.424227: Current learning rate: 0.00817 
2025-07-11 20:27:12.484636: train_loss -0.9647 
2025-07-11 20:27:12.485884: val_loss -0.9471 
2025-07-11 20:27:12.487125: Pseudo dice [np.float32(0.9517)] 
2025-07-11 20:27:12.488092: Epoch time: 68.07 s 
2025-07-11 20:27:13.401835:  
2025-07-11 20:27:13.403140: Epoch 202 
2025-07-11 20:27:13.404241: Current learning rate: 0.00816 
2025-07-11 20:28:21.626891: train_loss -0.9663 
2025-07-11 20:28:21.628047: val_loss -0.9487 
2025-07-11 20:28:21.628959: Pseudo dice [np.float32(0.9529)] 
2025-07-11 20:28:21.630175: Epoch time: 68.23 s 
2025-07-11 20:28:22.546572:  
2025-07-11 20:28:22.548221: Epoch 203 
2025-07-11 20:28:22.549280: Current learning rate: 0.00815 
2025-07-11 20:29:30.659301: train_loss -0.9681 
2025-07-11 20:29:30.660525: val_loss -0.9497 
2025-07-11 20:29:30.661690: Pseudo dice [np.float32(0.9533)] 
2025-07-11 20:29:30.662817: Epoch time: 68.12 s 
2025-07-11 20:29:31.581160:  
2025-07-11 20:29:31.582963: Epoch 204 
2025-07-11 20:29:31.584186: Current learning rate: 0.00814 
2025-07-11 20:30:39.576610: train_loss -0.9683 
2025-07-11 20:30:39.577672: val_loss -0.9546 
2025-07-11 20:30:39.578614: Pseudo dice [np.float32(0.9588)] 
2025-07-11 20:30:39.579715: Epoch time: 68.0 s 
2025-07-11 20:30:40.493613:  
2025-07-11 20:30:40.495396: Epoch 205 
2025-07-11 20:30:40.496518: Current learning rate: 0.00813 
2025-07-11 20:31:48.580654: train_loss -0.9676 
2025-07-11 20:31:48.582210: val_loss -0.9512 
2025-07-11 20:31:48.583240: Pseudo dice [np.float32(0.9559)] 
2025-07-11 20:31:48.584211: Epoch time: 68.09 s 
2025-07-11 20:31:49.699103:  
2025-07-11 20:31:49.700477: Epoch 206 
2025-07-11 20:31:49.701688: Current learning rate: 0.00813 
2025-07-11 20:32:57.989880: train_loss -0.9674 
2025-07-11 20:32:57.991324: val_loss -0.95 
2025-07-11 20:32:57.992298: Pseudo dice [np.float32(0.9546)] 
2025-07-11 20:32:57.993436: Epoch time: 68.29 s 
2025-07-11 20:32:58.875209:  
2025-07-11 20:32:58.876829: Epoch 207 
2025-07-11 20:32:58.877946: Current learning rate: 0.00812 
2025-07-11 20:34:07.271291: train_loss -0.9659 
2025-07-11 20:34:07.272899: val_loss -0.9496 
2025-07-11 20:34:07.273896: Pseudo dice [np.float32(0.9547)] 
2025-07-11 20:34:07.274906: Epoch time: 68.4 s 
2025-07-11 20:34:08.153665:  
2025-07-11 20:34:08.155520: Epoch 208 
2025-07-11 20:34:08.156643: Current learning rate: 0.00811 
2025-07-11 20:35:16.280798: train_loss -0.9673 
2025-07-11 20:35:16.282094: val_loss -0.9457 
2025-07-11 20:35:16.282985: Pseudo dice [np.float32(0.9502)] 
2025-07-11 20:35:16.284054: Epoch time: 68.13 s 
2025-07-11 20:35:17.166015:  
2025-07-11 20:35:17.167779: Epoch 209 
2025-07-11 20:35:17.168965: Current learning rate: 0.0081 
2025-07-11 20:36:25.416876: train_loss -0.9663 
2025-07-11 20:36:25.418217: val_loss -0.9484 
2025-07-11 20:36:25.419076: Pseudo dice [np.float32(0.9526)] 
2025-07-11 20:36:25.420226: Epoch time: 68.25 s 
2025-07-11 20:36:26.298744:  
2025-07-11 20:36:26.300775: Epoch 210 
2025-07-11 20:36:26.302163: Current learning rate: 0.00809 
2025-07-11 20:37:34.433478: train_loss -0.9674 
2025-07-11 20:37:34.434541: val_loss -0.9501 
2025-07-11 20:37:34.435550: Pseudo dice [np.float32(0.9541)] 
2025-07-11 20:37:34.436728: Epoch time: 68.14 s 
2025-07-11 20:37:35.302908:  
2025-07-11 20:37:35.304579: Epoch 211 
2025-07-11 20:37:35.305667: Current learning rate: 0.00808 
2025-07-11 20:38:43.468957: train_loss -0.9676 
2025-07-11 20:38:43.470202: val_loss -0.9485 
2025-07-11 20:38:43.471181: Pseudo dice [np.float32(0.9521)] 
2025-07-11 20:38:43.472240: Epoch time: 68.17 s 
2025-07-11 20:38:44.366511:  
2025-07-11 20:38:44.368013: Epoch 212 
2025-07-11 20:38:44.369320: Current learning rate: 0.00807 
2025-07-11 20:39:52.565197: train_loss -0.9674 
2025-07-11 20:39:52.566455: val_loss -0.9469 
2025-07-11 20:39:52.567530: Pseudo dice [np.float32(0.9509)] 
2025-07-11 20:39:52.568721: Epoch time: 68.2 s 
2025-07-11 20:39:53.453160:  
2025-07-11 20:39:53.454779: Epoch 213 
2025-07-11 20:39:53.455907: Current learning rate: 0.00806 
2025-07-11 20:41:01.811284: train_loss -0.9662 
2025-07-11 20:41:01.812512: val_loss -0.9543 
2025-07-11 20:41:01.813703: Pseudo dice [np.float32(0.9577)] 
2025-07-11 20:41:01.814776: Epoch time: 68.36 s 
2025-07-11 20:41:02.701493:  
2025-07-11 20:41:02.703064: Epoch 214 
2025-07-11 20:41:02.704566: Current learning rate: 0.00805 
2025-07-11 20:42:10.777951: train_loss -0.9679 
2025-07-11 20:42:10.779389: val_loss -0.9483 
2025-07-11 20:42:10.780609: Pseudo dice [np.float32(0.9526)] 
2025-07-11 20:42:10.781571: Epoch time: 68.08 s 
2025-07-11 20:42:11.657397:  
2025-07-11 20:42:11.658966: Epoch 215 
2025-07-11 20:42:11.660027: Current learning rate: 0.00804 
2025-07-11 20:43:19.752695: train_loss -0.9677 
2025-07-11 20:43:19.754158: val_loss -0.9488 
2025-07-11 20:43:19.755152: Pseudo dice [np.float32(0.9534)] 
2025-07-11 20:43:19.756122: Epoch time: 68.1 s 
2025-07-11 20:43:20.624520:  
2025-07-11 20:43:20.626096: Epoch 216 
2025-07-11 20:43:20.627298: Current learning rate: 0.00803 
2025-07-11 20:44:28.850298: train_loss -0.9692 
2025-07-11 20:44:28.851554: val_loss -0.9525 
2025-07-11 20:44:28.852913: Pseudo dice [np.float32(0.9567)] 
2025-07-11 20:44:28.853981: Epoch time: 68.23 s 
2025-07-11 20:44:29.756321:  
2025-07-11 20:44:29.757810: Epoch 217 
2025-07-11 20:44:29.758962: Current learning rate: 0.00802 
2025-07-11 20:45:37.792804: train_loss -0.9694 
2025-07-11 20:45:37.794050: val_loss -0.9496 
2025-07-11 20:45:37.794963: Pseudo dice [np.float32(0.9538)] 
2025-07-11 20:45:37.796167: Epoch time: 68.04 s 
2025-07-11 20:45:38.667052:  
2025-07-11 20:45:38.668759: Epoch 218 
2025-07-11 20:45:38.669991: Current learning rate: 0.00801 
2025-07-11 20:46:46.642823: train_loss -0.9687 
2025-07-11 20:46:46.644281: val_loss -0.9496 
2025-07-11 20:46:46.645224: Pseudo dice [np.float32(0.9538)] 
2025-07-11 20:46:46.646294: Epoch time: 67.98 s 
2025-07-11 20:46:47.531119:  
2025-07-11 20:46:47.532981: Epoch 219 
2025-07-11 20:46:47.534323: Current learning rate: 0.00801 
2025-07-11 20:47:55.693169: train_loss -0.9688 
2025-07-11 20:47:55.694925: val_loss -0.9518 
2025-07-11 20:47:55.695968: Pseudo dice [np.float32(0.9552)] 
2025-07-11 20:47:55.696920: Epoch time: 68.17 s 
2025-07-11 20:47:56.626756:  
2025-07-11 20:47:56.628848: Epoch 220 
2025-07-11 20:47:56.630163: Current learning rate: 0.008 
2025-07-11 20:49:04.955416: train_loss -0.9683 
2025-07-11 20:49:04.956611: val_loss -0.9511 
2025-07-11 20:49:04.957691: Pseudo dice [np.float32(0.9553)] 
2025-07-11 20:49:04.958723: Epoch time: 68.33 s 
2025-07-11 20:49:05.847355:  
2025-07-11 20:49:05.849207: Epoch 221 
2025-07-11 20:49:05.850415: Current learning rate: 0.00799 
2025-07-11 20:50:13.947461: train_loss -0.969 
2025-07-11 20:50:13.948962: val_loss -0.9521 
2025-07-11 20:50:13.949966: Pseudo dice [np.float32(0.9562)] 
2025-07-11 20:50:13.951104: Epoch time: 68.1 s 
2025-07-11 20:50:14.839428:  
2025-07-11 20:50:14.840742: Epoch 222 
2025-07-11 20:50:14.841818: Current learning rate: 0.00798 
2025-07-11 20:51:22.825681: train_loss -0.9682 
2025-07-11 20:51:22.826938: val_loss -0.9533 
2025-07-11 20:51:22.827948: Pseudo dice [np.float32(0.9573)] 
2025-07-11 20:51:22.828962: Epoch time: 67.99 s 
2025-07-11 20:51:23.745377:  
2025-07-11 20:51:23.746733: Epoch 223 
2025-07-11 20:51:23.747908: Current learning rate: 0.00797 
2025-07-11 20:52:32.063370: train_loss -0.9664 
2025-07-11 20:52:32.064572: val_loss -0.9523 
2025-07-11 20:52:32.065497: Pseudo dice [np.float32(0.9563)] 
2025-07-11 20:52:32.066549: Epoch time: 68.32 s 
2025-07-11 20:52:32.964133:  
2025-07-11 20:52:32.966009: Epoch 224 
2025-07-11 20:52:32.966926: Current learning rate: 0.00796 
2025-07-11 20:53:41.255626: train_loss -0.9675 
2025-07-11 20:53:41.256870: val_loss -0.9545 
2025-07-11 20:53:41.257991: Pseudo dice [np.float32(0.958)] 
2025-07-11 20:53:41.259243: Epoch time: 68.3 s 
2025-07-11 20:53:42.146731:  
2025-07-11 20:53:42.148528: Epoch 225 
2025-07-11 20:53:42.149716: Current learning rate: 0.00795 
2025-07-11 20:54:50.216325: train_loss -0.9687 
2025-07-11 20:54:50.218426: val_loss -0.9503 
2025-07-11 20:54:50.219287: Pseudo dice [np.float32(0.955)] 
2025-07-11 20:54:50.220658: Epoch time: 68.07 s 
2025-07-11 20:54:51.108021:  
2025-07-11 20:54:51.109539: Epoch 226 
2025-07-11 20:54:51.110551: Current learning rate: 0.00794 
2025-07-11 20:55:59.153068: train_loss -0.9676 
2025-07-11 20:55:59.154594: val_loss -0.9509 
2025-07-11 20:55:59.155985: Pseudo dice [np.float32(0.9552)] 
2025-07-11 20:55:59.157212: Epoch time: 68.05 s 
2025-07-11 20:56:00.030632:  
2025-07-11 20:56:00.032076: Epoch 227 
2025-07-11 20:56:00.033564: Current learning rate: 0.00793 
2025-07-11 20:57:08.310259: train_loss -0.9689 
2025-07-11 20:57:08.311660: val_loss -0.9517 
2025-07-11 20:57:08.312609: Pseudo dice [np.float32(0.9561)] 
2025-07-11 20:57:08.313679: Epoch time: 68.28 s 
2025-07-11 20:57:08.314691: Yayy! New best EMA pseudo Dice: 0.955299973487854 
2025-07-11 20:57:10.429159:  
2025-07-11 20:57:10.430735: Epoch 228 
2025-07-11 20:57:10.431890: Current learning rate: 0.00792 
2025-07-11 20:58:18.486318: train_loss -0.9696 
2025-07-11 20:58:18.487746: val_loss -0.9523 
2025-07-11 20:58:18.488634: Pseudo dice [np.float32(0.9561)] 
2025-07-11 20:58:18.489708: Epoch time: 68.06 s 
2025-07-11 20:58:18.490631: Yayy! New best EMA pseudo Dice: 0.9553999900817871 
2025-07-11 20:58:20.588735:  
2025-07-11 20:58:20.590030: Epoch 229 
2025-07-11 20:58:20.591108: Current learning rate: 0.00791 
2025-07-11 20:59:28.607455: train_loss -0.9682 
2025-07-11 20:59:28.608760: val_loss -0.9536 
2025-07-11 20:59:28.609796: Pseudo dice [np.float32(0.9575)] 
2025-07-11 20:59:28.610631: Epoch time: 68.02 s 
2025-07-11 20:59:28.611685: Yayy! New best EMA pseudo Dice: 0.9556000232696533 
2025-07-11 20:59:30.694692:  
2025-07-11 20:59:30.696150: Epoch 230 
2025-07-11 20:59:30.697258: Current learning rate: 0.0079 
2025-07-11 21:00:38.802166: train_loss -0.9687 
2025-07-11 21:00:38.803290: val_loss -0.9484 
2025-07-11 21:00:38.804210: Pseudo dice [np.float32(0.9529)] 
2025-07-11 21:00:38.805133: Epoch time: 68.11 s 
2025-07-11 21:00:39.662326:  
2025-07-11 21:00:39.663727: Epoch 231 
2025-07-11 21:00:39.664774: Current learning rate: 0.00789 
2025-07-11 21:01:47.688960: train_loss -0.9669 
2025-07-11 21:01:47.690250: val_loss -0.9479 
2025-07-11 21:01:47.691787: Pseudo dice [np.float32(0.953)] 
2025-07-11 21:01:47.692997: Epoch time: 68.03 s 
2025-07-11 21:01:48.548450:  
2025-07-11 21:01:48.549861: Epoch 232 
2025-07-11 21:01:48.551002: Current learning rate: 0.00789 
2025-07-11 21:02:56.459873: train_loss -0.9639 
2025-07-11 21:02:56.460989: val_loss -0.951 
2025-07-11 21:02:56.461891: Pseudo dice [np.float32(0.9556)] 
2025-07-11 21:02:56.463537: Epoch time: 67.91 s 
2025-07-11 21:02:57.327438:  
2025-07-11 21:02:57.329302: Epoch 233 
2025-07-11 21:02:57.330428: Current learning rate: 0.00788 
2025-07-11 21:04:05.358659: train_loss -0.9673 
2025-07-11 21:04:05.359783: val_loss -0.951 
2025-07-11 21:04:05.360752: Pseudo dice [np.float32(0.9552)] 
2025-07-11 21:04:05.361795: Epoch time: 68.03 s 
2025-07-11 21:04:06.247705:  
2025-07-11 21:04:06.249503: Epoch 234 
2025-07-11 21:04:06.250620: Current learning rate: 0.00787 
2025-07-11 21:05:14.649612: train_loss -0.9662 
2025-07-11 21:05:14.651135: val_loss -0.9487 
2025-07-11 21:05:14.652481: Pseudo dice [np.float32(0.9535)] 
2025-07-11 21:05:14.653562: Epoch time: 68.41 s 
2025-07-11 21:05:15.521214:  
2025-07-11 21:05:15.522606: Epoch 235 
2025-07-11 21:05:15.523738: Current learning rate: 0.00786 
2025-07-11 21:06:23.797154: train_loss -0.9674 
2025-07-11 21:06:23.798445: val_loss -0.9524 
2025-07-11 21:06:23.799714: Pseudo dice [np.float32(0.9562)] 
2025-07-11 21:06:23.800636: Epoch time: 68.28 s 
2025-07-11 21:06:24.667222:  
2025-07-11 21:06:24.668957: Epoch 236 
2025-07-11 21:06:24.670025: Current learning rate: 0.00785 
2025-07-11 21:07:32.702213: train_loss -0.9689 
2025-07-11 21:07:32.703335: val_loss -0.9484 
2025-07-11 21:07:32.704398: Pseudo dice [np.float32(0.9526)] 
2025-07-11 21:07:32.705407: Epoch time: 68.04 s 
2025-07-11 21:07:33.576977:  
2025-07-11 21:07:33.578741: Epoch 237 
2025-07-11 21:07:33.579858: Current learning rate: 0.00784 
2025-07-11 21:08:41.935149: train_loss -0.9664 
2025-07-11 21:08:41.936422: val_loss -0.9496 
2025-07-11 21:08:41.937452: Pseudo dice [np.float32(0.9545)] 
2025-07-11 21:08:41.938602: Epoch time: 68.36 s 
2025-07-11 21:08:42.812495:  
2025-07-11 21:08:42.814335: Epoch 238 
2025-07-11 21:08:42.815452: Current learning rate: 0.00783 
2025-07-11 21:09:50.971017: train_loss -0.9657 
2025-07-11 21:09:50.972313: val_loss -0.9497 
2025-07-11 21:09:50.973249: Pseudo dice [np.float32(0.9544)] 
2025-07-11 21:09:50.974230: Epoch time: 68.16 s 
2025-07-11 21:09:51.835407:  
2025-07-11 21:09:51.836994: Epoch 239 
2025-07-11 21:09:51.838107: Current learning rate: 0.00782 
2025-07-11 21:10:59.897599: train_loss -0.9667 
2025-07-11 21:10:59.898970: val_loss -0.9519 
2025-07-11 21:10:59.900016: Pseudo dice [np.float32(0.9553)] 
2025-07-11 21:10:59.901143: Epoch time: 68.07 s 
2025-07-11 21:11:00.772906:  
2025-07-11 21:11:00.774632: Epoch 240 
2025-07-11 21:11:00.775765: Current learning rate: 0.00781 
2025-07-11 21:12:08.964182: train_loss -0.9677 
2025-07-11 21:12:08.965455: val_loss -0.9504 
2025-07-11 21:12:08.966721: Pseudo dice [np.float32(0.9543)] 
2025-07-11 21:12:08.968030: Epoch time: 68.19 s 
2025-07-11 21:12:09.835605:  
2025-07-11 21:12:09.837118: Epoch 241 
2025-07-11 21:12:09.838224: Current learning rate: 0.0078 
2025-07-11 21:13:18.053625: train_loss -0.9684 
2025-07-11 21:13:18.054825: val_loss -0.9502 
2025-07-11 21:13:18.055771: Pseudo dice [np.float32(0.9541)] 
2025-07-11 21:13:18.056827: Epoch time: 68.22 s 
2025-07-11 21:13:18.926740:  
2025-07-11 21:13:18.928539: Epoch 242 
2025-07-11 21:13:18.929561: Current learning rate: 0.00779 
2025-07-11 21:14:27.101528: train_loss -0.9674 
2025-07-11 21:14:27.102664: val_loss -0.9523 
2025-07-11 21:14:27.103694: Pseudo dice [np.float32(0.9563)] 
2025-07-11 21:14:27.104698: Epoch time: 68.18 s 
2025-07-11 21:14:27.982000:  
2025-07-11 21:14:27.983783: Epoch 243 
2025-07-11 21:14:27.984936: Current learning rate: 0.00778 
2025-07-11 21:15:36.415904: train_loss -0.9671 
2025-07-11 21:15:36.417194: val_loss -0.9519 
2025-07-11 21:15:36.418171: Pseudo dice [np.float32(0.9565)] 
2025-07-11 21:15:36.419070: Epoch time: 68.44 s 
2025-07-11 21:15:37.284306:  
2025-07-11 21:15:37.286023: Epoch 244 
2025-07-11 21:15:37.287143: Current learning rate: 0.00777 
2025-07-11 21:16:45.866070: train_loss -0.9696 
2025-07-11 21:16:45.867562: val_loss -0.9478 
2025-07-11 21:16:45.868660: Pseudo dice [np.float32(0.9524)] 
2025-07-11 21:16:45.869615: Epoch time: 68.59 s 
2025-07-11 21:16:46.731898:  
2025-07-11 21:16:46.733290: Epoch 245 
2025-07-11 21:16:46.734287: Current learning rate: 0.00777 
2025-07-11 21:17:54.846062: train_loss -0.968 
2025-07-11 21:17:54.847580: val_loss -0.9506 
2025-07-11 21:17:54.848545: Pseudo dice [np.float32(0.9552)] 
2025-07-11 21:17:54.849616: Epoch time: 68.12 s 
2025-07-11 21:17:55.724556:  
2025-07-11 21:17:55.726241: Epoch 246 
2025-07-11 21:17:55.727376: Current learning rate: 0.00776 
2025-07-11 21:19:03.796942: train_loss -0.9708 
2025-07-11 21:19:03.798334: val_loss -0.9478 
2025-07-11 21:19:03.799441: Pseudo dice [np.float32(0.9519)] 
2025-07-11 21:19:03.800454: Epoch time: 68.08 s 
2025-07-11 21:19:04.679188:  
2025-07-11 21:19:04.681049: Epoch 247 
2025-07-11 21:19:04.682201: Current learning rate: 0.00775 
2025-07-11 21:20:12.952814: train_loss -0.9687 
2025-07-11 21:20:12.954174: val_loss -0.9513 
2025-07-11 21:20:12.955127: Pseudo dice [np.float32(0.9556)] 
2025-07-11 21:20:12.956280: Epoch time: 68.28 s 
2025-07-11 21:20:13.825840:  
2025-07-11 21:20:13.827333: Epoch 248 
2025-07-11 21:20:13.828407: Current learning rate: 0.00774 
2025-07-11 21:21:22.045914: train_loss -0.9696 
2025-07-11 21:21:22.047259: val_loss -0.9517 
2025-07-11 21:21:22.048270: Pseudo dice [np.float32(0.9557)] 
2025-07-11 21:21:22.049193: Epoch time: 68.22 s 
2025-07-11 21:21:22.925057:  
2025-07-11 21:21:22.926820: Epoch 249 
2025-07-11 21:21:22.927824: Current learning rate: 0.00773 
2025-07-11 21:22:31.153745: train_loss -0.9689 
2025-07-11 21:22:31.154984: val_loss -0.9495 
2025-07-11 21:22:31.156105: Pseudo dice [np.float32(0.9543)] 
2025-07-11 21:22:31.157022: Epoch time: 68.23 s 
2025-07-11 21:22:33.082234:  
2025-07-11 21:22:33.083858: Epoch 250 
2025-07-11 21:22:33.085002: Current learning rate: 0.00772 
2025-07-11 21:23:41.140601: train_loss -0.9704 
2025-07-11 21:23:41.142073: val_loss -0.9502 
2025-07-11 21:23:41.143317: Pseudo dice [np.float32(0.9547)] 
2025-07-11 21:23:41.144171: Epoch time: 68.06 s 
2025-07-11 21:23:42.251306:  
2025-07-11 21:23:42.252992: Epoch 251 
2025-07-11 21:23:42.254028: Current learning rate: 0.00771 
2025-07-11 21:24:50.385311: train_loss -0.9686 
2025-07-11 21:24:50.386537: val_loss -0.9495 
2025-07-11 21:24:50.387549: Pseudo dice [np.float32(0.9542)] 
2025-07-11 21:24:50.388623: Epoch time: 68.14 s 
2025-07-11 21:24:51.261089:  
2025-07-11 21:24:51.262703: Epoch 252 
2025-07-11 21:24:51.263712: Current learning rate: 0.0077 
2025-07-11 21:25:59.412292: train_loss -0.969 
2025-07-11 21:25:59.413979: val_loss -0.9501 
2025-07-11 21:25:59.415174: Pseudo dice [np.float32(0.9548)] 
2025-07-11 21:25:59.416309: Epoch time: 68.15 s 
2025-07-11 21:26:00.279417:  
2025-07-11 21:26:00.281084: Epoch 253 
2025-07-11 21:26:00.282298: Current learning rate: 0.00769 
2025-07-11 21:27:08.421115: train_loss -0.9699 
2025-07-11 21:27:08.422296: val_loss -0.9509 
2025-07-11 21:27:08.423160: Pseudo dice [np.float32(0.9552)] 
2025-07-11 21:27:08.424092: Epoch time: 68.15 s 
2025-07-11 21:27:09.293653:  
2025-07-11 21:27:09.295153: Epoch 254 
2025-07-11 21:27:09.296291: Current learning rate: 0.00768 
2025-07-11 21:28:17.571875: train_loss -0.9701 
2025-07-11 21:28:17.573093: val_loss -0.9498 
2025-07-11 21:28:17.574057: Pseudo dice [np.float32(0.9542)] 
2025-07-11 21:28:17.575031: Epoch time: 68.28 s 
2025-07-11 21:28:18.443726:  
2025-07-11 21:28:18.445036: Epoch 255 
2025-07-11 21:28:18.446041: Current learning rate: 0.00767 
2025-07-11 21:29:26.523860: train_loss -0.9683 
2025-07-11 21:29:26.525208: val_loss -0.9504 
2025-07-11 21:29:26.526244: Pseudo dice [np.float32(0.9546)] 
2025-07-11 21:29:26.527296: Epoch time: 68.08 s 
2025-07-11 21:29:27.392006:  
2025-07-11 21:29:27.393572: Epoch 256 
2025-07-11 21:29:27.394572: Current learning rate: 0.00766 
2025-07-11 21:30:35.469763: train_loss -0.9705 
2025-07-11 21:30:35.471110: val_loss -0.9502 
2025-07-11 21:30:35.472052: Pseudo dice [np.float32(0.9542)] 
2025-07-11 21:30:35.473603: Epoch time: 68.08 s 
2025-07-11 21:30:36.346178:  
2025-07-11 21:30:36.348042: Epoch 257 
2025-07-11 21:30:36.349126: Current learning rate: 0.00765 
2025-07-11 21:31:44.417015: train_loss -0.9705 
2025-07-11 21:31:44.418353: val_loss -0.9498 
2025-07-11 21:31:44.419262: Pseudo dice [np.float32(0.9536)] 
2025-07-11 21:31:44.420467: Epoch time: 68.07 s 
2025-07-11 21:31:45.291628:  
2025-07-11 21:31:45.293384: Epoch 258 
2025-07-11 21:31:45.294586: Current learning rate: 0.00764 
2025-07-11 21:32:53.654641: train_loss -0.9695 
2025-07-11 21:32:53.655996: val_loss -0.9537 
2025-07-11 21:32:53.657095: Pseudo dice [np.float32(0.9581)] 
2025-07-11 21:32:53.658137: Epoch time: 68.37 s 
2025-07-11 21:32:54.506721:  
2025-07-11 21:32:54.508153: Epoch 259 
2025-07-11 21:32:54.509147: Current learning rate: 0.00764 
2025-07-11 21:34:02.659067: train_loss -0.9687 
2025-07-11 21:34:02.660352: val_loss -0.95 
2025-07-11 21:34:02.661261: Pseudo dice [np.float32(0.9548)] 
2025-07-11 21:34:02.662250: Epoch time: 68.16 s 
2025-07-11 21:34:03.534118:  
2025-07-11 21:34:03.535939: Epoch 260 
2025-07-11 21:34:03.536976: Current learning rate: 0.00763 
2025-07-11 21:35:11.590093: train_loss -0.9689 
2025-07-11 21:35:11.591230: val_loss -0.949 
2025-07-11 21:35:11.592210: Pseudo dice [np.float32(0.9532)] 
2025-07-11 21:35:11.593311: Epoch time: 68.06 s 
2025-07-11 21:35:12.476769:  
2025-07-11 21:35:12.478608: Epoch 261 
2025-07-11 21:35:12.479684: Current learning rate: 0.00762 
2025-07-11 21:36:20.729952: train_loss -0.9684 
2025-07-11 21:36:20.731104: val_loss -0.9487 
2025-07-11 21:36:20.732141: Pseudo dice [np.float32(0.9521)] 
2025-07-11 21:36:20.733091: Epoch time: 68.26 s 
2025-07-11 21:36:21.607761:  
2025-07-11 21:36:21.609525: Epoch 262 
2025-07-11 21:36:21.610739: Current learning rate: 0.00761 
2025-07-11 21:37:29.814215: train_loss -0.9672 
2025-07-11 21:37:29.815704: val_loss -0.9524 
2025-07-11 21:37:29.816696: Pseudo dice [np.float32(0.9561)] 
2025-07-11 21:37:29.817901: Epoch time: 68.21 s 
2025-07-11 21:37:30.705326:  
2025-07-11 21:37:30.707157: Epoch 263 
2025-07-11 21:37:30.708312: Current learning rate: 0.0076 
2025-07-11 21:38:38.944206: train_loss -0.9697 
2025-07-11 21:38:38.945414: val_loss -0.9496 
2025-07-11 21:38:38.946590: Pseudo dice [np.float32(0.9535)] 
2025-07-11 21:38:38.947773: Epoch time: 68.24 s 
2025-07-11 21:38:39.825304:  
2025-07-11 21:38:39.827217: Epoch 264 
2025-07-11 21:38:39.828222: Current learning rate: 0.00759 
2025-07-11 21:39:48.059047: train_loss -0.9704 
2025-07-11 21:39:48.060323: val_loss -0.9477 
2025-07-11 21:39:48.061450: Pseudo dice [np.float32(0.9517)] 
2025-07-11 21:39:48.062580: Epoch time: 68.24 s 
2025-07-11 21:39:48.927741:  
2025-07-11 21:39:48.929517: Epoch 265 
2025-07-11 21:39:48.930662: Current learning rate: 0.00758 
2025-07-11 21:40:57.114504: train_loss -0.9702 
2025-07-11 21:40:57.115866: val_loss -0.9528 
2025-07-11 21:40:57.116718: Pseudo dice [np.float32(0.9563)] 
2025-07-11 21:40:57.118067: Epoch time: 68.19 s 
2025-07-11 21:40:57.984651:  
2025-07-11 21:40:57.986461: Epoch 266 
2025-07-11 21:40:57.987648: Current learning rate: 0.00757 
2025-07-11 21:42:06.001062: train_loss -0.9693 
2025-07-11 21:42:06.002611: val_loss -0.9501 
2025-07-11 21:42:06.003727: Pseudo dice [np.float32(0.9545)] 
2025-07-11 21:42:06.004896: Epoch time: 68.02 s 
2025-07-11 21:42:06.878680:  
2025-07-11 21:42:06.880623: Epoch 267 
2025-07-11 21:42:06.881759: Current learning rate: 0.00756 
2025-07-11 21:43:14.926977: train_loss -0.9697 
2025-07-11 21:43:14.928489: val_loss -0.9497 
2025-07-11 21:43:14.929621: Pseudo dice [np.float32(0.9541)] 
2025-07-11 21:43:14.930533: Epoch time: 68.05 s 
2025-07-11 21:43:15.801407:  
2025-07-11 21:43:15.803144: Epoch 268 
2025-07-11 21:43:15.804377: Current learning rate: 0.00755 
2025-07-11 21:44:24.117920: train_loss -0.9702 
2025-07-11 21:44:24.119328: val_loss -0.9517 
2025-07-11 21:44:24.120210: Pseudo dice [np.float32(0.9562)] 
2025-07-11 21:44:24.121210: Epoch time: 68.32 s 
2025-07-11 21:44:24.991598:  
2025-07-11 21:44:24.993473: Epoch 269 
2025-07-11 21:44:24.994648: Current learning rate: 0.00754 
2025-07-11 21:45:33.036746: train_loss -0.9697 
2025-07-11 21:45:33.038280: val_loss -0.9479 
2025-07-11 21:45:33.039330: Pseudo dice [np.float32(0.9518)] 
2025-07-11 21:45:33.040598: Epoch time: 68.05 s 
2025-07-11 21:45:33.915704:  
2025-07-11 21:45:33.917307: Epoch 270 
2025-07-11 21:45:33.918555: Current learning rate: 0.00753 
2025-07-11 21:46:41.826422: train_loss -0.9699 
2025-07-11 21:46:41.828049: val_loss -0.9518 
2025-07-11 21:46:41.829365: Pseudo dice [np.float32(0.9557)] 
2025-07-11 21:46:41.830407: Epoch time: 67.91 s 
2025-07-11 21:46:42.701704:  
2025-07-11 21:46:42.703311: Epoch 271 
2025-07-11 21:46:42.704341: Current learning rate: 0.00752 
2025-07-11 21:47:50.591403: train_loss -0.9691 
2025-07-11 21:47:50.592726: val_loss -0.9504 
2025-07-11 21:47:50.593883: Pseudo dice [np.float32(0.9546)] 
2025-07-11 21:47:50.595102: Epoch time: 67.89 s 
2025-07-11 21:47:51.476588:  
2025-07-11 21:47:51.478096: Epoch 272 
2025-07-11 21:47:51.479210: Current learning rate: 0.00751 
2025-07-11 21:48:59.502996: train_loss -0.9674 
2025-07-11 21:48:59.504486: val_loss -0.9492 
2025-07-11 21:48:59.505663: Pseudo dice [np.float32(0.9531)] 
2025-07-11 21:48:59.506738: Epoch time: 68.03 s 
2025-07-11 21:49:00.383990:  
2025-07-11 21:49:00.385666: Epoch 273 
2025-07-11 21:49:00.386788: Current learning rate: 0.00751 
2025-07-11 21:50:08.356234: train_loss -0.968 
2025-07-11 21:50:08.357365: val_loss -0.9524 
2025-07-11 21:50:08.358282: Pseudo dice [np.float32(0.9562)] 
2025-07-11 21:50:08.359346: Epoch time: 67.98 s 
2025-07-11 21:50:09.245043:  
2025-07-11 21:50:09.246769: Epoch 274 
2025-07-11 21:50:09.247896: Current learning rate: 0.0075 
2025-07-11 21:51:17.263426: train_loss -0.9688 
2025-07-11 21:51:17.264647: val_loss -0.9516 
2025-07-11 21:51:17.265516: Pseudo dice [np.float32(0.957)] 
2025-07-11 21:51:17.266425: Epoch time: 68.02 s 
2025-07-11 21:51:18.156816:  
2025-07-11 21:51:18.157997: Epoch 275 
2025-07-11 21:51:18.158962: Current learning rate: 0.00749 
2025-07-11 21:52:26.251415: train_loss -0.9678 
2025-07-11 21:52:26.252664: val_loss -0.9489 
2025-07-11 21:52:26.253719: Pseudo dice [np.float32(0.9538)] 
2025-07-11 21:52:26.254782: Epoch time: 68.1 s 
2025-07-11 21:52:27.123538:  
2025-07-11 21:52:27.125180: Epoch 276 
2025-07-11 21:52:27.126208: Current learning rate: 0.00748 
2025-07-11 21:53:35.115494: train_loss -0.9716 
2025-07-11 21:53:35.116636: val_loss -0.9553 
2025-07-11 21:53:35.117523: Pseudo dice [np.float32(0.9592)] 
2025-07-11 21:53:35.118687: Epoch time: 68.0 s 
2025-07-11 21:53:35.985281:  
2025-07-11 21:53:35.986946: Epoch 277 
2025-07-11 21:53:35.988035: Current learning rate: 0.00747 
2025-07-11 21:54:44.095437: train_loss -0.9717 
2025-07-11 21:54:44.096585: val_loss -0.9498 
2025-07-11 21:54:44.097629: Pseudo dice [np.float32(0.9541)] 
2025-07-11 21:54:44.098691: Epoch time: 68.11 s 
2025-07-11 21:54:44.988148:  
2025-07-11 21:54:44.989904: Epoch 278 
2025-07-11 21:54:44.991010: Current learning rate: 0.00746 
2025-07-11 21:55:53.059520: train_loss -0.9705 
2025-07-11 21:55:53.060617: val_loss -0.9481 
2025-07-11 21:55:53.061843: Pseudo dice [np.float32(0.9528)] 
2025-07-11 21:55:53.062904: Epoch time: 68.07 s 
2025-07-11 21:55:53.931960:  
2025-07-11 21:55:53.933341: Epoch 279 
2025-07-11 21:55:53.934442: Current learning rate: 0.00745 
2025-07-11 21:57:02.078515: train_loss -0.9701 
2025-07-11 21:57:02.079719: val_loss -0.9454 
2025-07-11 21:57:02.080625: Pseudo dice [np.float32(0.9495)] 
2025-07-11 21:57:02.081804: Epoch time: 68.15 s 
2025-07-11 21:57:02.963683:  
2025-07-11 21:57:02.965302: Epoch 280 
2025-07-11 21:57:02.966288: Current learning rate: 0.00744 
2025-07-11 21:58:11.409049: train_loss -0.9686 
2025-07-11 21:58:11.410224: val_loss -0.9515 
2025-07-11 21:58:11.411414: Pseudo dice [np.float32(0.9561)] 
2025-07-11 21:58:11.412691: Epoch time: 68.45 s 
2025-07-11 21:58:12.287097:  
2025-07-11 21:58:12.289138: Epoch 281 
2025-07-11 21:58:12.290139: Current learning rate: 0.00743 
2025-07-11 21:59:20.379473: train_loss -0.9695 
2025-07-11 21:59:20.380575: val_loss -0.9511 
2025-07-11 21:59:20.381588: Pseudo dice [np.float32(0.9552)] 
2025-07-11 21:59:20.382574: Epoch time: 68.1 s 
2025-07-11 21:59:21.257397:  
2025-07-11 21:59:21.259492: Epoch 282 
2025-07-11 21:59:21.260741: Current learning rate: 0.00742 
2025-07-11 22:00:29.534602: train_loss -0.9706 
2025-07-11 22:00:29.536793: val_loss -0.9517 
2025-07-11 22:00:29.537791: Pseudo dice [np.float32(0.9561)] 
2025-07-11 22:00:29.538964: Epoch time: 68.28 s 
2025-07-11 22:00:30.411372:  
2025-07-11 22:00:30.412927: Epoch 283 
2025-07-11 22:00:30.413996: Current learning rate: 0.00741 
2025-07-11 22:01:38.489020: train_loss -0.9696 
2025-07-11 22:01:38.490175: val_loss -0.9484 
2025-07-11 22:01:38.491084: Pseudo dice [np.float32(0.9525)] 
2025-07-11 22:01:38.492282: Epoch time: 68.08 s 
2025-07-11 22:01:39.350570:  
2025-07-11 22:01:39.351923: Epoch 284 
2025-07-11 22:01:39.353073: Current learning rate: 0.0074 
2025-07-11 22:02:47.481531: train_loss -0.9698 
2025-07-11 22:02:47.482759: val_loss -0.951 
2025-07-11 22:02:47.483637: Pseudo dice [np.float32(0.955)] 
2025-07-11 22:02:47.484562: Epoch time: 68.13 s 
2025-07-11 22:02:48.351648:  
2025-07-11 22:02:48.352942: Epoch 285 
2025-07-11 22:02:48.354078: Current learning rate: 0.00739 
2025-07-11 22:03:56.346579: train_loss -0.9702 
2025-07-11 22:03:56.348107: val_loss -0.947 
2025-07-11 22:03:56.349133: Pseudo dice [np.float32(0.9515)] 
2025-07-11 22:03:56.350202: Epoch time: 68.0 s 
2025-07-11 22:03:57.224604:  
2025-07-11 22:03:57.226141: Epoch 286 
2025-07-11 22:03:57.227278: Current learning rate: 0.00738 
2025-07-11 22:05:05.600912: train_loss -0.969 
2025-07-11 22:05:05.602107: val_loss -0.9542 
2025-07-11 22:05:05.603457: Pseudo dice [np.float32(0.9577)] 
2025-07-11 22:05:05.604349: Epoch time: 68.38 s 
2025-07-11 22:05:06.487115:  
2025-07-11 22:05:06.488977: Epoch 287 
2025-07-11 22:05:06.490213: Current learning rate: 0.00738 
2025-07-11 22:06:14.627651: train_loss -0.9703 
2025-07-11 22:06:14.628959: val_loss -0.9529 
2025-07-11 22:06:14.629948: Pseudo dice [np.float32(0.9569)] 
2025-07-11 22:06:14.630829: Epoch time: 68.14 s 
2025-07-11 22:06:15.509137:  
2025-07-11 22:06:15.510720: Epoch 288 
2025-07-11 22:06:15.511969: Current learning rate: 0.00737 
2025-07-11 22:07:23.716108: train_loss -0.9706 
2025-07-11 22:07:23.717354: val_loss -0.9493 
2025-07-11 22:07:23.718452: Pseudo dice [np.float32(0.9534)] 
2025-07-11 22:07:23.719853: Epoch time: 68.21 s 
2025-07-11 22:07:24.617596:  
2025-07-11 22:07:24.619381: Epoch 289 
2025-07-11 22:07:24.620554: Current learning rate: 0.00736 
2025-07-11 22:08:32.819551: train_loss -0.9701 
2025-07-11 22:08:32.820806: val_loss -0.9539 
2025-07-11 22:08:32.821960: Pseudo dice [np.float32(0.9577)] 
2025-07-11 22:08:32.823144: Epoch time: 68.21 s 
2025-07-11 22:08:33.708103:  
2025-07-11 22:08:33.709578: Epoch 290 
2025-07-11 22:08:33.710697: Current learning rate: 0.00735 
2025-07-11 22:09:41.772259: train_loss -0.9707 
2025-07-11 22:09:41.773756: val_loss -0.9534 
2025-07-11 22:09:41.774776: Pseudo dice [np.float32(0.9576)] 
2025-07-11 22:09:41.775865: Epoch time: 68.07 s 
2025-07-11 22:09:42.658964:  
2025-07-11 22:09:42.660703: Epoch 291 
2025-07-11 22:09:42.661882: Current learning rate: 0.00734 
2025-07-11 22:10:50.741313: train_loss -0.9699 
2025-07-11 22:10:50.742508: val_loss -0.9492 
2025-07-11 22:10:50.743572: Pseudo dice [np.float32(0.9534)] 
2025-07-11 22:10:50.744526: Epoch time: 68.09 s 
2025-07-11 22:10:51.635614:  
2025-07-11 22:10:51.637337: Epoch 292 
2025-07-11 22:10:51.638558: Current learning rate: 0.00733 
2025-07-11 22:11:59.900483: train_loss -0.9689 
2025-07-11 22:11:59.901813: val_loss -0.9513 
2025-07-11 22:11:59.902694: Pseudo dice [np.float32(0.9553)] 
2025-07-11 22:11:59.903918: Epoch time: 68.27 s 
2025-07-11 22:12:00.783287:  
2025-07-11 22:12:00.784857: Epoch 293 
2025-07-11 22:12:00.785802: Current learning rate: 0.00732 
2025-07-11 22:13:09.079038: train_loss -0.9703 
2025-07-11 22:13:09.080533: val_loss -0.9482 
2025-07-11 22:13:09.081721: Pseudo dice [np.float32(0.9519)] 
2025-07-11 22:13:09.082714: Epoch time: 68.3 s 
2025-07-11 22:13:09.968405:  
2025-07-11 22:13:09.970082: Epoch 294 
2025-07-11 22:13:09.971110: Current learning rate: 0.00731 
2025-07-11 22:14:18.288532: train_loss -0.9709 
2025-07-11 22:14:18.289734: val_loss -0.9475 
2025-07-11 22:14:18.290969: Pseudo dice [np.float32(0.9533)] 
2025-07-11 22:14:18.292108: Epoch time: 68.32 s 
2025-07-11 22:14:19.166774:  
2025-07-11 22:14:19.168396: Epoch 295 
2025-07-11 22:14:19.169496: Current learning rate: 0.0073 
2025-07-11 22:15:27.386830: train_loss -0.9704 
2025-07-11 22:15:27.388149: val_loss -0.9456 
2025-07-11 22:15:27.389172: Pseudo dice [np.float32(0.9498)] 
2025-07-11 22:15:27.390143: Epoch time: 68.22 s 
2025-07-11 22:15:28.284810:  
2025-07-11 22:15:28.286619: Epoch 296 
2025-07-11 22:15:28.287734: Current learning rate: 0.00729 
2025-07-11 22:16:36.603895: train_loss -0.9709 
2025-07-11 22:16:36.605089: val_loss -0.9489 
2025-07-11 22:16:36.606392: Pseudo dice [np.float32(0.9535)] 
2025-07-11 22:16:36.607564: Epoch time: 68.32 s 
2025-07-11 22:16:37.478601:  
2025-07-11 22:16:37.479956: Epoch 297 
2025-07-11 22:16:37.481319: Current learning rate: 0.00728 
2025-07-11 22:17:45.710053: train_loss -0.9702 
2025-07-11 22:17:45.711401: val_loss -0.9483 
2025-07-11 22:17:45.712584: Pseudo dice [np.float32(0.9537)] 
2025-07-11 22:17:45.713571: Epoch time: 68.23 s 
2025-07-11 22:17:46.608810:  
2025-07-11 22:17:46.610433: Epoch 298 
2025-07-11 22:17:46.611530: Current learning rate: 0.00727 
2025-07-11 22:18:54.700864: train_loss -0.9713 
2025-07-11 22:18:54.702382: val_loss -0.9498 
2025-07-11 22:18:54.703560: Pseudo dice [np.float32(0.9547)] 
2025-07-11 22:18:54.704578: Epoch time: 68.1 s 
2025-07-11 22:18:55.584678:  
2025-07-11 22:18:55.586433: Epoch 299 
2025-07-11 22:18:55.587519: Current learning rate: 0.00726 
2025-07-11 22:20:03.611559: train_loss -0.9696 
2025-07-11 22:20:03.612809: val_loss -0.9494 
2025-07-11 22:20:03.614082: Pseudo dice [np.float32(0.9541)] 
2025-07-11 22:20:03.615152: Epoch time: 68.03 s 
2025-07-11 22:20:05.586782:  
2025-07-11 22:20:05.588408: Epoch 300 
2025-07-11 22:20:05.589559: Current learning rate: 0.00725 
2025-07-11 22:21:13.803711: train_loss -0.9711 
2025-07-11 22:21:13.804918: val_loss -0.9515 
2025-07-11 22:21:13.805813: Pseudo dice [np.float32(0.9557)] 
2025-07-11 22:21:13.806775: Epoch time: 68.22 s 
2025-07-11 22:21:14.669870:  
2025-07-11 22:21:14.671605: Epoch 301 
2025-07-11 22:21:14.672720: Current learning rate: 0.00724 
2025-07-11 22:22:23.118043: train_loss -0.9727 
2025-07-11 22:22:23.119413: val_loss -0.9518 
2025-07-11 22:22:23.120696: Pseudo dice [np.float32(0.9556)] 
2025-07-11 22:22:23.121739: Epoch time: 68.45 s 
2025-07-11 22:22:24.028733:  
2025-07-11 22:22:24.030235: Epoch 302 
2025-07-11 22:22:24.031304: Current learning rate: 0.00724 
2025-07-11 22:23:32.171058: train_loss -0.9713 
2025-07-11 22:23:32.172385: val_loss -0.9508 
2025-07-11 22:23:32.173499: Pseudo dice [np.float32(0.9546)] 
2025-07-11 22:23:32.174569: Epoch time: 68.15 s 
2025-07-11 22:23:33.056145:  
2025-07-11 22:23:33.057852: Epoch 303 
2025-07-11 22:23:33.058991: Current learning rate: 0.00723 
2025-07-11 22:24:41.201161: train_loss -0.9709 
2025-07-11 22:24:41.202572: val_loss -0.9493 
2025-07-11 22:24:41.203760: Pseudo dice [np.float32(0.9532)] 
2025-07-11 22:24:41.204617: Epoch time: 68.15 s 
2025-07-11 22:24:42.094698:  
2025-07-11 22:24:42.096365: Epoch 304 
2025-07-11 22:24:42.097441: Current learning rate: 0.00722 
2025-07-11 22:25:50.116810: train_loss -0.9696 
2025-07-11 22:25:50.118222: val_loss -0.9521 
2025-07-11 22:25:50.119428: Pseudo dice [np.float32(0.9566)] 
2025-07-11 22:25:50.120594: Epoch time: 68.03 s 
2025-07-11 22:25:51.004816:  
2025-07-11 22:25:51.006158: Epoch 305 
2025-07-11 22:25:51.007267: Current learning rate: 0.00721 
2025-07-11 22:26:59.066644: train_loss -0.9703 
2025-07-11 22:26:59.067817: val_loss -0.9514 
2025-07-11 22:26:59.068869: Pseudo dice [np.float32(0.9563)] 
2025-07-11 22:26:59.069713: Epoch time: 68.07 s 
2025-07-11 22:26:59.958320:  
2025-07-11 22:26:59.959782: Epoch 306 
2025-07-11 22:26:59.960995: Current learning rate: 0.0072 
2025-07-11 22:28:08.178161: train_loss -0.9706 
2025-07-11 22:28:08.179307: val_loss -0.9485 
2025-07-11 22:28:08.180292: Pseudo dice [np.float32(0.9527)] 
2025-07-11 22:28:08.181192: Epoch time: 68.22 s 
2025-07-11 22:28:09.289646:  
2025-07-11 22:28:09.291540: Epoch 307 
2025-07-11 22:28:09.292630: Current learning rate: 0.00719 
2025-07-11 22:29:17.420335: train_loss -0.972 
2025-07-11 22:29:17.421724: val_loss -0.9501 
2025-07-11 22:29:17.422920: Pseudo dice [np.float32(0.9549)] 
2025-07-11 22:29:17.423846: Epoch time: 68.13 s 
2025-07-11 22:29:18.321561:  
2025-07-11 22:29:18.323304: Epoch 308 
2025-07-11 22:29:18.324485: Current learning rate: 0.00718 
2025-07-11 22:30:26.433455: train_loss -0.9703 
2025-07-11 22:30:26.434973: val_loss -0.9509 
2025-07-11 22:30:26.436296: Pseudo dice [np.float32(0.955)] 
2025-07-11 22:30:26.437207: Epoch time: 68.12 s 
2025-07-11 22:30:27.325511:  
2025-07-11 22:30:27.327019: Epoch 309 
2025-07-11 22:30:27.328348: Current learning rate: 0.00717 
2025-07-11 22:31:35.449101: train_loss -0.9717 
2025-07-11 22:31:35.450479: val_loss -0.9527 
2025-07-11 22:31:35.451603: Pseudo dice [np.float32(0.957)] 
2025-07-11 22:31:35.452603: Epoch time: 68.13 s 
2025-07-11 22:31:36.339808:  
2025-07-11 22:31:36.341181: Epoch 310 
2025-07-11 22:31:36.342364: Current learning rate: 0.00716 
2025-07-11 22:32:44.495736: train_loss -0.9721 
2025-07-11 22:32:44.497171: val_loss -0.9532 
2025-07-11 22:32:44.498020: Pseudo dice [np.float32(0.9573)] 
2025-07-11 22:32:44.499204: Epoch time: 68.16 s 
2025-07-11 22:32:45.376732:  
2025-07-11 22:32:45.378436: Epoch 311 
2025-07-11 22:32:45.379614: Current learning rate: 0.00715 
2025-07-11 22:33:53.452587: train_loss -0.9724 
2025-07-11 22:33:53.453960: val_loss -0.9524 
2025-07-11 22:33:53.454928: Pseudo dice [np.float32(0.9571)] 
2025-07-11 22:33:53.456055: Epoch time: 68.08 s 
2025-07-11 22:33:54.320818:  
2025-07-11 22:33:54.322387: Epoch 312 
2025-07-11 22:33:54.323388: Current learning rate: 0.00714 
2025-07-11 22:35:02.478747: train_loss -0.971 
2025-07-11 22:35:02.479893: val_loss -0.9509 
2025-07-11 22:35:02.480789: Pseudo dice [np.float32(0.9555)] 
2025-07-11 22:35:02.481822: Epoch time: 68.16 s 
2025-07-11 22:35:03.364342:  
2025-07-11 22:35:03.366090: Epoch 313 
2025-07-11 22:35:03.367138: Current learning rate: 0.00713 
2025-07-11 22:36:11.439409: train_loss -0.9709 
2025-07-11 22:36:11.440566: val_loss -0.9527 
2025-07-11 22:36:11.441644: Pseudo dice [np.float32(0.9567)] 
2025-07-11 22:36:11.442723: Epoch time: 68.08 s 
2025-07-11 22:36:12.325513:  
2025-07-11 22:36:12.327216: Epoch 314 
2025-07-11 22:36:12.328238: Current learning rate: 0.00712 
2025-07-11 22:37:20.489019: train_loss -0.9726 
2025-07-11 22:37:20.490457: val_loss -0.9543 
2025-07-11 22:37:20.491612: Pseudo dice [np.float32(0.9582)] 
2025-07-11 22:37:20.492583: Epoch time: 68.17 s 
2025-07-11 22:37:20.493747: Yayy! New best EMA pseudo Dice: 0.9556999802589417 
2025-07-11 22:37:22.641676:  
2025-07-11 22:37:22.643312: Epoch 315 
2025-07-11 22:37:22.644382: Current learning rate: 0.00711 
2025-07-11 22:38:30.524766: train_loss -0.9699 
2025-07-11 22:38:30.525885: val_loss -0.9507 
2025-07-11 22:38:30.527145: Pseudo dice [np.float32(0.956)] 
2025-07-11 22:38:30.528297: Epoch time: 67.89 s 
2025-07-11 22:38:30.529199: Yayy! New best EMA pseudo Dice: 0.9556999802589417 
2025-07-11 22:38:32.731067:  
2025-07-11 22:38:32.732774: Epoch 316 
2025-07-11 22:38:32.733918: Current learning rate: 0.0071 
2025-07-11 22:39:41.044492: train_loss -0.9704 
2025-07-11 22:39:41.045939: val_loss -0.9527 
2025-07-11 22:39:41.047189: Pseudo dice [np.float32(0.9573)] 
2025-07-11 22:39:41.048367: Epoch time: 68.32 s 
2025-07-11 22:39:41.049373: Yayy! New best EMA pseudo Dice: 0.9559000134468079 
2025-07-11 22:39:43.136659:  
2025-07-11 22:39:43.138224: Epoch 317 
2025-07-11 22:39:43.139395: Current learning rate: 0.0071 
2025-07-11 22:40:51.649238: train_loss -0.9719 
2025-07-11 22:40:51.650649: val_loss -0.9532 
2025-07-11 22:40:51.651546: Pseudo dice [np.float32(0.9574)] 
2025-07-11 22:40:51.652674: Epoch time: 68.52 s 
2025-07-11 22:40:51.653655: Yayy! New best EMA pseudo Dice: 0.9559999704360962 
2025-07-11 22:40:53.739089:  
2025-07-11 22:40:53.740760: Epoch 318 
2025-07-11 22:40:53.741993: Current learning rate: 0.00709 
2025-07-11 22:42:01.775668: train_loss -0.971 
2025-07-11 22:42:01.776981: val_loss -0.9469 
2025-07-11 22:42:01.778142: Pseudo dice [np.float32(0.9516)] 
2025-07-11 22:42:01.779534: Epoch time: 68.04 s 
2025-07-11 22:42:02.666000:  
2025-07-11 22:42:02.667573: Epoch 319 
2025-07-11 22:42:02.668788: Current learning rate: 0.00708 
2025-07-11 22:43:10.746593: train_loss -0.9686 
2025-07-11 22:43:10.747731: val_loss -0.9494 
2025-07-11 22:43:10.748913: Pseudo dice [np.float32(0.9534)] 
2025-07-11 22:43:10.749968: Epoch time: 68.08 s 
2025-07-11 22:43:11.636509:  
2025-07-11 22:43:11.638282: Epoch 320 
2025-07-11 22:43:11.639432: Current learning rate: 0.00707 
2025-07-11 22:44:19.862749: train_loss -0.9708 
2025-07-11 22:44:19.864210: val_loss -0.951 
2025-07-11 22:44:19.865545: Pseudo dice [np.float32(0.9549)] 
2025-07-11 22:44:19.866526: Epoch time: 68.23 s 
2025-07-11 22:44:20.760189:  
2025-07-11 22:44:20.761877: Epoch 321 
2025-07-11 22:44:20.762962: Current learning rate: 0.00706 
2025-07-11 22:45:28.808169: train_loss -0.9689 
2025-07-11 22:45:28.809469: val_loss -0.9498 
2025-07-11 22:45:28.810612: Pseudo dice [np.float32(0.9536)] 
2025-07-11 22:45:28.811712: Epoch time: 68.05 s 
2025-07-11 22:45:29.699575:  
2025-07-11 22:45:29.701260: Epoch 322 
2025-07-11 22:45:29.702365: Current learning rate: 0.00705 
2025-07-11 22:46:37.821266: train_loss -0.9704 
2025-07-11 22:46:37.822665: val_loss -0.9519 
2025-07-11 22:46:37.823681: Pseudo dice [np.float32(0.9559)] 
2025-07-11 22:46:37.824640: Epoch time: 68.13 s 
2025-07-11 22:46:38.706409:  
2025-07-11 22:46:38.708074: Epoch 323 
2025-07-11 22:46:38.709196: Current learning rate: 0.00704 
2025-07-11 22:47:46.793601: train_loss -0.9699 
2025-07-11 22:47:46.794753: val_loss -0.9504 
2025-07-11 22:47:46.795666: Pseudo dice [np.float32(0.9548)] 
2025-07-11 22:47:46.796532: Epoch time: 68.09 s 
2025-07-11 22:47:47.904903:  
2025-07-11 22:47:47.906582: Epoch 324 
2025-07-11 22:47:47.907648: Current learning rate: 0.00703 
2025-07-11 22:48:56.138386: train_loss -0.9699 
2025-07-11 22:48:56.139523: val_loss -0.9476 
2025-07-11 22:48:56.140646: Pseudo dice [np.float32(0.953)] 
2025-07-11 22:48:56.141666: Epoch time: 68.24 s 
2025-07-11 22:48:57.040861:  
2025-07-11 22:48:57.042164: Epoch 325 
2025-07-11 22:48:57.043183: Current learning rate: 0.00702 
2025-07-11 22:50:05.246829: train_loss -0.9699 
2025-07-11 22:50:05.248093: val_loss -0.9489 
2025-07-11 22:50:05.249243: Pseudo dice [np.float32(0.9531)] 
2025-07-11 22:50:05.250241: Epoch time: 68.21 s 
2025-07-11 22:50:06.135283:  
2025-07-11 22:50:06.136864: Epoch 326 
2025-07-11 22:50:06.138047: Current learning rate: 0.00701 
2025-07-11 22:51:14.181189: train_loss -0.9711 
2025-07-11 22:51:14.182402: val_loss -0.9481 
2025-07-11 22:51:14.183398: Pseudo dice [np.float32(0.952)] 
2025-07-11 22:51:14.184388: Epoch time: 68.05 s 
2025-07-11 22:51:15.049912:  
2025-07-11 22:51:15.051306: Epoch 327 
2025-07-11 22:51:15.052361: Current learning rate: 0.007 
2025-07-11 22:52:23.509148: train_loss -0.9704 
2025-07-11 22:52:23.510285: val_loss -0.9536 
2025-07-11 22:52:23.511262: Pseudo dice [np.float32(0.958)] 
2025-07-11 22:52:23.512090: Epoch time: 68.46 s 
2025-07-11 22:52:24.406871:  
2025-07-11 22:52:24.408796: Epoch 328 
2025-07-11 22:52:24.409891: Current learning rate: 0.00699 
2025-07-11 22:53:32.444021: train_loss -0.9698 
2025-07-11 22:53:32.445111: val_loss -0.9504 
2025-07-11 22:53:32.446035: Pseudo dice [np.float32(0.9547)] 
2025-07-11 22:53:32.447144: Epoch time: 68.04 s 
2025-07-11 22:53:33.332166:  
2025-07-11 22:53:33.333962: Epoch 329 
2025-07-11 22:53:33.335116: Current learning rate: 0.00698 
2025-07-11 22:54:41.475527: train_loss -0.9695 
2025-07-11 22:54:41.476828: val_loss -0.9482 
2025-07-11 22:54:41.478011: Pseudo dice [np.float32(0.9525)] 
2025-07-11 22:54:41.479587: Epoch time: 68.15 s 
2025-07-11 22:54:42.371771:  
2025-07-11 22:54:42.373677: Epoch 330 
2025-07-11 22:54:42.374813: Current learning rate: 0.00697 
2025-07-11 22:55:50.475898: train_loss -0.9689 
2025-07-11 22:55:50.477175: val_loss -0.9506 
2025-07-11 22:55:50.478197: Pseudo dice [np.float32(0.9557)] 
2025-07-11 22:55:50.479154: Epoch time: 68.11 s 
2025-07-11 22:55:51.595850:  
2025-07-11 22:55:51.597381: Epoch 331 
2025-07-11 22:55:51.598464: Current learning rate: 0.00696 
2025-07-11 22:56:59.729602: train_loss -0.9708 
2025-07-11 22:56:59.730915: val_loss -0.9508 
2025-07-11 22:56:59.731955: Pseudo dice [np.float32(0.9547)] 
2025-07-11 22:56:59.732899: Epoch time: 68.14 s 
2025-07-11 22:57:00.612702:  
2025-07-11 22:57:00.614571: Epoch 332 
2025-07-11 22:57:00.615745: Current learning rate: 0.00696 
2025-07-11 22:58:08.699712: train_loss -0.9718 
2025-07-11 22:58:08.700914: val_loss -0.9512 
2025-07-11 22:58:08.702416: Pseudo dice [np.float32(0.9555)] 
2025-07-11 22:58:08.703307: Epoch time: 68.09 s 
2025-07-11 22:58:09.593098:  
2025-07-11 22:58:09.595016: Epoch 333 
2025-07-11 22:58:09.596185: Current learning rate: 0.00695 
2025-07-11 22:59:17.690933: train_loss -0.971 
2025-07-11 22:59:17.692332: val_loss -0.9485 
2025-07-11 22:59:17.693276: Pseudo dice [np.float32(0.953)] 
2025-07-11 22:59:17.694413: Epoch time: 68.1 s 
2025-07-11 22:59:18.582211:  
2025-07-11 22:59:18.583637: Epoch 334 
2025-07-11 22:59:18.584720: Current learning rate: 0.00694 
2025-07-11 23:00:26.796800: train_loss -0.9715 
2025-07-11 23:00:26.798071: val_loss -0.9528 
2025-07-11 23:00:26.799214: Pseudo dice [np.float32(0.9575)] 
2025-07-11 23:00:26.800637: Epoch time: 68.22 s 
2025-07-11 23:00:27.692100:  
2025-07-11 23:00:27.693848: Epoch 335 
2025-07-11 23:00:27.694957: Current learning rate: 0.00693 
2025-07-11 23:01:35.771376: train_loss -0.9704 
2025-07-11 23:01:35.772870: val_loss -0.9524 
2025-07-11 23:01:35.773976: Pseudo dice [np.float32(0.9571)] 
2025-07-11 23:01:35.775000: Epoch time: 68.08 s 
2025-07-11 23:01:36.670099:  
2025-07-11 23:01:36.671900: Epoch 336 
2025-07-11 23:01:36.672873: Current learning rate: 0.00692 
2025-07-11 23:02:44.952401: train_loss -0.9702 
2025-07-11 23:02:44.953753: val_loss -0.9505 
2025-07-11 23:02:44.954944: Pseudo dice [np.float32(0.9541)] 
2025-07-11 23:02:44.955786: Epoch time: 68.29 s 
2025-07-11 23:02:45.850820:  
2025-07-11 23:02:45.852642: Epoch 337 
2025-07-11 23:02:45.853855: Current learning rate: 0.00691 
2025-07-11 23:03:54.048257: train_loss -0.9709 
2025-07-11 23:03:54.049634: val_loss -0.9479 
2025-07-11 23:03:54.050509: Pseudo dice [np.float32(0.952)] 
2025-07-11 23:03:54.051647: Epoch time: 68.2 s 
2025-07-11 23:03:55.183043:  
2025-07-11 23:03:55.184963: Epoch 338 
2025-07-11 23:03:55.186266: Current learning rate: 0.0069 
2025-07-11 23:05:03.326571: train_loss -0.9708 
2025-07-11 23:05:03.327884: val_loss -0.9525 
2025-07-11 23:05:03.328920: Pseudo dice [np.float32(0.9562)] 
2025-07-11 23:05:03.330006: Epoch time: 68.15 s 
2025-07-11 23:05:04.225595:  
2025-07-11 23:05:04.227083: Epoch 339 
2025-07-11 23:05:04.228601: Current learning rate: 0.00689 
2025-07-11 23:06:12.405889: train_loss -0.9712 
2025-07-11 23:06:12.408495: val_loss -0.9525 
2025-07-11 23:06:12.409624: Pseudo dice [np.float32(0.9565)] 
2025-07-11 23:06:12.410801: Epoch time: 68.18 s 
2025-07-11 23:06:13.309649:  
2025-07-11 23:06:13.311139: Epoch 340 
2025-07-11 23:06:13.312289: Current learning rate: 0.00688 
2025-07-11 23:07:21.448910: train_loss -0.9728 
2025-07-11 23:07:21.450202: val_loss -0.9497 
2025-07-11 23:07:21.451372: Pseudo dice [np.float32(0.9535)] 
2025-07-11 23:07:21.452269: Epoch time: 68.14 s 
2025-07-11 23:07:22.345689:  
2025-07-11 23:07:22.347314: Epoch 341 
2025-07-11 23:07:22.348386: Current learning rate: 0.00687 
2025-07-11 23:08:30.814065: train_loss -0.9715 
2025-07-11 23:08:30.815488: val_loss -0.9501 
2025-07-11 23:08:30.816601: Pseudo dice [np.float32(0.9548)] 
2025-07-11 23:08:30.817871: Epoch time: 68.47 s 
2025-07-11 23:08:31.717776:  
2025-07-11 23:08:31.719357: Epoch 342 
2025-07-11 23:08:31.720519: Current learning rate: 0.00686 
2025-07-11 23:09:39.989012: train_loss -0.9717 
2025-07-11 23:09:39.990396: val_loss -0.9523 
2025-07-11 23:09:39.991386: Pseudo dice [np.float32(0.9569)] 
2025-07-11 23:09:39.992712: Epoch time: 68.27 s 
2025-07-11 23:09:40.885167:  
2025-07-11 23:09:40.886876: Epoch 343 
2025-07-11 23:09:40.888040: Current learning rate: 0.00685 
2025-07-11 23:10:49.126486: train_loss -0.9723 
2025-07-11 23:10:49.127803: val_loss -0.9502 
2025-07-11 23:10:49.128704: Pseudo dice [np.float32(0.9536)] 
2025-07-11 23:10:49.129904: Epoch time: 68.24 s 
2025-07-11 23:10:50.023740:  
2025-07-11 23:10:50.025456: Epoch 344 
2025-07-11 23:10:50.026622: Current learning rate: 0.00684 
2025-07-11 23:11:58.186608: train_loss -0.9713 
2025-07-11 23:11:58.187853: val_loss -0.9521 
2025-07-11 23:11:58.188788: Pseudo dice [np.float32(0.9563)] 
2025-07-11 23:11:58.189765: Epoch time: 68.17 s 
2025-07-11 23:11:59.311931:  
2025-07-11 23:11:59.313764: Epoch 345 
2025-07-11 23:11:59.314872: Current learning rate: 0.00683 
2025-07-11 23:13:07.530728: train_loss -0.9701 
2025-07-11 23:13:07.532174: val_loss -0.9487 
2025-07-11 23:13:07.533529: Pseudo dice [np.float32(0.9525)] 
2025-07-11 23:13:07.534650: Epoch time: 68.22 s 
2025-07-11 23:13:08.421016:  
2025-07-11 23:13:08.422520: Epoch 346 
2025-07-11 23:13:08.423605: Current learning rate: 0.00682 
2025-07-11 23:14:16.641450: train_loss -0.9707 
2025-07-11 23:14:16.642759: val_loss -0.9523 
2025-07-11 23:14:16.643914: Pseudo dice [np.float32(0.9572)] 
2025-07-11 23:14:16.644976: Epoch time: 68.22 s 
2025-07-11 23:14:17.536996:  
2025-07-11 23:14:17.538631: Epoch 347 
2025-07-11 23:14:17.539790: Current learning rate: 0.00681 
2025-07-11 23:15:25.677794: train_loss -0.9724 
2025-07-11 23:15:25.679049: val_loss -0.9487 
2025-07-11 23:15:25.680347: Pseudo dice [np.float32(0.953)] 
2025-07-11 23:15:25.681489: Epoch time: 68.14 s 
2025-07-11 23:15:26.576655:  
2025-07-11 23:15:26.578276: Epoch 348 
2025-07-11 23:15:26.579517: Current learning rate: 0.0068 
2025-07-11 23:16:34.685654: train_loss -0.9714 
2025-07-11 23:16:34.686829: val_loss -0.9506 
2025-07-11 23:16:34.687906: Pseudo dice [np.float32(0.9543)] 
2025-07-11 23:16:34.688942: Epoch time: 68.11 s 
2025-07-11 23:16:35.592372:  
2025-07-11 23:16:35.593893: Epoch 349 
2025-07-11 23:16:35.594988: Current learning rate: 0.0068 
2025-07-11 23:17:43.580607: train_loss -0.971 
2025-07-11 23:17:43.581709: val_loss -0.9522 
2025-07-11 23:17:43.582698: Pseudo dice [np.float32(0.9564)] 
2025-07-11 23:17:43.583831: Epoch time: 67.99 s 
2025-07-11 23:17:45.683510:  
2025-07-11 23:17:45.685380: Epoch 350 
2025-07-11 23:17:45.686436: Current learning rate: 0.00679 
2025-07-11 23:18:53.669484: train_loss -0.9725 
2025-07-11 23:18:53.670782: val_loss -0.9501 
2025-07-11 23:18:53.671819: Pseudo dice [np.float32(0.954)] 
2025-07-11 23:18:53.672847: Epoch time: 67.99 s 
2025-07-11 23:18:54.572577:  
2025-07-11 23:18:54.573978: Epoch 351 
2025-07-11 23:18:54.575295: Current learning rate: 0.00678 
2025-07-11 23:20:02.662345: train_loss -0.9722 
2025-07-11 23:20:02.663572: val_loss -0.9524 
2025-07-11 23:20:02.664592: Pseudo dice [np.float32(0.9563)] 
2025-07-11 23:20:02.665618: Epoch time: 68.09 s 
2025-07-11 23:20:03.557264:  
2025-07-11 23:20:03.559028: Epoch 352 
2025-07-11 23:20:03.560181: Current learning rate: 0.00677 
2025-07-11 23:21:11.669162: train_loss -0.9709 
2025-07-11 23:21:11.670510: val_loss -0.947 
2025-07-11 23:21:11.671557: Pseudo dice [np.float32(0.9515)] 
2025-07-11 23:21:11.672728: Epoch time: 68.12 s 
2025-07-11 23:21:12.569312:  
2025-07-11 23:21:12.571181: Epoch 353 
2025-07-11 23:21:12.572253: Current learning rate: 0.00676 
2025-07-11 23:22:20.630522: train_loss -0.9701 
2025-07-11 23:22:20.631821: val_loss -0.9471 
2025-07-11 23:22:20.633103: Pseudo dice [np.float32(0.9511)] 
2025-07-11 23:22:20.634017: Epoch time: 68.06 s 
2025-07-11 23:22:21.532505:  
2025-07-11 23:22:21.534113: Epoch 354 
2025-07-11 23:22:21.535577: Current learning rate: 0.00675 
2025-07-11 23:23:29.577095: train_loss -0.9716 
2025-07-11 23:23:29.578363: val_loss -0.9517 
2025-07-11 23:23:29.579575: Pseudo dice [np.float32(0.9554)] 
2025-07-11 23:23:29.580769: Epoch time: 68.05 s 
2025-07-11 23:23:30.699921:  
2025-07-11 23:23:30.701572: Epoch 355 
2025-07-11 23:23:30.702794: Current learning rate: 0.00674 
2025-07-11 23:24:38.721199: train_loss -0.9716 
2025-07-11 23:24:38.722928: val_loss -0.9509 
2025-07-11 23:24:38.724079: Pseudo dice [np.float32(0.9548)] 
2025-07-11 23:24:38.725268: Epoch time: 68.02 s 
2025-07-11 23:24:39.630077:  
2025-07-11 23:24:39.631986: Epoch 356 
2025-07-11 23:24:39.633128: Current learning rate: 0.00673 
2025-07-11 23:25:47.768508: train_loss -0.9717 
2025-07-11 23:25:47.769811: val_loss -0.9494 
2025-07-11 23:25:47.770787: Pseudo dice [np.float32(0.9531)] 
2025-07-11 23:25:47.771807: Epoch time: 68.14 s 
2025-07-11 23:25:48.658800:  
2025-07-11 23:25:48.660161: Epoch 357 
2025-07-11 23:25:48.661386: Current learning rate: 0.00672 
2025-07-11 23:26:56.885462: train_loss -0.9719 
2025-07-11 23:26:56.886840: val_loss -0.951 
2025-07-11 23:26:56.887759: Pseudo dice [np.float32(0.9549)] 
2025-07-11 23:26:56.888817: Epoch time: 68.23 s 
2025-07-11 23:26:57.780439:  
2025-07-11 23:26:57.781955: Epoch 358 
2025-07-11 23:26:57.783133: Current learning rate: 0.00671 
2025-07-11 23:28:06.207948: train_loss -0.9714 
2025-07-11 23:28:06.209307: val_loss -0.9507 
2025-07-11 23:28:06.210402: Pseudo dice [np.float32(0.9541)] 
2025-07-11 23:28:06.211576: Epoch time: 68.43 s 
2025-07-11 23:28:07.098548:  
2025-07-11 23:28:07.100467: Epoch 359 
2025-07-11 23:28:07.101637: Current learning rate: 0.0067 
2025-07-11 23:29:15.310302: train_loss -0.972 
2025-07-11 23:29:15.311726: val_loss -0.9528 
2025-07-11 23:29:15.312607: Pseudo dice [np.float32(0.9567)] 
2025-07-11 23:29:15.313649: Epoch time: 68.22 s 
2025-07-11 23:29:16.213695:  
2025-07-11 23:29:16.215013: Epoch 360 
2025-07-11 23:29:16.216361: Current learning rate: 0.00669 
2025-07-11 23:30:24.362765: train_loss -0.9732 
2025-07-11 23:30:24.364054: val_loss -0.9523 
2025-07-11 23:30:24.365166: Pseudo dice [np.float32(0.9563)] 
2025-07-11 23:30:24.366298: Epoch time: 68.15 s 
2025-07-11 23:30:25.259204:  
2025-07-11 23:30:25.260643: Epoch 361 
2025-07-11 23:30:25.261648: Current learning rate: 0.00668 
2025-07-11 23:31:33.406110: train_loss -0.9717 
2025-07-11 23:31:33.407716: val_loss -0.9531 
2025-07-11 23:31:33.408774: Pseudo dice [np.float32(0.9568)] 
2025-07-11 23:31:33.409916: Epoch time: 68.15 s 
2025-07-11 23:31:34.528171:  
2025-07-11 23:31:34.529919: Epoch 362 
2025-07-11 23:31:34.531161: Current learning rate: 0.00667 
2025-07-11 23:32:42.707532: train_loss -0.9717 
2025-07-11 23:32:42.709184: val_loss -0.9519 
2025-07-11 23:32:42.710174: Pseudo dice [np.float32(0.9557)] 
2025-07-11 23:32:42.711267: Epoch time: 68.18 s 
2025-07-11 23:32:43.607033:  
2025-07-11 23:32:43.608299: Epoch 363 
2025-07-11 23:32:43.609370: Current learning rate: 0.00666 
2025-07-11 23:33:51.745104: train_loss -0.9727 
2025-07-11 23:33:51.746269: val_loss -0.9496 
2025-07-11 23:33:51.747302: Pseudo dice [np.float32(0.9537)] 
2025-07-11 23:33:51.748279: Epoch time: 68.14 s 
2025-07-11 23:33:52.652555:  
2025-07-11 23:33:52.654125: Epoch 364 
2025-07-11 23:33:52.655214: Current learning rate: 0.00665 
2025-07-11 23:35:00.792646: train_loss -0.9732 
2025-07-11 23:35:00.793977: val_loss -0.9507 
2025-07-11 23:35:00.795057: Pseudo dice [np.float32(0.9549)] 
2025-07-11 23:35:00.796216: Epoch time: 68.14 s 
2025-07-11 23:35:01.705942:  
2025-07-11 23:35:01.707585: Epoch 365 
2025-07-11 23:35:01.708752: Current learning rate: 0.00665 
2025-07-11 23:36:09.867505: train_loss -0.9716 
2025-07-11 23:36:09.868735: val_loss -0.9489 
2025-07-11 23:36:09.869785: Pseudo dice [np.float32(0.9536)] 
2025-07-11 23:36:09.870631: Epoch time: 68.17 s 
2025-07-11 23:36:10.778906:  
2025-07-11 23:36:10.780766: Epoch 366 
2025-07-11 23:36:10.781891: Current learning rate: 0.00664 
2025-07-11 23:37:18.692979: train_loss -0.9711 
2025-07-11 23:37:18.694123: val_loss -0.9518 
2025-07-11 23:37:18.695381: Pseudo dice [np.float32(0.9561)] 
2025-07-11 23:37:18.696543: Epoch time: 67.92 s 
2025-07-11 23:37:19.603347:  
2025-07-11 23:37:19.604957: Epoch 367 
2025-07-11 23:37:19.606050: Current learning rate: 0.00663 
2025-07-11 23:38:27.487859: train_loss -0.9726 
2025-07-11 23:38:27.489258: val_loss -0.9538 
2025-07-11 23:38:27.490366: Pseudo dice [np.float32(0.9577)] 
2025-07-11 23:38:27.491514: Epoch time: 67.89 s 
2025-07-11 23:38:28.396173:  
2025-07-11 23:38:28.398014: Epoch 368 
2025-07-11 23:38:28.399333: Current learning rate: 0.00662 
2025-07-11 23:39:36.366727: train_loss -0.9709 
2025-07-11 23:39:36.367982: val_loss -0.9501 
2025-07-11 23:39:36.369123: Pseudo dice [np.float32(0.9547)] 
2025-07-11 23:39:36.370480: Epoch time: 67.97 s 
2025-07-11 23:39:37.491028:  
2025-07-11 23:39:37.492887: Epoch 369 
2025-07-11 23:39:37.494059: Current learning rate: 0.00661 
2025-07-11 23:40:45.471503: train_loss -0.9705 
2025-07-11 23:40:45.472750: val_loss -0.9508 
2025-07-11 23:40:45.474297: Pseudo dice [np.float32(0.9566)] 
2025-07-11 23:40:45.475209: Epoch time: 67.98 s 
2025-07-11 23:40:46.380837:  
2025-07-11 23:40:46.382640: Epoch 370 
2025-07-11 23:40:46.383748: Current learning rate: 0.0066 
2025-07-11 23:41:54.407348: train_loss -0.9721 
2025-07-11 23:41:54.408699: val_loss -0.9515 
2025-07-11 23:41:54.409927: Pseudo dice [np.float32(0.9548)] 
2025-07-11 23:41:54.411099: Epoch time: 68.03 s 
2025-07-11 23:41:55.311315:  
2025-07-11 23:41:55.312801: Epoch 371 
2025-07-11 23:41:55.313931: Current learning rate: 0.00659 
2025-07-11 23:43:03.135988: train_loss -0.9732 
2025-07-11 23:43:03.137067: val_loss -0.951 
2025-07-11 23:43:03.137940: Pseudo dice [np.float32(0.955)] 
2025-07-11 23:43:03.139326: Epoch time: 67.83 s 
2025-07-11 23:43:04.037019:  
2025-07-11 23:43:04.038535: Epoch 372 
2025-07-11 23:43:04.039763: Current learning rate: 0.00658 
2025-07-11 23:44:12.074552: train_loss -0.9724 
2025-07-11 23:44:12.075875: val_loss -0.9472 
2025-07-11 23:44:12.077055: Pseudo dice [np.float32(0.952)] 
2025-07-11 23:44:12.078087: Epoch time: 68.04 s 
2025-07-11 23:44:12.979450:  
2025-07-11 23:44:12.980998: Epoch 373 
2025-07-11 23:44:12.982117: Current learning rate: 0.00657 
2025-07-11 23:45:21.323507: train_loss -0.973 
2025-07-11 23:45:21.324911: val_loss -0.9512 
2025-07-11 23:45:21.326233: Pseudo dice [np.float32(0.9557)] 
2025-07-11 23:45:21.327408: Epoch time: 68.35 s 
2025-07-11 23:45:22.232664:  
2025-07-11 23:45:22.234314: Epoch 374 
2025-07-11 23:45:22.235319: Current learning rate: 0.00656 
2025-07-11 23:46:30.444822: train_loss -0.9736 
2025-07-11 23:46:30.446083: val_loss -0.9515 
2025-07-11 23:46:30.447165: Pseudo dice [np.float32(0.955)] 
2025-07-11 23:46:30.448131: Epoch time: 68.22 s 
2025-07-11 23:46:31.342724:  
2025-07-11 23:46:31.344421: Epoch 375 
2025-07-11 23:46:31.345515: Current learning rate: 0.00655 
2025-07-11 23:47:39.589645: train_loss -0.9736 
2025-07-11 23:47:39.590917: val_loss -0.9494 
2025-07-11 23:47:39.592044: Pseudo dice [np.float32(0.9532)] 
2025-07-11 23:47:39.593136: Epoch time: 68.25 s 
2025-07-11 23:47:40.710862:  
2025-07-11 23:47:40.712270: Epoch 376 
2025-07-11 23:47:40.713411: Current learning rate: 0.00654 
2025-07-11 23:48:48.782584: train_loss -0.9731 
2025-07-11 23:48:48.784007: val_loss -0.9525 
2025-07-11 23:48:48.784975: Pseudo dice [np.float32(0.9569)] 
2025-07-11 23:48:48.785981: Epoch time: 68.08 s 
2025-07-11 23:48:49.680234:  
2025-07-11 23:48:49.681949: Epoch 377 
2025-07-11 23:48:49.683050: Current learning rate: 0.00653 
2025-07-11 23:49:57.725283: train_loss -0.973 
2025-07-11 23:49:57.726697: val_loss -0.9477 
2025-07-11 23:49:57.727708: Pseudo dice [np.float32(0.9524)] 
2025-07-11 23:49:57.728787: Epoch time: 68.05 s 
2025-07-11 23:49:58.627788:  
2025-07-11 23:49:58.629154: Epoch 378 
2025-07-11 23:49:58.630280: Current learning rate: 0.00652 
2025-07-11 23:51:06.647313: train_loss -0.972 
2025-07-11 23:51:06.648632: val_loss -0.9472 
2025-07-11 23:51:06.649509: Pseudo dice [np.float32(0.9522)] 
2025-07-11 23:51:06.650396: Epoch time: 68.02 s 
2025-07-11 23:51:07.542814:  
2025-07-11 23:51:07.544364: Epoch 379 
2025-07-11 23:51:07.545510: Current learning rate: 0.00651 
2025-07-11 23:52:15.850794: train_loss -0.9713 
2025-07-11 23:52:15.852184: val_loss -0.9507 
2025-07-11 23:52:15.853405: Pseudo dice [np.float32(0.9551)] 
2025-07-11 23:52:15.854746: Epoch time: 68.31 s 
2025-07-11 23:52:16.749568:  
2025-07-11 23:52:16.751539: Epoch 380 
2025-07-11 23:52:16.752615: Current learning rate: 0.0065 
2025-07-11 23:53:24.755040: train_loss -0.9706 
2025-07-11 23:53:24.756221: val_loss -0.9494 
2025-07-11 23:53:24.757365: Pseudo dice [np.float32(0.9535)] 
2025-07-11 23:53:24.758398: Epoch time: 68.01 s 
2025-07-11 23:53:25.661509:  
2025-07-11 23:53:25.663320: Epoch 381 
2025-07-11 23:53:25.664409: Current learning rate: 0.00649 
2025-07-11 23:54:33.733935: train_loss -0.9696 
2025-07-11 23:54:33.735095: val_loss -0.9479 
2025-07-11 23:54:33.736084: Pseudo dice [np.float32(0.9531)] 
2025-07-11 23:54:33.736995: Epoch time: 68.08 s 
2025-07-11 23:54:34.639053:  
2025-07-11 23:54:34.640890: Epoch 382 
2025-07-11 23:54:34.642029: Current learning rate: 0.00648 
2025-07-11 23:55:42.620678: train_loss -0.9691 
2025-07-11 23:55:42.621786: val_loss -0.9502 
2025-07-11 23:55:42.622865: Pseudo dice [np.float32(0.9551)] 
2025-07-11 23:55:42.623945: Epoch time: 67.99 s 
2025-07-11 23:55:43.744199:  
2025-07-11 23:55:43.745893: Epoch 383 
2025-07-11 23:55:43.747070: Current learning rate: 0.00648 
2025-07-11 23:56:51.736072: train_loss -0.9709 
2025-07-11 23:56:51.737268: val_loss -0.9519 
2025-07-11 23:56:51.738275: Pseudo dice [np.float32(0.9561)] 
2025-07-11 23:56:51.739576: Epoch time: 68.0 s 
2025-07-11 23:56:52.642897:  
2025-07-11 23:56:52.644591: Epoch 384 
2025-07-11 23:56:52.645737: Current learning rate: 0.00647 
2025-07-11 23:58:00.682406: train_loss -0.9727 
2025-07-11 23:58:00.683803: val_loss -0.9501 
2025-07-11 23:58:00.685158: Pseudo dice [np.float32(0.9549)] 
2025-07-11 23:58:00.686063: Epoch time: 68.04 s 
2025-07-11 23:58:01.599209:  
2025-07-11 23:58:01.600811: Epoch 385 
2025-07-11 23:58:01.601902: Current learning rate: 0.00646 
2025-07-11 23:59:09.551042: train_loss -0.9728 
2025-07-11 23:59:09.552259: val_loss -0.9541 
2025-07-11 23:59:09.553226: Pseudo dice [np.float32(0.9577)] 
2025-07-11 23:59:09.554173: Epoch time: 67.96 s 
2025-07-11 23:59:10.450010:  
2025-07-11 23:59:10.451787: Epoch 386 
2025-07-11 23:59:10.452916: Current learning rate: 0.00645 
2025-07-12 00:00:18.572991: train_loss -0.9728 
2025-07-12 00:00:18.574380: val_loss -0.9527 
2025-07-12 00:00:18.575463: Pseudo dice [np.float32(0.9563)] 
2025-07-12 00:00:18.576547: Epoch time: 68.13 s 
2025-07-12 00:00:19.476601:  
2025-07-12 00:00:19.477860: Epoch 387 
2025-07-12 00:00:19.479098: Current learning rate: 0.00644 
2025-07-12 00:01:27.473194: train_loss -0.973 
2025-07-12 00:01:27.474478: val_loss -0.952 
2025-07-12 00:01:27.475626: Pseudo dice [np.float32(0.9566)] 
2025-07-12 00:01:27.476706: Epoch time: 68.0 s 
2025-07-12 00:01:28.384437:  
2025-07-12 00:01:28.385810: Epoch 388 
2025-07-12 00:01:28.386807: Current learning rate: 0.00643 
2025-07-12 00:02:36.405451: train_loss -0.9727 
2025-07-12 00:02:36.406889: val_loss -0.9485 
2025-07-12 00:02:36.408119: Pseudo dice [np.float32(0.9529)] 
2025-07-12 00:02:36.409145: Epoch time: 68.02 s 
2025-07-12 00:02:37.330193:  
2025-07-12 00:02:37.331869: Epoch 389 
2025-07-12 00:02:37.333035: Current learning rate: 0.00642 
2025-07-12 00:03:45.394489: train_loss -0.9705 
2025-07-12 00:03:45.395925: val_loss -0.946 
2025-07-12 00:03:45.397044: Pseudo dice [np.float32(0.9509)] 
2025-07-12 00:03:45.398225: Epoch time: 68.07 s 
2025-07-12 00:03:46.536648:  
2025-07-12 00:03:46.538781: Epoch 390 
2025-07-12 00:03:46.539959: Current learning rate: 0.00641 
2025-07-12 00:04:54.631591: train_loss -0.9697 
2025-07-12 00:04:54.632784: val_loss -0.9457 
2025-07-12 00:04:54.633606: Pseudo dice [np.float32(0.95)] 
2025-07-12 00:04:54.634742: Epoch time: 68.1 s 
2025-07-12 00:04:55.536153:  
2025-07-12 00:04:55.537991: Epoch 391 
2025-07-12 00:04:55.539145: Current learning rate: 0.0064 
2025-07-12 00:06:03.566869: train_loss -0.9715 
2025-07-12 00:06:03.568406: val_loss -0.9535 
2025-07-12 00:06:03.569715: Pseudo dice [np.float32(0.9578)] 
2025-07-12 00:06:03.570668: Epoch time: 68.03 s 
2025-07-12 00:06:04.478855:  
2025-07-12 00:06:04.480424: Epoch 392 
2025-07-12 00:06:04.481686: Current learning rate: 0.00639 
2025-07-12 00:07:12.485703: train_loss -0.9716 
2025-07-12 00:07:12.486843: val_loss -0.9523 
2025-07-12 00:07:12.487854: Pseudo dice [np.float32(0.9563)] 
2025-07-12 00:07:12.489008: Epoch time: 68.01 s 
2025-07-12 00:07:13.393457:  
2025-07-12 00:07:13.395288: Epoch 393 
2025-07-12 00:07:13.396481: Current learning rate: 0.00638 
2025-07-12 00:08:21.544631: train_loss -0.9713 
2025-07-12 00:08:21.546133: val_loss -0.9503 
2025-07-12 00:08:21.547379: Pseudo dice [np.float32(0.9543)] 
2025-07-12 00:08:21.548534: Epoch time: 68.15 s 
2025-07-12 00:08:22.464111:  
2025-07-12 00:08:22.466208: Epoch 394 
2025-07-12 00:08:22.467373: Current learning rate: 0.00637 
2025-07-12 00:09:30.391774: train_loss -0.9719 
2025-07-12 00:09:30.393130: val_loss -0.9536 
2025-07-12 00:09:30.394144: Pseudo dice [np.float32(0.9576)] 
2025-07-12 00:09:30.395358: Epoch time: 67.93 s 
2025-07-12 00:09:31.287607:  
2025-07-12 00:09:31.289526: Epoch 395 
2025-07-12 00:09:31.290731: Current learning rate: 0.00636 
2025-07-12 00:10:39.255562: train_loss -0.9731 
2025-07-12 00:10:39.256841: val_loss -0.9501 
2025-07-12 00:10:39.257876: Pseudo dice [np.float32(0.9548)] 
2025-07-12 00:10:39.259081: Epoch time: 67.97 s 
2025-07-12 00:10:40.173318:  
2025-07-12 00:10:40.174667: Epoch 396 
2025-07-12 00:10:40.175659: Current learning rate: 0.00635 
2025-07-12 00:11:48.084804: train_loss -0.9709 
2025-07-12 00:11:48.087516: val_loss -0.9493 
2025-07-12 00:11:48.088744: Pseudo dice [np.float32(0.954)] 
2025-07-12 00:11:48.089906: Epoch time: 67.91 s 
2025-07-12 00:11:49.218102:  
2025-07-12 00:11:49.219743: Epoch 397 
2025-07-12 00:11:49.220935: Current learning rate: 0.00634 
2025-07-12 00:12:57.130968: train_loss -0.9723 
2025-07-12 00:12:57.132190: val_loss -0.9528 
2025-07-12 00:12:57.133288: Pseudo dice [np.float32(0.9568)] 
2025-07-12 00:12:57.134288: Epoch time: 67.92 s 
2025-07-12 00:12:58.031838:  
2025-07-12 00:12:58.033673: Epoch 398 
2025-07-12 00:12:58.034924: Current learning rate: 0.00633 
2025-07-12 00:14:05.959067: train_loss -0.9723 
2025-07-12 00:14:05.960416: val_loss -0.9523 
2025-07-12 00:14:05.961661: Pseudo dice [np.float32(0.9567)] 
2025-07-12 00:14:05.962723: Epoch time: 67.93 s 
2025-07-12 00:14:06.867709:  
2025-07-12 00:14:06.869316: Epoch 399 
2025-07-12 00:14:06.870661: Current learning rate: 0.00632 
2025-07-12 00:15:14.893401: train_loss -0.9723 
2025-07-12 00:15:14.894746: val_loss -0.952 
2025-07-12 00:15:14.895965: Pseudo dice [np.float32(0.9559)] 
2025-07-12 00:15:14.897594: Epoch time: 68.03 s 
2025-07-12 00:15:17.250954:  
2025-07-12 00:15:17.252514: Epoch 400 
2025-07-12 00:15:17.253610: Current learning rate: 0.00631 
2025-07-12 00:16:25.417461: train_loss -0.9719 
2025-07-12 00:16:25.418736: val_loss -0.9526 
2025-07-12 00:16:25.419655: Pseudo dice [np.float32(0.9568)] 
2025-07-12 00:16:25.420643: Epoch time: 68.17 s 
2025-07-12 00:16:26.317014:  
2025-07-12 00:16:26.318856: Epoch 401 
2025-07-12 00:16:26.319997: Current learning rate: 0.0063 
2025-07-12 00:17:34.499401: train_loss -0.9728 
2025-07-12 00:17:34.500618: val_loss -0.9479 
2025-07-12 00:17:34.501528: Pseudo dice [np.float32(0.9521)] 
2025-07-12 00:17:34.502713: Epoch time: 68.19 s 
2025-07-12 00:17:35.395691:  
2025-07-12 00:17:35.397449: Epoch 402 
2025-07-12 00:17:35.398522: Current learning rate: 0.0063 
2025-07-12 00:18:43.483014: train_loss -0.9731 
2025-07-12 00:18:43.484241: val_loss -0.9493 
2025-07-12 00:18:43.485304: Pseudo dice [np.float32(0.9547)] 
2025-07-12 00:18:43.486231: Epoch time: 68.09 s 
2025-07-12 00:18:44.398155:  
2025-07-12 00:18:44.399714: Epoch 403 
2025-07-12 00:18:44.400662: Current learning rate: 0.00629 
2025-07-12 00:19:52.871689: train_loss -0.9742 
2025-07-12 00:19:52.872990: val_loss -0.9504 
2025-07-12 00:19:52.873829: Pseudo dice [np.float32(0.9545)] 
2025-07-12 00:19:52.874851: Epoch time: 68.48 s 
2025-07-12 00:19:53.781175:  
2025-07-12 00:19:53.782601: Epoch 404 
2025-07-12 00:19:53.783808: Current learning rate: 0.00628 
2025-07-12 00:21:02.016878: train_loss -0.9746 
2025-07-12 00:21:02.018092: val_loss -0.9529 
2025-07-12 00:21:02.019035: Pseudo dice [np.float32(0.9569)] 
2025-07-12 00:21:02.020104: Epoch time: 68.24 s 
2025-07-12 00:21:02.918174:  
2025-07-12 00:21:02.919850: Epoch 405 
2025-07-12 00:21:02.920974: Current learning rate: 0.00627 
2025-07-12 00:22:11.006472: train_loss -0.9733 
2025-07-12 00:22:11.007751: val_loss -0.9484 
2025-07-12 00:22:11.008847: Pseudo dice [np.float32(0.9523)] 
2025-07-12 00:22:11.009798: Epoch time: 68.09 s 
2025-07-12 00:22:11.923992:  
2025-07-12 00:22:11.925701: Epoch 406 
2025-07-12 00:22:11.926838: Current learning rate: 0.00626 
2025-07-12 00:23:20.052979: train_loss -0.9714 
2025-07-12 00:23:20.054402: val_loss -0.9514 
2025-07-12 00:23:20.055451: Pseudo dice [np.float32(0.9558)] 
2025-07-12 00:23:20.056734: Epoch time: 68.13 s 
2025-07-12 00:23:21.212209:  
2025-07-12 00:23:21.213620: Epoch 407 
2025-07-12 00:23:21.214783: Current learning rate: 0.00625 
2025-07-12 00:24:29.356917: train_loss -0.97 
2025-07-12 00:24:29.358325: val_loss -0.9486 
2025-07-12 00:24:29.359481: Pseudo dice [np.float32(0.9526)] 
2025-07-12 00:24:29.360652: Epoch time: 68.15 s 
2025-07-12 00:24:30.278931:  
2025-07-12 00:24:30.280667: Epoch 408 
2025-07-12 00:24:30.281795: Current learning rate: 0.00624 
2025-07-12 00:25:38.344141: train_loss -0.9696 
2025-07-12 00:25:38.345317: val_loss -0.9483 
2025-07-12 00:25:38.346178: Pseudo dice [np.float32(0.9529)] 
2025-07-12 00:25:38.347084: Epoch time: 68.07 s 
2025-07-12 00:25:39.252192:  
2025-07-12 00:25:39.254046: Epoch 409 
2025-07-12 00:25:39.255174: Current learning rate: 0.00623 
2025-07-12 00:26:47.319603: train_loss -0.9719 
2025-07-12 00:26:47.320960: val_loss -0.9494 
2025-07-12 00:26:47.322000: Pseudo dice [np.float32(0.9531)] 
2025-07-12 00:26:47.323044: Epoch time: 68.07 s 
2025-07-12 00:26:48.240489:  
2025-07-12 00:26:48.241897: Epoch 410 
2025-07-12 00:26:48.242965: Current learning rate: 0.00622 
2025-07-12 00:27:56.552663: train_loss -0.9717 
2025-07-12 00:27:56.553802: val_loss -0.9497 
2025-07-12 00:27:56.554881: Pseudo dice [np.float32(0.9537)] 
2025-07-12 00:27:56.555812: Epoch time: 68.32 s 
2025-07-12 00:27:57.428879:  
2025-07-12 00:27:57.430482: Epoch 411 
2025-07-12 00:27:57.431559: Current learning rate: 0.00621 
2025-07-12 00:29:05.552486: train_loss -0.9719 
2025-07-12 00:29:05.553693: val_loss -0.9526 
2025-07-12 00:29:05.554788: Pseudo dice [np.float32(0.9566)] 
2025-07-12 00:29:05.555960: Epoch time: 68.13 s 
2025-07-12 00:29:06.424192:  
2025-07-12 00:29:06.425500: Epoch 412 
2025-07-12 00:29:06.426505: Current learning rate: 0.0062 
2025-07-12 00:30:14.500873: train_loss -0.9715 
2025-07-12 00:30:14.502187: val_loss -0.9489 
2025-07-12 00:30:14.503162: Pseudo dice [np.float32(0.9525)] 
2025-07-12 00:30:14.504221: Epoch time: 68.08 s 
2025-07-12 00:30:15.375997:  
2025-07-12 00:30:15.377840: Epoch 413 
2025-07-12 00:30:15.378921: Current learning rate: 0.00619 
2025-07-12 00:31:23.410473: train_loss -0.972 
2025-07-12 00:31:23.411641: val_loss -0.9476 
2025-07-12 00:31:23.412571: Pseudo dice [np.float32(0.9519)] 
2025-07-12 00:31:23.413445: Epoch time: 68.04 s 
2025-07-12 00:31:24.278561:  
2025-07-12 00:31:24.280412: Epoch 414 
2025-07-12 00:31:24.281571: Current learning rate: 0.00618 
2025-07-12 00:32:32.469188: train_loss -0.9734 
2025-07-12 00:32:32.470581: val_loss -0.9503 
2025-07-12 00:32:32.471605: Pseudo dice [np.float32(0.954)] 
2025-07-12 00:32:32.472656: Epoch time: 68.19 s 
2025-07-12 00:32:33.345308:  
2025-07-12 00:32:33.346785: Epoch 415 
2025-07-12 00:32:33.347766: Current learning rate: 0.00617 
2025-07-12 00:33:41.409344: train_loss -0.9725 
2025-07-12 00:33:41.410896: val_loss -0.9525 
2025-07-12 00:33:41.411796: Pseudo dice [np.float32(0.956)] 
2025-07-12 00:33:41.412961: Epoch time: 68.07 s 
2025-07-12 00:33:42.268050:  
2025-07-12 00:33:42.269710: Epoch 416 
2025-07-12 00:33:42.270692: Current learning rate: 0.00616 
2025-07-12 00:34:50.295471: train_loss -0.9719 
2025-07-12 00:34:50.296811: val_loss -0.9538 
2025-07-12 00:34:50.297733: Pseudo dice [np.float32(0.958)] 
2025-07-12 00:34:50.298615: Epoch time: 68.03 s 
2025-07-12 00:34:51.164559:  
2025-07-12 00:34:51.166495: Epoch 417 
2025-07-12 00:34:51.167634: Current learning rate: 0.00615 
2025-07-12 00:35:59.344594: train_loss -0.9734 
2025-07-12 00:35:59.345913: val_loss -0.9498 
2025-07-12 00:35:59.346920: Pseudo dice [np.float32(0.9535)] 
2025-07-12 00:35:59.348114: Epoch time: 68.18 s 
2025-07-12 00:36:00.222399:  
2025-07-12 00:36:00.223848: Epoch 418 
2025-07-12 00:36:00.225209: Current learning rate: 0.00614 
2025-07-12 00:37:08.312666: train_loss -0.9743 
2025-07-12 00:37:08.313820: val_loss -0.9493 
2025-07-12 00:37:08.314792: Pseudo dice [np.float32(0.9534)] 
2025-07-12 00:37:08.315746: Epoch time: 68.09 s 
2025-07-12 00:37:09.203021:  
2025-07-12 00:37:09.204750: Epoch 419 
2025-07-12 00:37:09.205828: Current learning rate: 0.00613 
2025-07-12 00:38:17.270023: train_loss -0.9734 
2025-07-12 00:38:17.271331: val_loss -0.9511 
2025-07-12 00:38:17.272570: Pseudo dice [np.float32(0.9556)] 
2025-07-12 00:38:17.273954: Epoch time: 68.07 s 
2025-07-12 00:38:18.156360:  
2025-07-12 00:38:18.158129: Epoch 420 
2025-07-12 00:38:18.159292: Current learning rate: 0.00612 
2025-07-12 00:39:26.129192: train_loss -0.9726 
2025-07-12 00:39:26.130538: val_loss -0.9472 
2025-07-12 00:39:26.131728: Pseudo dice [np.float32(0.9517)] 
2025-07-12 00:39:26.132833: Epoch time: 67.98 s 
2025-07-12 00:39:27.003439:  
2025-07-12 00:39:27.005178: Epoch 421 
2025-07-12 00:39:27.006420: Current learning rate: 0.00612 
2025-07-12 00:40:35.210053: train_loss -0.9714 
2025-07-12 00:40:35.211186: val_loss -0.9522 
2025-07-12 00:40:35.212407: Pseudo dice [np.float32(0.9564)] 
2025-07-12 00:40:35.213471: Epoch time: 68.21 s 
2025-07-12 00:40:36.096993:  
2025-07-12 00:40:36.098873: Epoch 422 
2025-07-12 00:40:36.100094: Current learning rate: 0.00611 
2025-07-12 00:41:44.098629: train_loss -0.9721 
2025-07-12 00:41:44.099904: val_loss -0.9502 
2025-07-12 00:41:44.101064: Pseudo dice [np.float32(0.9548)] 
2025-07-12 00:41:44.102009: Epoch time: 68.01 s 
2025-07-12 00:41:44.991837:  
2025-07-12 00:41:44.993562: Epoch 423 
2025-07-12 00:41:44.994781: Current learning rate: 0.0061 
2025-07-12 00:42:53.013217: train_loss -0.9735 
2025-07-12 00:42:53.014747: val_loss -0.9532 
2025-07-12 00:42:53.016179: Pseudo dice [np.float32(0.9578)] 
2025-07-12 00:42:53.017274: Epoch time: 68.02 s 
2025-07-12 00:42:53.899194:  
2025-07-12 00:42:53.900918: Epoch 424 
2025-07-12 00:42:53.901969: Current learning rate: 0.00609 
2025-07-12 00:44:02.146988: train_loss -0.9743 
2025-07-12 00:44:02.148618: val_loss -0.9508 
2025-07-12 00:44:02.149761: Pseudo dice [np.float32(0.9543)] 
2025-07-12 00:44:02.150880: Epoch time: 68.25 s 
2025-07-12 00:44:03.007072:  
2025-07-12 00:44:03.008905: Epoch 425 
2025-07-12 00:44:03.010031: Current learning rate: 0.00608 
2025-07-12 00:45:11.443487: train_loss -0.9743 
2025-07-12 00:45:11.444794: val_loss -0.9508 
2025-07-12 00:45:11.445749: Pseudo dice [np.float32(0.9557)] 
2025-07-12 00:45:11.446905: Epoch time: 68.44 s 
2025-07-12 00:45:12.319297:  
2025-07-12 00:45:12.321190: Epoch 426 
2025-07-12 00:45:12.322352: Current learning rate: 0.00607 
2025-07-12 00:46:20.391294: train_loss -0.9733 
2025-07-12 00:46:20.392556: val_loss -0.9477 
2025-07-12 00:46:20.393703: Pseudo dice [np.float32(0.9527)] 
2025-07-12 00:46:20.394903: Epoch time: 68.08 s 
2025-07-12 00:46:21.264448:  
2025-07-12 00:46:21.266078: Epoch 427 
2025-07-12 00:46:21.267160: Current learning rate: 0.00606 
2025-07-12 00:47:29.361955: train_loss -0.973 
2025-07-12 00:47:29.363554: val_loss -0.9517 
2025-07-12 00:47:29.364616: Pseudo dice [np.float32(0.9557)] 
2025-07-12 00:47:29.365622: Epoch time: 68.1 s 
2025-07-12 00:47:30.245311:  
2025-07-12 00:47:30.246804: Epoch 428 
2025-07-12 00:47:30.247935: Current learning rate: 0.00605 
2025-07-12 00:48:38.425923: train_loss -0.9735 
2025-07-12 00:48:38.427548: val_loss -0.9517 
2025-07-12 00:48:38.428890: Pseudo dice [np.float32(0.956)] 
2025-07-12 00:48:38.430061: Epoch time: 68.18 s 
2025-07-12 00:48:39.307117:  
2025-07-12 00:48:39.308393: Epoch 429 
2025-07-12 00:48:39.309497: Current learning rate: 0.00604 
2025-07-12 00:49:47.259779: train_loss -0.9739 
2025-07-12 00:49:47.261262: val_loss -0.9491 
2025-07-12 00:49:47.262426: Pseudo dice [np.float32(0.9534)] 
2025-07-12 00:49:47.263454: Epoch time: 67.96 s 
2025-07-12 00:49:48.135545:  
2025-07-12 00:49:48.137460: Epoch 430 
2025-07-12 00:49:48.138667: Current learning rate: 0.00603 
2025-07-12 00:50:56.092788: train_loss -0.9728 
2025-07-12 00:50:56.093879: val_loss -0.9508 
2025-07-12 00:50:56.094960: Pseudo dice [np.float32(0.9545)] 
2025-07-12 00:50:56.095858: Epoch time: 67.96 s 
2025-07-12 00:50:56.990678:  
2025-07-12 00:50:56.992098: Epoch 431 
2025-07-12 00:50:56.993226: Current learning rate: 0.00602 
2025-07-12 00:52:05.122452: train_loss -0.9735 
2025-07-12 00:52:05.124074: val_loss -0.9494 
2025-07-12 00:52:05.125031: Pseudo dice [np.float32(0.9535)] 
2025-07-12 00:52:05.126216: Epoch time: 68.14 s 
2025-07-12 00:52:06.007235:  
2025-07-12 00:52:06.009203: Epoch 432 
2025-07-12 00:52:06.010377: Current learning rate: 0.00601 
2025-07-12 00:53:13.963890: train_loss -0.9739 
2025-07-12 00:53:13.965777: val_loss -0.9514 
2025-07-12 00:53:13.966827: Pseudo dice [np.float32(0.9552)] 
2025-07-12 00:53:13.967717: Epoch time: 67.96 s 
2025-07-12 00:53:14.844690:  
2025-07-12 00:53:14.846153: Epoch 433 
2025-07-12 00:53:14.847124: Current learning rate: 0.006 
2025-07-12 00:54:23.004478: train_loss -0.9725 
2025-07-12 00:54:23.005784: val_loss -0.9498 
2025-07-12 00:54:23.006761: Pseudo dice [np.float32(0.9534)] 
2025-07-12 00:54:23.007797: Epoch time: 68.16 s 
2025-07-12 00:54:24.405431:  
2025-07-12 00:54:24.406985: Epoch 434 
2025-07-12 00:54:24.408142: Current learning rate: 0.00599 
2025-07-12 00:55:32.587852: train_loss -0.9743 
2025-07-12 00:55:32.589049: val_loss -0.9527 
2025-07-12 00:55:32.590112: Pseudo dice [np.float32(0.957)] 
2025-07-12 00:55:32.591297: Epoch time: 68.19 s 
2025-07-12 00:55:33.469743:  
2025-07-12 00:55:33.471573: Epoch 435 
2025-07-12 00:55:33.472688: Current learning rate: 0.00598 
2025-07-12 00:56:41.646566: train_loss -0.9731 
2025-07-12 00:56:41.647928: val_loss -0.9535 
2025-07-12 00:56:41.648977: Pseudo dice [np.float32(0.9578)] 
2025-07-12 00:56:41.651056: Epoch time: 68.18 s 
2025-07-12 00:56:42.540675:  
2025-07-12 00:56:42.542239: Epoch 436 
2025-07-12 00:56:42.543639: Current learning rate: 0.00597 
2025-07-12 00:57:50.586986: train_loss -0.9746 
2025-07-12 00:57:50.588260: val_loss -0.9516 
2025-07-12 00:57:50.589503: Pseudo dice [np.float32(0.956)] 
2025-07-12 00:57:50.590480: Epoch time: 68.05 s 
2025-07-12 00:57:51.478975:  
2025-07-12 00:57:51.480130: Epoch 437 
2025-07-12 00:57:51.481147: Current learning rate: 0.00596 
2025-07-12 00:58:59.517366: train_loss -0.9744 
2025-07-12 00:58:59.518710: val_loss -0.952 
2025-07-12 00:58:59.519835: Pseudo dice [np.float32(0.9559)] 
2025-07-12 00:58:59.520859: Epoch time: 68.04 s 
2025-07-12 00:59:00.395593:  
2025-07-12 00:59:00.397026: Epoch 438 
2025-07-12 00:59:00.398357: Current learning rate: 0.00595 
2025-07-12 01:00:08.612319: train_loss -0.9733 
2025-07-12 01:00:08.613700: val_loss -0.9519 
2025-07-12 01:00:08.614773: Pseudo dice [np.float32(0.9563)] 
2025-07-12 01:00:08.615860: Epoch time: 68.22 s 
2025-07-12 01:00:09.490177:  
2025-07-12 01:00:09.491882: Epoch 439 
2025-07-12 01:00:09.493026: Current learning rate: 0.00594 
2025-07-12 01:01:17.491619: train_loss -0.9728 
2025-07-12 01:01:17.493175: val_loss -0.9499 
2025-07-12 01:01:17.494088: Pseudo dice [np.float32(0.9544)] 
2025-07-12 01:01:17.495136: Epoch time: 68.0 s 
2025-07-12 01:01:18.380655:  
2025-07-12 01:01:18.382166: Epoch 440 
2025-07-12 01:01:18.383575: Current learning rate: 0.00593 
2025-07-12 01:02:26.421813: train_loss -0.9735 
2025-07-12 01:02:26.423021: val_loss -0.9507 
2025-07-12 01:02:26.424141: Pseudo dice [np.float32(0.9554)] 
2025-07-12 01:02:26.425404: Epoch time: 68.04 s 
2025-07-12 01:02:27.401878:  
2025-07-12 01:02:27.403388: Epoch 441 
2025-07-12 01:02:27.404452: Current learning rate: 0.00592 
2025-07-12 01:03:35.338631: train_loss -0.9739 
2025-07-12 01:03:35.339786: val_loss -0.9557 
2025-07-12 01:03:35.340777: Pseudo dice [np.float32(0.9591)] 
2025-07-12 01:03:35.341780: Epoch time: 67.94 s 
2025-07-12 01:03:36.222115:  
2025-07-12 01:03:36.223698: Epoch 442 
2025-07-12 01:03:36.224836: Current learning rate: 0.00592 
2025-07-12 01:04:44.354625: train_loss -0.975 
2025-07-12 01:04:44.355839: val_loss -0.9521 
2025-07-12 01:04:44.356773: Pseudo dice [np.float32(0.9559)] 
2025-07-12 01:04:44.357746: Epoch time: 68.14 s 
2025-07-12 01:04:45.239175:  
2025-07-12 01:04:45.240632: Epoch 443 
2025-07-12 01:04:45.241666: Current learning rate: 0.00591 
2025-07-12 01:05:53.266532: train_loss -0.9735 
2025-07-12 01:05:53.267780: val_loss -0.9523 
2025-07-12 01:05:53.268732: Pseudo dice [np.float32(0.9565)] 
2025-07-12 01:05:53.269620: Epoch time: 68.03 s 
2025-07-12 01:05:54.149607:  
2025-07-12 01:05:54.151031: Epoch 444 
2025-07-12 01:05:54.152054: Current learning rate: 0.0059 
2025-07-12 01:07:02.112368: train_loss -0.9738 
2025-07-12 01:07:02.113636: val_loss -0.9484 
2025-07-12 01:07:02.114553: Pseudo dice [np.float32(0.9524)] 
2025-07-12 01:07:02.115574: Epoch time: 67.97 s 
2025-07-12 01:07:02.978943:  
2025-07-12 01:07:02.980747: Epoch 445 
2025-07-12 01:07:02.981962: Current learning rate: 0.00589 
2025-07-12 01:08:11.034113: train_loss -0.9728 
2025-07-12 01:08:11.035356: val_loss -0.9547 
2025-07-12 01:08:11.036687: Pseudo dice [np.float32(0.9587)] 
2025-07-12 01:08:11.037902: Epoch time: 68.06 s 
2025-07-12 01:08:11.904757:  
2025-07-12 01:08:11.906392: Epoch 446 
2025-07-12 01:08:11.907535: Current learning rate: 0.00588 
2025-07-12 01:09:19.711979: train_loss -0.9742 
2025-07-12 01:09:19.713359: val_loss -0.9492 
2025-07-12 01:09:19.714377: Pseudo dice [np.float32(0.9534)] 
2025-07-12 01:09:19.715313: Epoch time: 67.81 s 
2025-07-12 01:09:20.583080:  
2025-07-12 01:09:20.584565: Epoch 447 
2025-07-12 01:09:20.585839: Current learning rate: 0.00587 
2025-07-12 01:10:28.451244: train_loss -0.9744 
2025-07-12 01:10:28.452704: val_loss -0.9544 
2025-07-12 01:10:28.453570: Pseudo dice [np.float32(0.9578)] 
2025-07-12 01:10:28.454493: Epoch time: 67.87 s 
2025-07-12 01:10:29.333363:  
2025-07-12 01:10:29.335142: Epoch 448 
2025-07-12 01:10:29.336346: Current learning rate: 0.00586 
2025-07-12 01:11:37.092142: train_loss -0.9749 
2025-07-12 01:11:37.093482: val_loss -0.9518 
2025-07-12 01:11:37.094598: Pseudo dice [np.float32(0.956)] 
2025-07-12 01:11:37.095849: Epoch time: 67.76 s 
2025-07-12 01:11:37.959834:  
2025-07-12 01:11:37.961084: Epoch 449 
2025-07-12 01:11:37.962228: Current learning rate: 0.00585 
2025-07-12 01:12:45.888210: train_loss -0.9734 
2025-07-12 01:12:45.889503: val_loss -0.9511 
2025-07-12 01:12:45.890547: Pseudo dice [np.float32(0.9541)] 
2025-07-12 01:12:45.891810: Epoch time: 67.93 s 
2025-07-12 01:12:47.813933:  
2025-07-12 01:12:47.815581: Epoch 450 
2025-07-12 01:12:47.816785: Current learning rate: 0.00584 
2025-07-12 01:13:55.678421: train_loss -0.9725 
2025-07-12 01:13:55.679720: val_loss -0.953 
2025-07-12 01:13:55.680849: Pseudo dice [np.float32(0.9577)] 
2025-07-12 01:13:55.681993: Epoch time: 67.87 s 
2025-07-12 01:13:56.558722:  
2025-07-12 01:13:56.560585: Epoch 451 
2025-07-12 01:13:56.561740: Current learning rate: 0.00583 
2025-07-12 01:15:04.470788: train_loss -0.9737 
2025-07-12 01:15:04.472092: val_loss -0.953 
2025-07-12 01:15:04.473241: Pseudo dice [np.float32(0.9574)] 
2025-07-12 01:15:04.474215: Epoch time: 67.92 s 
2025-07-12 01:15:05.336412:  
2025-07-12 01:15:05.337837: Epoch 452 
2025-07-12 01:15:05.339031: Current learning rate: 0.00582 
2025-07-12 01:16:13.389767: train_loss -0.975 
2025-07-12 01:16:13.391199: val_loss -0.9515 
2025-07-12 01:16:13.392318: Pseudo dice [np.float32(0.9549)] 
2025-07-12 01:16:13.393610: Epoch time: 68.06 s 
2025-07-12 01:16:14.243445:  
2025-07-12 01:16:14.245073: Epoch 453 
2025-07-12 01:16:14.246145: Current learning rate: 0.00581 
2025-07-12 01:17:22.278203: train_loss -0.9755 
2025-07-12 01:17:22.280777: val_loss -0.9525 
2025-07-12 01:17:22.281790: Pseudo dice [np.float32(0.9565)] 
2025-07-12 01:17:22.282754: Epoch time: 68.04 s 
2025-07-12 01:17:23.151046:  
2025-07-12 01:17:23.152849: Epoch 454 
2025-07-12 01:17:23.154084: Current learning rate: 0.0058 
2025-07-12 01:18:31.041131: train_loss -0.9747 
2025-07-12 01:18:31.042627: val_loss -0.9515 
2025-07-12 01:18:31.043768: Pseudo dice [np.float32(0.9562)] 
2025-07-12 01:18:31.045190: Epoch time: 67.89 s 
2025-07-12 01:18:31.893470:  
2025-07-12 01:18:31.895068: Epoch 455 
2025-07-12 01:18:31.896161: Current learning rate: 0.00579 
2025-07-12 01:19:39.765093: train_loss -0.9749 
2025-07-12 01:19:39.766279: val_loss -0.9514 
2025-07-12 01:19:39.767217: Pseudo dice [np.float32(0.9557)] 
2025-07-12 01:19:39.768206: Epoch time: 67.88 s 
2025-07-12 01:19:40.635498:  
2025-07-12 01:19:40.637078: Epoch 456 
2025-07-12 01:19:40.638191: Current learning rate: 0.00578 
2025-07-12 01:20:48.585944: train_loss -0.975 
2025-07-12 01:20:48.587153: val_loss -0.9513 
2025-07-12 01:20:48.588237: Pseudo dice [np.float32(0.9558)] 
2025-07-12 01:20:48.589173: Epoch time: 67.95 s 
2025-07-12 01:20:49.457563:  
2025-07-12 01:20:49.459002: Epoch 457 
2025-07-12 01:20:49.460121: Current learning rate: 0.00577 
2025-07-12 01:21:57.286026: train_loss -0.9754 
2025-07-12 01:21:57.287715: val_loss -0.9544 
2025-07-12 01:21:57.288678: Pseudo dice [np.float32(0.9584)] 
2025-07-12 01:21:57.289797: Epoch time: 67.83 s 
2025-07-12 01:21:57.291067: Yayy! New best EMA pseudo Dice: 0.9562000036239624 
2025-07-12 01:21:59.217675:  
2025-07-12 01:21:59.219313: Epoch 458 
2025-07-12 01:21:59.220428: Current learning rate: 0.00576 
2025-07-12 01:23:07.073360: train_loss -0.9742 
2025-07-12 01:23:07.074629: val_loss -0.9521 
2025-07-12 01:23:07.075583: Pseudo dice [np.float32(0.9557)] 
2025-07-12 01:23:07.076679: Epoch time: 67.86 s 
2025-07-12 01:23:07.941560:  
2025-07-12 01:23:07.943315: Epoch 459 
2025-07-12 01:23:07.944768: Current learning rate: 0.00575 
2025-07-12 01:24:15.945791: train_loss -0.974 
2025-07-12 01:24:15.947264: val_loss -0.9506 
2025-07-12 01:24:15.948285: Pseudo dice [np.float32(0.9551)] 
2025-07-12 01:24:15.949427: Epoch time: 68.01 s 
2025-07-12 01:24:16.811490:  
2025-07-12 01:24:16.813172: Epoch 460 
2025-07-12 01:24:16.814187: Current learning rate: 0.00574 
2025-07-12 01:25:24.676094: train_loss -0.9732 
2025-07-12 01:25:24.677568: val_loss -0.9521 
2025-07-12 01:25:24.678700: Pseudo dice [np.float32(0.9557)] 
2025-07-12 01:25:24.680084: Epoch time: 67.87 s 
2025-07-12 01:25:25.556511:  
2025-07-12 01:25:25.558180: Epoch 461 
2025-07-12 01:25:25.559320: Current learning rate: 0.00573 
2025-07-12 01:26:33.436255: train_loss -0.9753 
2025-07-12 01:26:33.437473: val_loss -0.9517 
2025-07-12 01:26:33.438636: Pseudo dice [np.float32(0.9562)] 
2025-07-12 01:26:33.439543: Epoch time: 67.88 s 
2025-07-12 01:26:34.312737:  
2025-07-12 01:26:34.314333: Epoch 462 
2025-07-12 01:26:34.315524: Current learning rate: 0.00572 
2025-07-12 01:27:42.254279: train_loss -0.9746 
2025-07-12 01:27:42.255656: val_loss -0.9519 
2025-07-12 01:27:42.256707: Pseudo dice [np.float32(0.9558)] 
2025-07-12 01:27:42.257968: Epoch time: 67.94 s 
2025-07-12 01:27:43.128106:  
2025-07-12 01:27:43.129903: Epoch 463 
2025-07-12 01:27:43.131131: Current learning rate: 0.00571 
2025-07-12 01:28:51.308448: train_loss -0.9748 
2025-07-12 01:28:51.309869: val_loss -0.9534 
2025-07-12 01:28:51.310956: Pseudo dice [np.float32(0.9574)] 
2025-07-12 01:28:51.312039: Epoch time: 68.18 s 
2025-07-12 01:28:52.186207:  
2025-07-12 01:28:52.187839: Epoch 464 
2025-07-12 01:28:52.188907: Current learning rate: 0.0057 
2025-07-12 01:30:00.194484: train_loss -0.9746 
2025-07-12 01:30:00.195792: val_loss -0.9505 
2025-07-12 01:30:00.196781: Pseudo dice [np.float32(0.9543)] 
2025-07-12 01:30:00.197907: Epoch time: 68.01 s 
2025-07-12 01:30:01.061244:  
2025-07-12 01:30:01.062819: Epoch 465 
2025-07-12 01:30:01.064112: Current learning rate: 0.0057 
2025-07-12 01:31:08.942344: train_loss -0.9752 
2025-07-12 01:31:08.943486: val_loss -0.9496 
2025-07-12 01:31:08.944585: Pseudo dice [np.float32(0.9536)] 
2025-07-12 01:31:08.945650: Epoch time: 67.88 s 
2025-07-12 01:31:09.806802:  
2025-07-12 01:31:09.808574: Epoch 466 
2025-07-12 01:31:09.809820: Current learning rate: 0.00569 
2025-07-12 01:32:17.883878: train_loss -0.9754 
2025-07-12 01:32:17.885457: val_loss -0.9536 
2025-07-12 01:32:17.886591: Pseudo dice [np.float32(0.9576)] 
2025-07-12 01:32:17.887658: Epoch time: 68.08 s 
2025-07-12 01:32:18.749573:  
2025-07-12 01:32:18.751100: Epoch 467 
2025-07-12 01:32:18.752209: Current learning rate: 0.00568 
2025-07-12 01:33:26.797700: train_loss -0.9746 
2025-07-12 01:33:26.798895: val_loss -0.95 
2025-07-12 01:33:26.799939: Pseudo dice [np.float32(0.9539)] 
2025-07-12 01:33:26.801138: Epoch time: 68.05 s 
2025-07-12 01:33:27.661433:  
2025-07-12 01:33:27.662740: Epoch 468 
2025-07-12 01:33:27.663892: Current learning rate: 0.00567 
2025-07-12 01:34:35.689401: train_loss -0.9738 
2025-07-12 01:34:35.690687: val_loss -0.9474 
2025-07-12 01:34:35.691814: Pseudo dice [np.float32(0.9511)] 
2025-07-12 01:34:35.692971: Epoch time: 68.03 s 
2025-07-12 01:34:36.559511:  
2025-07-12 01:34:36.561303: Epoch 469 
2025-07-12 01:34:36.562514: Current learning rate: 0.00566 
2025-07-12 01:35:44.553380: train_loss -0.9747 
2025-07-12 01:35:44.554776: val_loss -0.9523 
2025-07-12 01:35:44.555766: Pseudo dice [np.float32(0.9559)] 
2025-07-12 01:35:44.556714: Epoch time: 68.0 s 
2025-07-12 01:35:45.427347:  
2025-07-12 01:35:45.429252: Epoch 470 
2025-07-12 01:35:45.430629: Current learning rate: 0.00565 
2025-07-12 01:36:53.476435: train_loss -0.9745 
2025-07-12 01:36:53.477724: val_loss -0.9507 
2025-07-12 01:36:53.478734: Pseudo dice [np.float32(0.9549)] 
2025-07-12 01:36:53.479622: Epoch time: 68.05 s 
2025-07-12 01:36:54.349994:  
2025-07-12 01:36:54.351548: Epoch 471 
2025-07-12 01:36:54.352607: Current learning rate: 0.00564 
2025-07-12 01:38:02.332701: train_loss -0.9737 
2025-07-12 01:38:02.334165: val_loss -0.9513 
2025-07-12 01:38:02.335233: Pseudo dice [np.float32(0.9546)] 
2025-07-12 01:38:02.336300: Epoch time: 67.99 s 
2025-07-12 01:38:03.201750:  
2025-07-12 01:38:03.203317: Epoch 472 
2025-07-12 01:38:03.204530: Current learning rate: 0.00563 
2025-07-12 01:39:11.308606: train_loss -0.9735 
2025-07-12 01:39:11.310121: val_loss -0.951 
2025-07-12 01:39:11.311151: Pseudo dice [np.float32(0.9548)] 
2025-07-12 01:39:11.312283: Epoch time: 68.11 s 
2025-07-12 01:39:12.170415:  
2025-07-12 01:39:12.171970: Epoch 473 
2025-07-12 01:39:12.173031: Current learning rate: 0.00562 
2025-07-12 01:40:20.260590: train_loss -0.9734 
2025-07-12 01:40:20.261808: val_loss -0.9515 
2025-07-12 01:40:20.262883: Pseudo dice [np.float32(0.9563)] 
2025-07-12 01:40:20.264064: Epoch time: 68.09 s 
2025-07-12 01:40:21.132761:  
2025-07-12 01:40:21.134214: Epoch 474 
2025-07-12 01:40:21.135241: Current learning rate: 0.00561 
2025-07-12 01:41:29.088818: train_loss -0.9745 
2025-07-12 01:41:29.090214: val_loss -0.9515 
2025-07-12 01:41:29.091199: Pseudo dice [np.float32(0.9558)] 
2025-07-12 01:41:29.092097: Epoch time: 67.96 s 
2025-07-12 01:41:29.952487:  
2025-07-12 01:41:29.953896: Epoch 475 
2025-07-12 01:41:29.954992: Current learning rate: 0.0056 
2025-07-12 01:42:37.883616: train_loss -0.9742 
2025-07-12 01:42:37.884772: val_loss -0.9525 
2025-07-12 01:42:37.885882: Pseudo dice [np.float32(0.9569)] 
2025-07-12 01:42:37.886853: Epoch time: 67.93 s 
2025-07-12 01:42:38.752108:  
2025-07-12 01:42:38.753668: Epoch 476 
2025-07-12 01:42:38.754710: Current learning rate: 0.00559 
2025-07-12 01:43:46.671259: train_loss -0.9736 
2025-07-12 01:43:46.672474: val_loss -0.9514 
2025-07-12 01:43:46.673495: Pseudo dice [np.float32(0.9554)] 
2025-07-12 01:43:46.674539: Epoch time: 67.92 s 
2025-07-12 01:43:47.541458:  
2025-07-12 01:43:47.542835: Epoch 477 
2025-07-12 01:43:47.544310: Current learning rate: 0.00558 
2025-07-12 01:44:55.610307: train_loss -0.9736 
2025-07-12 01:44:55.611772: val_loss -0.9519 
2025-07-12 01:44:55.612649: Pseudo dice [np.float32(0.9557)] 
2025-07-12 01:44:55.613582: Epoch time: 68.07 s 
2025-07-12 01:44:56.489930:  
2025-07-12 01:44:56.491536: Epoch 478 
2025-07-12 01:44:56.492681: Current learning rate: 0.00557 
2025-07-12 01:46:04.455366: train_loss -0.9752 
2025-07-12 01:46:04.456647: val_loss -0.9512 
2025-07-12 01:46:04.457932: Pseudo dice [np.float32(0.9555)] 
2025-07-12 01:46:04.459195: Epoch time: 67.97 s 
2025-07-12 01:46:05.333663:  
2025-07-12 01:46:05.335384: Epoch 479 
2025-07-12 01:46:05.337019: Current learning rate: 0.00556 
2025-07-12 01:47:13.346337: train_loss -0.9743 
2025-07-12 01:47:13.347628: val_loss -0.9514 
2025-07-12 01:47:13.348831: Pseudo dice [np.float32(0.9558)] 
2025-07-12 01:47:13.349965: Epoch time: 68.02 s 
2025-07-12 01:47:14.215165:  
2025-07-12 01:47:14.216588: Epoch 480 
2025-07-12 01:47:14.218157: Current learning rate: 0.00555 
2025-07-12 01:48:22.265081: train_loss -0.9741 
2025-07-12 01:48:22.266592: val_loss -0.9521 
2025-07-12 01:48:22.267900: Pseudo dice [np.float32(0.956)] 
2025-07-12 01:48:22.269062: Epoch time: 68.05 s 
2025-07-12 01:48:23.135965:  
2025-07-12 01:48:23.137554: Epoch 481 
2025-07-12 01:48:23.139425: Current learning rate: 0.00554 
2025-07-12 01:49:31.131663: train_loss -0.9756 
2025-07-12 01:49:31.132837: val_loss -0.9541 
2025-07-12 01:49:31.133685: Pseudo dice [np.float32(0.9577)] 
2025-07-12 01:49:31.134752: Epoch time: 68.0 s 
2025-07-12 01:49:32.034524:  
2025-07-12 01:49:32.036133: Epoch 482 
2025-07-12 01:49:32.037207: Current learning rate: 0.00553 
2025-07-12 01:50:39.991939: train_loss -0.9757 
2025-07-12 01:50:39.993461: val_loss -0.9509 
2025-07-12 01:50:39.994555: Pseudo dice [np.float32(0.9546)] 
2025-07-12 01:50:39.995503: Epoch time: 67.96 s 
2025-07-12 01:50:40.868380:  
2025-07-12 01:50:40.869706: Epoch 483 
2025-07-12 01:50:40.870787: Current learning rate: 0.00552 
2025-07-12 01:51:48.897623: train_loss -0.9756 
2025-07-12 01:51:48.898769: val_loss -0.9501 
2025-07-12 01:51:48.899777: Pseudo dice [np.float32(0.9542)] 
2025-07-12 01:51:48.900801: Epoch time: 68.03 s 
2025-07-12 01:51:49.762851:  
2025-07-12 01:51:49.764144: Epoch 484 
2025-07-12 01:51:49.765332: Current learning rate: 0.00551 
2025-07-12 01:52:58.053679: train_loss -0.9744 
2025-07-12 01:52:58.055586: val_loss -0.9519 
2025-07-12 01:52:58.056757: Pseudo dice [np.float32(0.9563)] 
2025-07-12 01:52:58.057915: Epoch time: 68.29 s 
2025-07-12 01:52:58.926192:  
2025-07-12 01:52:58.927729: Epoch 485 
2025-07-12 01:52:58.928746: Current learning rate: 0.0055 
2025-07-12 01:54:06.961485: train_loss -0.9764 
2025-07-12 01:54:06.962704: val_loss -0.9519 
2025-07-12 01:54:06.963701: Pseudo dice [np.float32(0.9554)] 
2025-07-12 01:54:06.964698: Epoch time: 68.04 s 
2025-07-12 01:54:07.826292:  
2025-07-12 01:54:07.827758: Epoch 486 
2025-07-12 01:54:07.829141: Current learning rate: 0.00549 
2025-07-12 01:55:15.767148: train_loss -0.9754 
2025-07-12 01:55:15.768718: val_loss -0.9519 
2025-07-12 01:55:15.769751: Pseudo dice [np.float32(0.9564)] 
2025-07-12 01:55:15.771003: Epoch time: 67.94 s 
2025-07-12 01:55:16.648234:  
2025-07-12 01:55:16.650001: Epoch 487 
2025-07-12 01:55:16.651177: Current learning rate: 0.00548 
2025-07-12 01:56:24.649527: train_loss -0.9757 
2025-07-12 01:56:24.651024: val_loss -0.9544 
2025-07-12 01:56:24.652155: Pseudo dice [np.float32(0.9583)] 
2025-07-12 01:56:24.653243: Epoch time: 68.0 s 
2025-07-12 01:56:25.747731:  
2025-07-12 01:56:25.749391: Epoch 488 
2025-07-12 01:56:25.750489: Current learning rate: 0.00547 
2025-07-12 01:57:33.626597: train_loss -0.9756 
2025-07-12 01:57:33.627920: val_loss -0.9526 
2025-07-12 01:57:33.629189: Pseudo dice [np.float32(0.9564)] 
2025-07-12 01:57:33.630692: Epoch time: 67.88 s 
2025-07-12 01:57:34.503148:  
2025-07-12 01:57:34.504915: Epoch 489 
2025-07-12 01:57:34.506124: Current learning rate: 0.00546 
2025-07-12 01:58:42.472102: train_loss -0.9754 
2025-07-12 01:58:42.473539: val_loss -0.9504 
2025-07-12 01:58:42.474541: Pseudo dice [np.float32(0.9542)] 
2025-07-12 01:58:42.475463: Epoch time: 67.97 s 
2025-07-12 01:58:43.355848:  
2025-07-12 01:58:43.357359: Epoch 490 
2025-07-12 01:58:43.359283: Current learning rate: 0.00546 
2025-07-12 01:59:51.314884: train_loss -0.9758 
2025-07-12 01:59:51.316021: val_loss -0.95 
2025-07-12 01:59:51.317171: Pseudo dice [np.float32(0.954)] 
2025-07-12 01:59:51.318233: Epoch time: 67.96 s 
2025-07-12 01:59:52.172825:  
2025-07-12 01:59:52.174634: Epoch 491 
2025-07-12 01:59:52.176083: Current learning rate: 0.00545 
2025-07-12 02:01:00.280531: train_loss -0.9761 
2025-07-12 02:01:00.282035: val_loss -0.9512 
2025-07-12 02:01:00.283003: Pseudo dice [np.float32(0.9553)] 
2025-07-12 02:01:00.284039: Epoch time: 68.11 s 
2025-07-12 02:01:01.153777:  
2025-07-12 02:01:01.155354: Epoch 492 
2025-07-12 02:01:01.157169: Current learning rate: 0.00544 
2025-07-12 02:02:09.120194: train_loss -0.9754 
2025-07-12 02:02:09.121509: val_loss -0.9499 
2025-07-12 02:02:09.122690: Pseudo dice [np.float32(0.9534)] 
2025-07-12 02:02:09.123698: Epoch time: 67.97 s 
2025-07-12 02:02:09.995834:  
2025-07-12 02:02:09.997422: Epoch 493 
2025-07-12 02:02:09.999169: Current learning rate: 0.00543 
2025-07-12 02:03:18.070353: train_loss -0.9746 
2025-07-12 02:03:18.071616: val_loss -0.9499 
2025-07-12 02:03:18.072538: Pseudo dice [np.float32(0.9545)] 
2025-07-12 02:03:18.073462: Epoch time: 68.08 s 
2025-07-12 02:03:18.942707:  
2025-07-12 02:03:18.944476: Epoch 494 
2025-07-12 02:03:18.945710: Current learning rate: 0.00542 
2025-07-12 02:04:27.224090: train_loss -0.975 
2025-07-12 02:04:27.225202: val_loss -0.951 
2025-07-12 02:04:27.226277: Pseudo dice [np.float32(0.9551)] 
2025-07-12 02:04:27.227422: Epoch time: 68.28 s 
2025-07-12 02:04:28.331834:  
2025-07-12 02:04:28.333427: Epoch 495 
2025-07-12 02:04:28.335005: Current learning rate: 0.00541 
2025-07-12 02:05:36.575254: train_loss -0.9753 
2025-07-12 02:05:36.576583: val_loss -0.9542 
2025-07-12 02:05:36.577702: Pseudo dice [np.float32(0.9586)] 
2025-07-12 02:05:36.578772: Epoch time: 68.25 s 
2025-07-12 02:05:37.464593:  
2025-07-12 02:05:37.466033: Epoch 496 
2025-07-12 02:05:37.467262: Current learning rate: 0.0054 
2025-07-12 02:06:45.761860: train_loss -0.9763 
2025-07-12 02:06:45.763005: val_loss -0.9508 
2025-07-12 02:06:45.764086: Pseudo dice [np.float32(0.9552)] 
2025-07-12 02:06:45.764971: Epoch time: 68.3 s 
2025-07-12 02:06:46.644122:  
2025-07-12 02:06:46.645543: Epoch 497 
2025-07-12 02:06:46.646457: Current learning rate: 0.00539 
2025-07-12 02:07:54.815530: train_loss -0.9759 
2025-07-12 02:07:54.816664: val_loss -0.9526 
2025-07-12 02:07:54.817579: Pseudo dice [np.float32(0.9562)] 
2025-07-12 02:07:54.818490: Epoch time: 68.17 s 
2025-07-12 02:07:55.698097:  
2025-07-12 02:07:55.699581: Epoch 498 
2025-07-12 02:07:55.700714: Current learning rate: 0.00538 
2025-07-12 02:09:03.952924: train_loss -0.9763 
2025-07-12 02:09:03.954167: val_loss -0.949 
2025-07-12 02:09:03.955131: Pseudo dice [np.float32(0.9531)] 
2025-07-12 02:09:03.956127: Epoch time: 68.26 s 
2025-07-12 02:09:04.819115:  
2025-07-12 02:09:04.820488: Epoch 499 
2025-07-12 02:09:04.821527: Current learning rate: 0.00537 
2025-07-12 02:10:13.097038: train_loss -0.9749 
2025-07-12 02:10:13.098276: val_loss -0.9503 
2025-07-12 02:10:13.099195: Pseudo dice [np.float32(0.9539)] 
2025-07-12 02:10:13.100217: Epoch time: 68.28 s 
2025-07-12 02:10:15.186281:  
2025-07-12 02:10:15.187539: Epoch 500 
2025-07-12 02:10:15.188605: Current learning rate: 0.00536 
2025-07-12 02:11:23.223142: train_loss -0.9727 
2025-07-12 02:11:23.224393: val_loss -0.9478 
2025-07-12 02:11:23.225251: Pseudo dice [np.float32(0.9535)] 
2025-07-12 02:11:23.226276: Epoch time: 68.04 s 
2025-07-12 02:11:24.099192:  
2025-07-12 02:11:24.100713: Epoch 501 
2025-07-12 02:11:24.101797: Current learning rate: 0.00535 
2025-07-12 02:12:32.176250: train_loss -0.9742 
2025-07-12 02:12:32.177606: val_loss -0.9472 
2025-07-12 02:12:32.178907: Pseudo dice [np.float32(0.9519)] 
2025-07-12 02:12:32.179858: Epoch time: 68.08 s 
2025-07-12 02:12:33.271980:  
2025-07-12 02:12:33.273522: Epoch 502 
2025-07-12 02:12:33.274740: Current learning rate: 0.00534 
2025-07-12 02:13:41.387423: train_loss -0.9743 
2025-07-12 02:13:41.388583: val_loss -0.9533 
2025-07-12 02:13:41.389493: Pseudo dice [np.float32(0.9572)] 
2025-07-12 02:13:41.390567: Epoch time: 68.12 s 
2025-07-12 02:13:42.259221:  
2025-07-12 02:13:42.260645: Epoch 503 
2025-07-12 02:13:42.261693: Current learning rate: 0.00533 
2025-07-12 02:14:50.404564: train_loss -0.9747 
2025-07-12 02:14:50.405838: val_loss -0.9502 
2025-07-12 02:14:50.407008: Pseudo dice [np.float32(0.955)] 
2025-07-12 02:14:50.407940: Epoch time: 68.15 s 
2025-07-12 02:14:51.287716:  
2025-07-12 02:14:51.289078: Epoch 504 
2025-07-12 02:14:51.290319: Current learning rate: 0.00532 
2025-07-12 02:15:59.331802: train_loss -0.9742 
2025-07-12 02:15:59.332952: val_loss -0.9469 
2025-07-12 02:15:59.333819: Pseudo dice [np.float32(0.9508)] 
2025-07-12 02:15:59.334776: Epoch time: 68.05 s 
2025-07-12 02:16:00.203925:  
2025-07-12 02:16:00.205259: Epoch 505 
2025-07-12 02:16:00.206346: Current learning rate: 0.00531 
2025-07-12 02:17:08.500105: train_loss -0.9751 
2025-07-12 02:17:08.501412: val_loss -0.9489 
2025-07-12 02:17:08.502315: Pseudo dice [np.float32(0.9525)] 
2025-07-12 02:17:08.503182: Epoch time: 68.3 s 
2025-07-12 02:17:09.355129:  
2025-07-12 02:17:09.356651: Epoch 506 
2025-07-12 02:17:09.357659: Current learning rate: 0.0053 
2025-07-12 02:18:17.371288: train_loss -0.9739 
2025-07-12 02:18:17.372507: val_loss -0.949 
2025-07-12 02:18:17.373559: Pseudo dice [np.float32(0.9544)] 
2025-07-12 02:18:17.374505: Epoch time: 68.02 s 
2025-07-12 02:18:18.245167:  
2025-07-12 02:18:18.246845: Epoch 507 
2025-07-12 02:18:18.247840: Current learning rate: 0.00529 
2025-07-12 02:19:26.206535: train_loss -0.9746 
2025-07-12 02:19:26.207768: val_loss -0.9544 
2025-07-12 02:19:26.208771: Pseudo dice [np.float32(0.9585)] 
2025-07-12 02:19:26.209745: Epoch time: 67.96 s 
2025-07-12 02:19:27.078659:  
2025-07-12 02:19:27.080298: Epoch 508 
2025-07-12 02:19:27.081435: Current learning rate: 0.00528 
2025-07-12 02:20:35.117955: train_loss -0.9742 
2025-07-12 02:20:35.119332: val_loss -0.9511 
2025-07-12 02:20:35.120609: Pseudo dice [np.float32(0.955)] 
2025-07-12 02:20:35.121585: Epoch time: 68.04 s 
2025-07-12 02:20:36.216741:  
2025-07-12 02:20:36.218140: Epoch 509 
2025-07-12 02:20:36.219169: Current learning rate: 0.00527 
2025-07-12 02:21:44.202821: train_loss -0.9745 
2025-07-12 02:21:44.204277: val_loss -0.9486 
2025-07-12 02:21:44.205200: Pseudo dice [np.float32(0.9528)] 
2025-07-12 02:21:44.206194: Epoch time: 67.99 s 
2025-07-12 02:21:45.076986:  
2025-07-12 02:21:45.078532: Epoch 510 
2025-07-12 02:21:45.080314: Current learning rate: 0.00526 
2025-07-12 02:22:53.051199: train_loss -0.9753 
2025-07-12 02:22:53.053305: val_loss -0.9512 
2025-07-12 02:22:53.054206: Pseudo dice [np.float32(0.9559)] 
2025-07-12 02:22:53.055075: Epoch time: 67.98 s 
2025-07-12 02:22:53.926921:  
2025-07-12 02:22:53.928546: Epoch 511 
2025-07-12 02:22:53.929710: Current learning rate: 0.00525 
2025-07-12 02:24:01.924396: train_loss -0.9758 
2025-07-12 02:24:01.925638: val_loss -0.949 
2025-07-12 02:24:01.926589: Pseudo dice [np.float32(0.9534)] 
2025-07-12 02:24:01.927615: Epoch time: 68.0 s 
2025-07-12 02:24:02.795560:  
2025-07-12 02:24:02.797044: Epoch 512 
2025-07-12 02:24:02.798394: Current learning rate: 0.00524 
2025-07-12 02:25:10.925053: train_loss -0.9756 
2025-07-12 02:25:10.926541: val_loss -0.9525 
2025-07-12 02:25:10.927426: Pseudo dice [np.float32(0.9566)] 
2025-07-12 02:25:10.928409: Epoch time: 68.13 s 
2025-07-12 02:25:11.801472:  
2025-07-12 02:25:11.802869: Epoch 513 
2025-07-12 02:25:11.803993: Current learning rate: 0.00523 
2025-07-12 02:26:20.094984: train_loss -0.9741 
2025-07-12 02:26:20.096352: val_loss -0.9512 
2025-07-12 02:26:20.097526: Pseudo dice [np.float32(0.9552)] 
2025-07-12 02:26:20.098867: Epoch time: 68.3 s 
2025-07-12 02:26:20.974218:  
2025-07-12 02:26:20.975667: Epoch 514 
2025-07-12 02:26:20.977262: Current learning rate: 0.00522 
2025-07-12 02:27:29.171849: train_loss -0.9744 
2025-07-12 02:27:29.173435: val_loss -0.9482 
2025-07-12 02:27:29.174358: Pseudo dice [np.float32(0.9524)] 
2025-07-12 02:27:29.175464: Epoch time: 68.2 s 
2025-07-12 02:27:30.051807:  
2025-07-12 02:27:30.053560: Epoch 515 
2025-07-12 02:27:30.055009: Current learning rate: 0.00521 
2025-07-12 02:28:38.212321: train_loss -0.9743 
2025-07-12 02:28:38.213505: val_loss -0.9516 
2025-07-12 02:28:38.214452: Pseudo dice [np.float32(0.9564)] 
2025-07-12 02:28:38.215413: Epoch time: 68.16 s 
2025-07-12 02:28:39.077800:  
2025-07-12 02:28:39.079228: Epoch 516 
2025-07-12 02:28:39.080642: Current learning rate: 0.0052 
2025-07-12 02:29:47.340846: train_loss -0.9733 
2025-07-12 02:29:47.342301: val_loss -0.9495 
2025-07-12 02:29:47.343313: Pseudo dice [np.float32(0.9542)] 
2025-07-12 02:29:47.344285: Epoch time: 68.27 s 
2025-07-12 02:29:48.225535:  
2025-07-12 02:29:48.227092: Epoch 517 
2025-07-12 02:29:48.228178: Current learning rate: 0.00519 
2025-07-12 02:30:56.273315: train_loss -0.9743 
2025-07-12 02:30:56.274428: val_loss -0.956 
2025-07-12 02:30:56.275382: Pseudo dice [np.float32(0.9607)] 
2025-07-12 02:30:56.276435: Epoch time: 68.05 s 
2025-07-12 02:30:57.156626:  
2025-07-12 02:30:57.158003: Epoch 518 
2025-07-12 02:30:57.159127: Current learning rate: 0.00518 
2025-07-12 02:32:05.308236: train_loss -0.9743 
2025-07-12 02:32:05.309326: val_loss -0.9524 
2025-07-12 02:32:05.310192: Pseudo dice [np.float32(0.9567)] 
2025-07-12 02:32:05.311032: Epoch time: 68.16 s 
2025-07-12 02:32:06.201756:  
2025-07-12 02:32:06.203293: Epoch 519 
2025-07-12 02:32:06.204313: Current learning rate: 0.00518 
2025-07-12 02:33:14.467794: train_loss -0.974 
2025-07-12 02:33:14.468881: val_loss -0.9502 
2025-07-12 02:33:14.469723: Pseudo dice [np.float32(0.9543)] 
2025-07-12 02:33:14.470743: Epoch time: 68.27 s 
2025-07-12 02:33:15.348915:  
2025-07-12 02:33:15.350404: Epoch 520 
2025-07-12 02:33:15.351672: Current learning rate: 0.00517 
2025-07-12 02:34:23.343697: train_loss -0.9751 
2025-07-12 02:34:23.345098: val_loss -0.9508 
2025-07-12 02:34:23.346184: Pseudo dice [np.float32(0.9553)] 
2025-07-12 02:34:23.347303: Epoch time: 68.0 s 
2025-07-12 02:34:24.217966:  
2025-07-12 02:34:24.219532: Epoch 521 
2025-07-12 02:34:24.220780: Current learning rate: 0.00516 
2025-07-12 02:35:32.095052: train_loss -0.9756 
2025-07-12 02:35:32.096206: val_loss -0.9485 
2025-07-12 02:35:32.097132: Pseudo dice [np.float32(0.9526)] 
2025-07-12 02:35:32.098017: Epoch time: 67.88 s 
2025-07-12 02:35:32.971117:  
2025-07-12 02:35:32.972716: Epoch 522 
2025-07-12 02:35:32.973729: Current learning rate: 0.00515 
2025-07-12 02:36:40.804783: train_loss -0.9757 
2025-07-12 02:36:40.805844: val_loss -0.9559 
2025-07-12 02:36:40.806742: Pseudo dice [np.float32(0.9597)] 
2025-07-12 02:36:40.807601: Epoch time: 67.84 s 
2025-07-12 02:36:41.680761:  
2025-07-12 02:36:41.682096: Epoch 523 
2025-07-12 02:36:41.683206: Current learning rate: 0.00514 
2025-07-12 02:37:49.779195: train_loss -0.9769 
2025-07-12 02:37:49.780608: val_loss -0.9525 
2025-07-12 02:37:49.781502: Pseudo dice [np.float32(0.9564)] 
2025-07-12 02:37:49.782705: Epoch time: 68.1 s 
2025-07-12 02:37:50.645810:  
2025-07-12 02:37:50.647403: Epoch 524 
2025-07-12 02:37:50.648532: Current learning rate: 0.00513 
2025-07-12 02:38:58.555371: train_loss -0.9759 
2025-07-12 02:38:58.556483: val_loss -0.949 
2025-07-12 02:38:58.557659: Pseudo dice [np.float32(0.9529)] 
2025-07-12 02:38:58.558660: Epoch time: 67.91 s 
2025-07-12 02:38:59.440097:  
2025-07-12 02:38:59.441651: Epoch 525 
2025-07-12 02:38:59.442742: Current learning rate: 0.00512 
2025-07-12 02:40:07.302027: train_loss -0.9755 
2025-07-12 02:40:07.303369: val_loss -0.9549 
2025-07-12 02:40:07.304374: Pseudo dice [np.float32(0.9588)] 
2025-07-12 02:40:07.305295: Epoch time: 67.87 s 
2025-07-12 02:40:08.189270:  
2025-07-12 02:40:08.190679: Epoch 526 
2025-07-12 02:40:08.191701: Current learning rate: 0.00511 
2025-07-12 02:41:16.299794: train_loss -0.9761 
2025-07-12 02:41:16.300897: val_loss -0.9491 
2025-07-12 02:41:16.301883: Pseudo dice [np.float32(0.9536)] 
2025-07-12 02:41:16.302861: Epoch time: 68.11 s 
2025-07-12 02:41:17.176395:  
2025-07-12 02:41:17.178028: Epoch 527 
2025-07-12 02:41:17.179127: Current learning rate: 0.0051 
2025-07-12 02:42:25.316067: train_loss -0.9761 
2025-07-12 02:42:25.317588: val_loss -0.9529 
2025-07-12 02:42:25.318968: Pseudo dice [np.float32(0.9562)] 
2025-07-12 02:42:25.319836: Epoch time: 68.14 s 
2025-07-12 02:42:26.217687:  
2025-07-12 02:42:26.219335: Epoch 528 
2025-07-12 02:42:26.220399: Current learning rate: 0.00509 
2025-07-12 02:43:34.262938: train_loss -0.9766 
2025-07-12 02:43:34.264157: val_loss -0.9484 
2025-07-12 02:43:34.265401: Pseudo dice [np.float32(0.9537)] 
2025-07-12 02:43:34.266449: Epoch time: 68.05 s 
2025-07-12 02:43:35.141623:  
2025-07-12 02:43:35.143035: Epoch 529 
2025-07-12 02:43:35.144206: Current learning rate: 0.00508 
2025-07-12 02:44:43.176336: train_loss -0.9774 
2025-07-12 02:44:43.177760: val_loss -0.9506 
2025-07-12 02:44:43.178879: Pseudo dice [np.float32(0.9552)] 
2025-07-12 02:44:43.179891: Epoch time: 68.04 s 
2025-07-12 02:44:44.068187:  
2025-07-12 02:44:44.069813: Epoch 530 
2025-07-12 02:44:44.071604: Current learning rate: 0.00507 
2025-07-12 02:45:52.428057: train_loss -0.9754 
2025-07-12 02:45:52.429306: val_loss -0.9502 
2025-07-12 02:45:52.430224: Pseudo dice [np.float32(0.9544)] 
2025-07-12 02:45:52.431173: Epoch time: 68.36 s 
2025-07-12 02:45:53.308986:  
2025-07-12 02:45:53.310272: Epoch 531 
2025-07-12 02:45:53.311579: Current learning rate: 0.00506 
2025-07-12 02:47:01.291899: train_loss -0.9745 
2025-07-12 02:47:01.293287: val_loss -0.9428 
2025-07-12 02:47:01.294199: Pseudo dice [np.float32(0.9476)] 
2025-07-12 02:47:01.295041: Epoch time: 67.99 s 
2025-07-12 02:47:02.162213:  
2025-07-12 02:47:02.163633: Epoch 532 
2025-07-12 02:47:02.164629: Current learning rate: 0.00505 
2025-07-12 02:48:10.360978: train_loss -0.9735 
2025-07-12 02:48:10.362251: val_loss -0.9507 
2025-07-12 02:48:10.363348: Pseudo dice [np.float32(0.9549)] 
2025-07-12 02:48:10.364413: Epoch time: 68.2 s 
2025-07-12 02:48:11.244531:  
2025-07-12 02:48:11.245862: Epoch 533 
2025-07-12 02:48:11.246856: Current learning rate: 0.00504 
2025-07-12 02:49:19.434685: train_loss -0.9739 
2025-07-12 02:49:19.436022: val_loss -0.9523 
2025-07-12 02:49:19.437269: Pseudo dice [np.float32(0.9558)] 
2025-07-12 02:49:19.438318: Epoch time: 68.19 s 
2025-07-12 02:49:20.323932:  
2025-07-12 02:49:20.325578: Epoch 534 
2025-07-12 02:49:20.327200: Current learning rate: 0.00503 
2025-07-12 02:50:28.341980: train_loss -0.9741 
2025-07-12 02:50:28.343540: val_loss -0.9484 
2025-07-12 02:50:28.344591: Pseudo dice [np.float32(0.9532)] 
2025-07-12 02:50:28.345484: Epoch time: 68.02 s 
2025-07-12 02:50:29.219399:  
2025-07-12 02:50:29.220821: Epoch 535 
2025-07-12 02:50:29.221955: Current learning rate: 0.00502 
2025-07-12 02:51:37.209986: train_loss -0.9756 
2025-07-12 02:51:37.211297: val_loss -0.9512 
2025-07-12 02:51:37.212276: Pseudo dice [np.float32(0.9559)] 
2025-07-12 02:51:37.213223: Epoch time: 67.99 s 
2025-07-12 02:51:38.089977:  
2025-07-12 02:51:38.091678: Epoch 536 
2025-07-12 02:51:38.092742: Current learning rate: 0.00501 
2025-07-12 02:52:46.128922: train_loss -0.9753 
2025-07-12 02:52:46.130595: val_loss -0.9486 
2025-07-12 02:52:46.131527: Pseudo dice [np.float32(0.9531)] 
2025-07-12 02:52:46.132511: Epoch time: 68.04 s 
2025-07-12 02:52:47.016049:  
2025-07-12 02:52:47.017595: Epoch 537 
2025-07-12 02:52:47.018658: Current learning rate: 0.005 
2025-07-12 02:53:55.205523: train_loss -0.9766 
2025-07-12 02:53:55.207053: val_loss -0.9505 
2025-07-12 02:53:55.208005: Pseudo dice [np.float32(0.9544)] 
2025-07-12 02:53:55.208900: Epoch time: 68.19 s 
2025-07-12 02:53:56.088144:  
2025-07-12 02:53:56.089664: Epoch 538 
2025-07-12 02:53:56.090661: Current learning rate: 0.00499 
2025-07-12 02:55:04.224295: train_loss -0.9762 
2025-07-12 02:55:04.225440: val_loss -0.9536 
2025-07-12 02:55:04.226451: Pseudo dice [np.float32(0.958)] 
2025-07-12 02:55:04.227464: Epoch time: 68.14 s 
2025-07-12 02:55:05.098143:  
2025-07-12 02:55:05.099576: Epoch 539 
2025-07-12 02:55:05.100944: Current learning rate: 0.00498 
2025-07-12 02:56:13.225153: train_loss -0.9754 
2025-07-12 02:56:13.226623: val_loss -0.9517 
2025-07-12 02:56:13.227814: Pseudo dice [np.float32(0.9562)] 
2025-07-12 02:56:13.228790: Epoch time: 68.13 s 
2025-07-12 02:56:14.099103:  
2025-07-12 02:56:14.100558: Epoch 540 
2025-07-12 02:56:14.101931: Current learning rate: 0.00497 
2025-07-12 02:57:22.275759: train_loss -0.9744 
2025-07-12 02:57:22.276911: val_loss -0.9478 
2025-07-12 02:57:22.277821: Pseudo dice [np.float32(0.9518)] 
2025-07-12 02:57:22.278931: Epoch time: 68.18 s 
2025-07-12 02:57:23.147753:  
2025-07-12 02:57:23.149433: Epoch 541 
2025-07-12 02:57:23.150504: Current learning rate: 0.00496 
2025-07-12 02:58:31.388366: train_loss -0.9753 
2025-07-12 02:58:31.389657: val_loss -0.947 
2025-07-12 02:58:31.390759: Pseudo dice [np.float32(0.951)] 
2025-07-12 02:58:31.391806: Epoch time: 68.24 s 
2025-07-12 02:58:32.270964:  
2025-07-12 02:58:32.272595: Epoch 542 
2025-07-12 02:58:32.274130: Current learning rate: 0.00495 
2025-07-12 02:59:40.293435: train_loss -0.976 
2025-07-12 02:59:40.294505: val_loss -0.9526 
2025-07-12 02:59:40.295447: Pseudo dice [np.float32(0.956)] 
2025-07-12 02:59:40.296375: Epoch time: 68.03 s 
2025-07-12 02:59:41.177453:  
2025-07-12 02:59:41.178923: Epoch 543 
2025-07-12 02:59:41.180102: Current learning rate: 0.00494 
2025-07-12 03:00:49.233131: train_loss -0.975 
2025-07-12 03:00:49.234329: val_loss -0.9519 
2025-07-12 03:00:49.235258: Pseudo dice [np.float32(0.9564)] 
2025-07-12 03:00:49.236402: Epoch time: 68.06 s 
2025-07-12 03:00:50.120003:  
2025-07-12 03:00:50.121856: Epoch 544 
2025-07-12 03:00:50.123153: Current learning rate: 0.00493 
2025-07-12 03:01:58.330355: train_loss -0.9753 
2025-07-12 03:01:58.331749: val_loss -0.9525 
2025-07-12 03:01:58.332913: Pseudo dice [np.float32(0.9563)] 
2025-07-12 03:01:58.333883: Epoch time: 68.21 s 
2025-07-12 03:01:59.204640:  
2025-07-12 03:01:59.206163: Epoch 545 
2025-07-12 03:01:59.207242: Current learning rate: 0.00492 
2025-07-12 03:03:07.189850: train_loss -0.9757 
2025-07-12 03:03:07.191203: val_loss -0.9484 
2025-07-12 03:03:07.192394: Pseudo dice [np.float32(0.9533)] 
2025-07-12 03:03:07.193597: Epoch time: 67.99 s 
2025-07-12 03:03:08.064831:  
2025-07-12 03:03:08.066463: Epoch 546 
2025-07-12 03:03:08.067726: Current learning rate: 0.00491 
2025-07-12 03:04:15.989036: train_loss -0.9758 
2025-07-12 03:04:15.990204: val_loss -0.9508 
2025-07-12 03:04:15.991136: Pseudo dice [np.float32(0.9549)] 
2025-07-12 03:04:15.992288: Epoch time: 67.93 s 
2025-07-12 03:04:16.874004:  
2025-07-12 03:04:16.875392: Epoch 547 
2025-07-12 03:04:16.876609: Current learning rate: 0.0049 
2025-07-12 03:05:25.003947: train_loss -0.9756 
2025-07-12 03:05:25.005349: val_loss -0.9496 
2025-07-12 03:05:25.006295: Pseudo dice [np.float32(0.9535)] 
2025-07-12 03:05:25.007536: Epoch time: 68.13 s 
2025-07-12 03:05:25.884289:  
2025-07-12 03:05:25.885961: Epoch 548 
2025-07-12 03:05:25.887075: Current learning rate: 0.00489 
2025-07-12 03:06:33.837456: train_loss -0.9757 
2025-07-12 03:06:33.838836: val_loss -0.9503 
2025-07-12 03:06:33.840797: Pseudo dice [np.float32(0.9542)] 
2025-07-12 03:06:33.842116: Epoch time: 67.96 s 
2025-07-12 03:06:34.715828:  
2025-07-12 03:06:34.717328: Epoch 549 
2025-07-12 03:06:34.718862: Current learning rate: 0.00488 
2025-07-12 03:07:42.878930: train_loss -0.9747 
2025-07-12 03:07:42.880221: val_loss -0.9514 
2025-07-12 03:07:42.881162: Pseudo dice [np.float32(0.9552)] 
2025-07-12 03:07:42.882311: Epoch time: 68.17 s 
2025-07-12 03:07:44.999690:  
2025-07-12 03:07:45.001113: Epoch 550 
2025-07-12 03:07:45.002480: Current learning rate: 0.00487 
2025-07-12 03:08:53.002017: train_loss -0.976 
2025-07-12 03:08:53.003350: val_loss -0.952 
2025-07-12 03:08:53.004451: Pseudo dice [np.float32(0.9552)] 
2025-07-12 03:08:53.005489: Epoch time: 68.01 s 
2025-07-12 03:08:54.089758:  
2025-07-12 03:08:54.091181: Epoch 551 
2025-07-12 03:08:54.092265: Current learning rate: 0.00486 
2025-07-12 03:10:02.165649: train_loss -0.9753 
2025-07-12 03:10:02.167038: val_loss -0.9517 
2025-07-12 03:10:02.167988: Pseudo dice [np.float32(0.9555)] 
2025-07-12 03:10:02.168902: Epoch time: 68.08 s 
2025-07-12 03:10:03.040020:  
2025-07-12 03:10:03.041331: Epoch 552 
2025-07-12 03:10:03.042359: Current learning rate: 0.00485 
2025-07-12 03:11:11.054402: train_loss -0.9761 
2025-07-12 03:11:11.056010: val_loss -0.9506 
2025-07-12 03:11:11.057309: Pseudo dice [np.float32(0.9546)] 
2025-07-12 03:11:11.058725: Epoch time: 68.02 s 
2025-07-12 03:11:11.931060:  
2025-07-12 03:11:11.932653: Epoch 553 
2025-07-12 03:11:11.933718: Current learning rate: 0.00484 
2025-07-12 03:12:19.995830: train_loss -0.976 
2025-07-12 03:12:19.997021: val_loss -0.951 
2025-07-12 03:12:19.998049: Pseudo dice [np.float32(0.9549)] 
2025-07-12 03:12:19.999167: Epoch time: 68.07 s 
2025-07-12 03:12:20.872563:  
2025-07-12 03:12:20.873958: Epoch 554 
2025-07-12 03:12:20.875290: Current learning rate: 0.00484 
2025-07-12 03:13:29.085309: train_loss -0.9759 
2025-07-12 03:13:29.086460: val_loss -0.9501 
2025-07-12 03:13:29.087377: Pseudo dice [np.float32(0.9544)] 
2025-07-12 03:13:29.088324: Epoch time: 68.22 s 
2025-07-12 03:13:29.962984:  
2025-07-12 03:13:29.964390: Epoch 555 
2025-07-12 03:13:29.965449: Current learning rate: 0.00483 
2025-07-12 03:14:38.073716: train_loss -0.976 
2025-07-12 03:14:38.074882: val_loss -0.9502 
2025-07-12 03:14:38.075839: Pseudo dice [np.float32(0.954)] 
2025-07-12 03:14:38.076919: Epoch time: 68.11 s 
2025-07-12 03:14:38.962003:  
2025-07-12 03:14:38.963411: Epoch 556 
2025-07-12 03:14:38.964556: Current learning rate: 0.00482 
2025-07-12 03:15:46.980270: train_loss -0.9756 
2025-07-12 03:15:46.981576: val_loss -0.9511 
2025-07-12 03:15:46.982773: Pseudo dice [np.float32(0.9548)] 
2025-07-12 03:15:46.983757: Epoch time: 68.02 s 
2025-07-12 03:15:47.867227:  
2025-07-12 03:15:47.868705: Epoch 557 
2025-07-12 03:15:47.870064: Current learning rate: 0.00481 
2025-07-12 03:16:55.863019: train_loss -0.9764 
2025-07-12 03:16:55.864178: val_loss -0.9516 
2025-07-12 03:16:55.865123: Pseudo dice [np.float32(0.9556)] 
2025-07-12 03:16:55.866025: Epoch time: 68.0 s 
2025-07-12 03:16:56.738358:  
2025-07-12 03:16:56.740552: Epoch 558 
2025-07-12 03:16:56.741855: Current learning rate: 0.0048 
2025-07-12 03:18:04.924911: train_loss -0.9762 
2025-07-12 03:18:04.926414: val_loss -0.9506 
2025-07-12 03:18:04.927479: Pseudo dice [np.float32(0.9546)] 
2025-07-12 03:18:04.928771: Epoch time: 68.19 s 
2025-07-12 03:18:05.805207:  
2025-07-12 03:18:05.806772: Epoch 559 
2025-07-12 03:18:05.807982: Current learning rate: 0.00479 
2025-07-12 03:19:13.895202: train_loss -0.9761 
2025-07-12 03:19:13.896460: val_loss -0.9492 
2025-07-12 03:19:13.897633: Pseudo dice [np.float32(0.9537)] 
2025-07-12 03:19:13.898562: Epoch time: 68.09 s 
2025-07-12 03:19:14.784541:  
2025-07-12 03:19:14.785996: Epoch 560 
2025-07-12 03:19:14.787039: Current learning rate: 0.00478 
2025-07-12 03:20:22.747505: train_loss -0.9765 
2025-07-12 03:20:22.748703: val_loss -0.9541 
2025-07-12 03:20:22.749786: Pseudo dice [np.float32(0.9575)] 
2025-07-12 03:20:22.750815: Epoch time: 67.97 s 
2025-07-12 03:20:23.626797:  
2025-07-12 03:20:23.628208: Epoch 561 
2025-07-12 03:20:23.629595: Current learning rate: 0.00477 
2025-07-12 03:21:31.752721: train_loss -0.9765 
2025-07-12 03:21:31.753907: val_loss -0.9515 
2025-07-12 03:21:31.755288: Pseudo dice [np.float32(0.9552)] 
2025-07-12 03:21:31.756674: Epoch time: 68.13 s 
2025-07-12 03:21:32.625221:  
2025-07-12 03:21:32.626802: Epoch 562 
2025-07-12 03:21:32.627879: Current learning rate: 0.00476 
2025-07-12 03:22:40.553675: train_loss -0.9769 
2025-07-12 03:22:40.555026: val_loss -0.9543 
2025-07-12 03:22:40.556272: Pseudo dice [np.float32(0.9585)] 
2025-07-12 03:22:40.557210: Epoch time: 67.93 s 
2025-07-12 03:22:41.429264:  
2025-07-12 03:22:41.430681: Epoch 563 
2025-07-12 03:22:41.431907: Current learning rate: 0.00475 
2025-07-12 03:23:49.644335: train_loss -0.9765 
2025-07-12 03:23:49.645520: val_loss -0.9519 
2025-07-12 03:23:49.646694: Pseudo dice [np.float32(0.9562)] 
2025-07-12 03:23:49.647701: Epoch time: 68.22 s 
2025-07-12 03:23:50.527915:  
2025-07-12 03:23:50.529761: Epoch 564 
2025-07-12 03:23:50.530976: Current learning rate: 0.00474 
2025-07-12 03:24:58.509361: train_loss -0.9754 
2025-07-12 03:24:58.510756: val_loss -0.9513 
2025-07-12 03:24:58.511779: Pseudo dice [np.float32(0.9549)] 
2025-07-12 03:24:58.512837: Epoch time: 67.98 s 
2025-07-12 03:24:59.389025:  
2025-07-12 03:24:59.390700: Epoch 565 
2025-07-12 03:24:59.391762: Current learning rate: 0.00473 
2025-07-12 03:26:07.569918: train_loss -0.9743 
2025-07-12 03:26:07.571494: val_loss -0.9466 
2025-07-12 03:26:07.572385: Pseudo dice [np.float32(0.9505)] 
2025-07-12 03:26:07.573375: Epoch time: 68.18 s 
2025-07-12 03:26:08.449997:  
2025-07-12 03:26:08.451403: Epoch 566 
2025-07-12 03:26:08.452413: Current learning rate: 0.00472 
2025-07-12 03:27:16.502269: train_loss -0.9759 
2025-07-12 03:27:16.503564: val_loss -0.9527 
2025-07-12 03:27:16.504627: Pseudo dice [np.float32(0.9564)] 
2025-07-12 03:27:16.505914: Epoch time: 68.06 s 
2025-07-12 03:27:17.382239:  
2025-07-12 03:27:17.383871: Epoch 567 
2025-07-12 03:27:17.386071: Current learning rate: 0.00471 
2025-07-12 03:28:25.395102: train_loss -0.9751 
2025-07-12 03:28:25.397354: val_loss -0.9503 
2025-07-12 03:28:25.398544: Pseudo dice [np.float32(0.9535)] 
2025-07-12 03:28:25.399485: Epoch time: 68.02 s 
2025-07-12 03:28:26.286854:  
2025-07-12 03:28:26.288626: Epoch 568 
2025-07-12 03:28:26.289745: Current learning rate: 0.0047 
2025-07-12 03:29:34.533368: train_loss -0.9765 
2025-07-12 03:29:34.534562: val_loss -0.9533 
2025-07-12 03:29:34.535554: Pseudo dice [np.float32(0.9577)] 
2025-07-12 03:29:34.536560: Epoch time: 68.25 s 
2025-07-12 03:29:35.424756:  
2025-07-12 03:29:35.426116: Epoch 569 
2025-07-12 03:29:35.427241: Current learning rate: 0.00469 
2025-07-12 03:30:43.437459: train_loss -0.976 
2025-07-12 03:30:43.438724: val_loss -0.946 
2025-07-12 03:30:43.439987: Pseudo dice [np.float32(0.95)] 
2025-07-12 03:30:43.440882: Epoch time: 68.02 s 
2025-07-12 03:30:44.335484:  
2025-07-12 03:30:44.336881: Epoch 570 
2025-07-12 03:30:44.338492: Current learning rate: 0.00468 
2025-07-12 03:31:52.285541: train_loss -0.9745 
2025-07-12 03:31:52.287047: val_loss -0.9517 
2025-07-12 03:31:52.288574: Pseudo dice [np.float32(0.9558)] 
2025-07-12 03:31:52.289544: Epoch time: 67.95 s 
2025-07-12 03:31:53.156345:  
2025-07-12 03:31:53.157949: Epoch 571 
2025-07-12 03:31:53.159593: Current learning rate: 0.00467 
2025-07-12 03:33:01.128484: train_loss -0.9754 
2025-07-12 03:33:01.129611: val_loss -0.9515 
2025-07-12 03:33:01.130579: Pseudo dice [np.float32(0.9551)] 
2025-07-12 03:33:01.131494: Epoch time: 67.98 s 
2025-07-12 03:33:02.006154:  
2025-07-12 03:33:02.007653: Epoch 572 
2025-07-12 03:33:02.008751: Current learning rate: 0.00466 
2025-07-12 03:34:10.273039: train_loss -0.9767 
2025-07-12 03:34:10.274655: val_loss -0.9547 
2025-07-12 03:34:10.275924: Pseudo dice [np.float32(0.9585)] 
2025-07-12 03:34:10.277061: Epoch time: 68.27 s 
2025-07-12 03:34:11.152969:  
2025-07-12 03:34:11.154357: Epoch 573 
2025-07-12 03:34:11.155926: Current learning rate: 0.00465 
2025-07-12 03:35:18.984383: train_loss -0.9768 
2025-07-12 03:35:18.985533: val_loss -0.9536 
2025-07-12 03:35:18.986505: Pseudo dice [np.float32(0.9569)] 
2025-07-12 03:35:18.987573: Epoch time: 67.83 s 
2025-07-12 03:35:19.872981:  
2025-07-12 03:35:19.874490: Epoch 574 
2025-07-12 03:35:19.876064: Current learning rate: 0.00464 
2025-07-12 03:36:27.778044: train_loss -0.9769 
2025-07-12 03:36:27.779160: val_loss -0.9495 
2025-07-12 03:36:27.780033: Pseudo dice [np.float32(0.9534)] 
2025-07-12 03:36:27.781117: Epoch time: 67.91 s 
2025-07-12 03:36:28.672884:  
2025-07-12 03:36:28.674236: Epoch 575 
2025-07-12 03:36:28.675300: Current learning rate: 0.00463 
2025-07-12 03:37:36.588878: train_loss -0.9762 
2025-07-12 03:37:36.590054: val_loss -0.9519 
2025-07-12 03:37:36.590960: Pseudo dice [np.float32(0.9558)] 
2025-07-12 03:37:36.592000: Epoch time: 67.92 s 
2025-07-12 03:37:37.489402:  
2025-07-12 03:37:37.491024: Epoch 576 
2025-07-12 03:37:37.492062: Current learning rate: 0.00462 
2025-07-12 03:38:45.284446: train_loss -0.9761 
2025-07-12 03:38:45.285674: val_loss -0.9509 
2025-07-12 03:38:45.286889: Pseudo dice [np.float32(0.9557)] 
2025-07-12 03:38:45.287853: Epoch time: 67.8 s 
2025-07-12 03:38:46.171822:  
2025-07-12 03:38:46.173441: Epoch 577 
2025-07-12 03:38:46.174683: Current learning rate: 0.00461 
2025-07-12 03:39:53.946675: train_loss -0.9761 
2025-07-12 03:39:53.947979: val_loss -0.9477 
2025-07-12 03:39:53.949053: Pseudo dice [np.float32(0.9511)] 
2025-07-12 03:39:53.950016: Epoch time: 67.78 s 
2025-07-12 03:39:54.836224:  
2025-07-12 03:39:54.837604: Epoch 578 
2025-07-12 03:39:54.838825: Current learning rate: 0.0046 
2025-07-12 03:41:02.671470: train_loss -0.9752 
2025-07-12 03:41:02.672787: val_loss -0.953 
2025-07-12 03:41:02.673676: Pseudo dice [np.float32(0.9568)] 
2025-07-12 03:41:02.674704: Epoch time: 67.84 s 
2025-07-12 03:41:03.561148:  
2025-07-12 03:41:03.562363: Epoch 579 
2025-07-12 03:41:03.563661: Current learning rate: 0.00459 
2025-07-12 03:42:11.554393: train_loss -0.9757 
2025-07-12 03:42:11.555749: val_loss -0.9535 
2025-07-12 03:42:11.556769: Pseudo dice [np.float32(0.9577)] 
2025-07-12 03:42:11.557670: Epoch time: 68.0 s 
2025-07-12 03:42:12.447006:  
2025-07-12 03:42:12.448403: Epoch 580 
2025-07-12 03:42:12.449486: Current learning rate: 0.00458 
2025-07-12 03:43:20.371712: train_loss -0.9762 
2025-07-12 03:43:20.372825: val_loss -0.9503 
2025-07-12 03:43:20.373966: Pseudo dice [np.float32(0.9542)] 
2025-07-12 03:43:20.375011: Epoch time: 67.93 s 
2025-07-12 03:43:21.257149:  
2025-07-12 03:43:21.258471: Epoch 581 
2025-07-12 03:43:21.259615: Current learning rate: 0.00457 
2025-07-12 03:44:29.066428: train_loss -0.9761 
2025-07-12 03:44:29.067564: val_loss -0.9518 
2025-07-12 03:44:29.068546: Pseudo dice [np.float32(0.9565)] 
2025-07-12 03:44:29.069502: Epoch time: 67.81 s 
2025-07-12 03:44:29.957650:  
2025-07-12 03:44:29.958932: Epoch 582 
2025-07-12 03:44:29.960650: Current learning rate: 0.00456 
2025-07-12 03:45:38.131744: train_loss -0.9759 
2025-07-12 03:45:38.133026: val_loss -0.9526 
2025-07-12 03:45:38.134003: Pseudo dice [np.float32(0.9571)] 
2025-07-12 03:45:38.134956: Epoch time: 68.18 s 
2025-07-12 03:45:39.014678:  
2025-07-12 03:45:39.016294: Epoch 583 
2025-07-12 03:45:39.017447: Current learning rate: 0.00455 
2025-07-12 03:46:47.008165: train_loss -0.9764 
2025-07-12 03:46:47.009537: val_loss -0.9508 
2025-07-12 03:46:47.010444: Pseudo dice [np.float32(0.9555)] 
2025-07-12 03:46:47.011323: Epoch time: 68.0 s 
2025-07-12 03:46:47.897223:  
2025-07-12 03:46:47.898854: Epoch 584 
2025-07-12 03:46:47.900311: Current learning rate: 0.00454 
2025-07-12 03:47:56.021065: train_loss -0.9772 
2025-07-12 03:47:56.022139: val_loss -0.9547 
2025-07-12 03:47:56.023019: Pseudo dice [np.float32(0.958)] 
2025-07-12 03:47:56.024092: Epoch time: 68.13 s 
2025-07-12 03:47:56.901706:  
2025-07-12 03:47:56.903021: Epoch 585 
2025-07-12 03:47:56.904145: Current learning rate: 0.00453 
2025-07-12 03:49:04.847501: train_loss -0.9774 
2025-07-12 03:49:04.848637: val_loss -0.9528 
2025-07-12 03:49:04.849535: Pseudo dice [np.float32(0.9571)] 
2025-07-12 03:49:04.850565: Epoch time: 67.95 s 
2025-07-12 03:49:05.725941:  
2025-07-12 03:49:05.727318: Epoch 586 
2025-07-12 03:49:05.728376: Current learning rate: 0.00452 
2025-07-12 03:50:13.667796: train_loss -0.9773 
2025-07-12 03:50:13.669020: val_loss -0.9509 
2025-07-12 03:50:13.670267: Pseudo dice [np.float32(0.9553)] 
2025-07-12 03:50:13.671216: Epoch time: 67.95 s 
2025-07-12 03:50:14.560791:  
2025-07-12 03:50:14.562265: Epoch 587 
2025-07-12 03:50:14.563394: Current learning rate: 0.00451 
2025-07-12 03:51:22.408668: train_loss -0.9761 
2025-07-12 03:51:22.410086: val_loss -0.9513 
2025-07-12 03:51:22.410996: Pseudo dice [np.float32(0.9556)] 
2025-07-12 03:51:22.412072: Epoch time: 67.85 s 
2025-07-12 03:51:23.290084:  
2025-07-12 03:51:23.291640: Epoch 588 
2025-07-12 03:51:23.292694: Current learning rate: 0.0045 
2025-07-12 03:52:31.190403: train_loss -0.9766 
2025-07-12 03:52:31.191547: val_loss -0.9518 
2025-07-12 03:52:31.192646: Pseudo dice [np.float32(0.9555)] 
2025-07-12 03:52:31.193747: Epoch time: 67.9 s 
2025-07-12 03:52:32.083856:  
2025-07-12 03:52:32.085340: Epoch 589 
2025-07-12 03:52:32.086586: Current learning rate: 0.00449 
2025-07-12 03:53:40.199778: train_loss -0.9766 
2025-07-12 03:53:40.200924: val_loss -0.9511 
2025-07-12 03:53:40.201909: Pseudo dice [np.float32(0.9549)] 
2025-07-12 03:53:40.202987: Epoch time: 68.12 s 
2025-07-12 03:53:41.084479:  
2025-07-12 03:53:41.085676: Epoch 590 
2025-07-12 03:53:41.086673: Current learning rate: 0.00448 
2025-07-12 03:54:48.920928: train_loss -0.9765 
2025-07-12 03:54:48.922332: val_loss -0.9509 
2025-07-12 03:54:48.923469: Pseudo dice [np.float32(0.9555)] 
2025-07-12 03:54:48.924826: Epoch time: 67.84 s 
2025-07-12 03:54:49.805822:  
2025-07-12 03:54:49.807831: Epoch 591 
2025-07-12 03:54:49.808823: Current learning rate: 0.00447 
2025-07-12 03:55:57.614355: train_loss -0.977 
2025-07-12 03:55:57.615549: val_loss -0.9467 
2025-07-12 03:55:57.616556: Pseudo dice [np.float32(0.9517)] 
2025-07-12 03:55:57.617451: Epoch time: 67.81 s 
2025-07-12 03:55:58.502712:  
2025-07-12 03:55:58.504892: Epoch 592 
2025-07-12 03:55:58.506100: Current learning rate: 0.00446 
2025-07-12 03:57:06.399185: train_loss -0.9758 
2025-07-12 03:57:06.400717: val_loss -0.9498 
2025-07-12 03:57:06.401712: Pseudo dice [np.float32(0.9538)] 
2025-07-12 03:57:06.402722: Epoch time: 67.9 s 
2025-07-12 03:57:07.284122:  
2025-07-12 03:57:07.285516: Epoch 593 
2025-07-12 03:57:07.286647: Current learning rate: 0.00445 
2025-07-12 03:58:15.175538: train_loss -0.9769 
2025-07-12 03:58:15.176803: val_loss -0.9539 
2025-07-12 03:58:15.177808: Pseudo dice [np.float32(0.9576)] 
2025-07-12 03:58:15.178817: Epoch time: 67.89 s 
2025-07-12 03:58:16.064515:  
2025-07-12 03:58:16.066106: Epoch 594 
2025-07-12 03:58:16.067317: Current learning rate: 0.00444 
2025-07-12 03:59:23.922122: train_loss -0.9764 
2025-07-12 03:59:23.923628: val_loss -0.9532 
2025-07-12 03:59:23.924612: Pseudo dice [np.float32(0.9569)] 
2025-07-12 03:59:23.925580: Epoch time: 67.86 s 
2025-07-12 03:59:24.807196:  
2025-07-12 03:59:24.808820: Epoch 595 
2025-07-12 03:59:24.810419: Current learning rate: 0.00443 
2025-07-12 04:00:32.834643: train_loss -0.9735 
2025-07-12 04:00:32.835966: val_loss -0.9495 
2025-07-12 04:00:32.837326: Pseudo dice [np.float32(0.9535)] 
2025-07-12 04:00:32.838456: Epoch time: 68.03 s 
2025-07-12 04:00:33.721061:  
2025-07-12 04:00:33.722666: Epoch 596 
2025-07-12 04:00:33.724454: Current learning rate: 0.00442 
2025-07-12 04:01:41.647805: train_loss -0.9748 
2025-07-12 04:01:41.649187: val_loss -0.951 
2025-07-12 04:01:41.650443: Pseudo dice [np.float32(0.9549)] 
2025-07-12 04:01:41.651638: Epoch time: 67.93 s 
2025-07-12 04:01:42.531717:  
2025-07-12 04:01:42.533143: Epoch 597 
2025-07-12 04:01:42.534178: Current learning rate: 0.00441 
2025-07-12 04:02:50.446456: train_loss -0.976 
2025-07-12 04:02:50.447677: val_loss -0.9527 
2025-07-12 04:02:50.448642: Pseudo dice [np.float32(0.9568)] 
2025-07-12 04:02:50.449629: Epoch time: 67.92 s 
2025-07-12 04:02:51.329198:  
2025-07-12 04:02:51.330591: Epoch 598 
2025-07-12 04:02:51.331576: Current learning rate: 0.0044 
2025-07-12 04:03:59.253499: train_loss -0.9766 
2025-07-12 04:03:59.254865: val_loss -0.953 
2025-07-12 04:03:59.256237: Pseudo dice [np.float32(0.9569)] 
2025-07-12 04:03:59.257162: Epoch time: 67.93 s 
2025-07-12 04:04:00.145915:  
2025-07-12 04:04:00.147233: Epoch 599 
2025-07-12 04:04:00.148859: Current learning rate: 0.00439 
2025-07-12 04:05:07.914035: train_loss -0.977 
2025-07-12 04:05:07.915344: val_loss -0.9529 
2025-07-12 04:05:07.916392: Pseudo dice [np.float32(0.9569)] 
2025-07-12 04:05:07.917431: Epoch time: 67.77 s 
2025-07-12 04:05:10.313473:  
2025-07-12 04:05:10.315000: Epoch 600 
2025-07-12 04:05:10.316105: Current learning rate: 0.00438 
2025-07-12 04:06:18.445294: train_loss -0.9764 
2025-07-12 04:06:18.446462: val_loss -0.9526 
2025-07-12 04:06:18.447491: Pseudo dice [np.float32(0.9569)] 
2025-07-12 04:06:18.448607: Epoch time: 68.14 s 
2025-07-12 04:06:19.343730:  
2025-07-12 04:06:19.345177: Epoch 601 
2025-07-12 04:06:19.346305: Current learning rate: 0.00437 
2025-07-12 04:07:27.536135: train_loss -0.9763 
2025-07-12 04:07:27.537307: val_loss -0.95 
2025-07-12 04:07:27.538182: Pseudo dice [np.float32(0.9545)] 
2025-07-12 04:07:27.539171: Epoch time: 68.2 s 
2025-07-12 04:07:28.428989:  
2025-07-12 04:07:28.430357: Epoch 602 
2025-07-12 04:07:28.431710: Current learning rate: 0.00436 
2025-07-12 04:08:36.526555: train_loss -0.9766 
2025-07-12 04:08:36.527835: val_loss -0.9497 
2025-07-12 04:08:36.528821: Pseudo dice [np.float32(0.954)] 
2025-07-12 04:08:36.529927: Epoch time: 68.1 s 
2025-07-12 04:08:37.414955:  
2025-07-12 04:08:37.416560: Epoch 603 
2025-07-12 04:08:37.418417: Current learning rate: 0.00435 
2025-07-12 04:09:45.545582: train_loss -0.9768 
2025-07-12 04:09:45.547036: val_loss -0.952 
2025-07-12 04:09:45.548109: Pseudo dice [np.float32(0.9561)] 
2025-07-12 04:09:45.549447: Epoch time: 68.13 s 
2025-07-12 04:09:46.433719:  
2025-07-12 04:09:46.435121: Epoch 604 
2025-07-12 04:09:46.436163: Current learning rate: 0.00434 
2025-07-12 04:10:54.369041: train_loss -0.9762 
2025-07-12 04:10:54.370675: val_loss -0.9519 
2025-07-12 04:10:54.371586: Pseudo dice [np.float32(0.9553)] 
2025-07-12 04:10:54.372560: Epoch time: 67.94 s 
2025-07-12 04:10:55.268091:  
2025-07-12 04:10:55.269762: Epoch 605 
2025-07-12 04:10:55.270772: Current learning rate: 0.00433 
2025-07-12 04:12:03.147060: train_loss -0.9765 
2025-07-12 04:12:03.149001: val_loss -0.9529 
2025-07-12 04:12:03.150032: Pseudo dice [np.float32(0.9572)] 
2025-07-12 04:12:03.150976: Epoch time: 67.88 s 
2025-07-12 04:12:04.040629:  
2025-07-12 04:12:04.042174: Epoch 606 
2025-07-12 04:12:04.043205: Current learning rate: 0.00432 
2025-07-12 04:13:11.906256: train_loss -0.9765 
2025-07-12 04:13:11.907357: val_loss -0.9511 
2025-07-12 04:13:11.908521: Pseudo dice [np.float32(0.9551)] 
2025-07-12 04:13:11.909608: Epoch time: 67.87 s 
2025-07-12 04:13:13.027577:  
2025-07-12 04:13:13.029069: Epoch 607 
2025-07-12 04:13:13.030138: Current learning rate: 0.00431 
2025-07-12 04:14:21.008789: train_loss -0.9772 
2025-07-12 04:14:21.010194: val_loss -0.9529 
2025-07-12 04:14:21.011083: Pseudo dice [np.float32(0.9573)] 
2025-07-12 04:14:21.012125: Epoch time: 67.98 s 
2025-07-12 04:14:21.904716:  
2025-07-12 04:14:21.906123: Epoch 608 
2025-07-12 04:14:21.907136: Current learning rate: 0.0043 
2025-07-12 04:15:29.827174: train_loss -0.9783 
2025-07-12 04:15:29.828625: val_loss -0.95 
2025-07-12 04:15:29.829827: Pseudo dice [np.float32(0.9547)] 
2025-07-12 04:15:29.830932: Epoch time: 67.93 s 
2025-07-12 04:15:30.727197:  
2025-07-12 04:15:30.728566: Epoch 609 
2025-07-12 04:15:30.729727: Current learning rate: 0.00429 
2025-07-12 04:16:38.625894: train_loss -0.9768 
2025-07-12 04:16:38.626982: val_loss -0.953 
2025-07-12 04:16:38.627955: Pseudo dice [np.float32(0.9569)] 
2025-07-12 04:16:38.629072: Epoch time: 67.9 s 
2025-07-12 04:16:39.524475:  
2025-07-12 04:16:39.526026: Epoch 610 
2025-07-12 04:16:39.527022: Current learning rate: 0.00429 
2025-07-12 04:17:47.618619: train_loss -0.9781 
2025-07-12 04:17:47.619796: val_loss -0.953 
2025-07-12 04:17:47.620821: Pseudo dice [np.float32(0.9569)] 
2025-07-12 04:17:47.622043: Epoch time: 68.1 s 
2025-07-12 04:17:48.516594:  
2025-07-12 04:17:48.518051: Epoch 611 
2025-07-12 04:17:48.519136: Current learning rate: 0.00428 
2025-07-12 04:18:56.435730: train_loss -0.9767 
2025-07-12 04:18:56.436821: val_loss -0.9539 
2025-07-12 04:18:56.437742: Pseudo dice [np.float32(0.958)] 
2025-07-12 04:18:56.438688: Epoch time: 67.92 s 
2025-07-12 04:18:57.322641:  
2025-07-12 04:18:57.324079: Epoch 612 
2025-07-12 04:18:57.325127: Current learning rate: 0.00427 
2025-07-12 04:20:05.188959: train_loss -0.9776 
2025-07-12 04:20:05.190223: val_loss -0.9504 
2025-07-12 04:20:05.191353: Pseudo dice [np.float32(0.9544)] 
2025-07-12 04:20:05.192356: Epoch time: 67.87 s 
2025-07-12 04:20:06.085942:  
2025-07-12 04:20:06.087686: Epoch 613 
2025-07-12 04:20:06.088712: Current learning rate: 0.00426 
2025-07-12 04:21:14.078426: train_loss -0.9769 
2025-07-12 04:21:14.079589: val_loss -0.9507 
2025-07-12 04:21:14.080512: Pseudo dice [np.float32(0.9549)] 
2025-07-12 04:21:14.081641: Epoch time: 68.0 s 
2025-07-12 04:21:14.966518:  
2025-07-12 04:21:14.967950: Epoch 614 
2025-07-12 04:21:14.968944: Current learning rate: 0.00425 
2025-07-12 04:22:23.179683: train_loss -0.9779 
2025-07-12 04:22:23.180695: val_loss -0.9497 
2025-07-12 04:22:23.181631: Pseudo dice [np.float32(0.9537)] 
2025-07-12 04:22:23.182530: Epoch time: 68.22 s 
2025-07-12 04:22:24.082112:  
2025-07-12 04:22:24.083544: Epoch 615 
2025-07-12 04:22:24.084583: Current learning rate: 0.00424 
2025-07-12 04:23:32.014583: train_loss -0.9763 
2025-07-12 04:23:32.015726: val_loss -0.9526 
2025-07-12 04:23:32.016702: Pseudo dice [np.float32(0.9568)] 
2025-07-12 04:23:32.017990: Epoch time: 67.94 s 
2025-07-12 04:23:32.904156:  
2025-07-12 04:23:32.905700: Epoch 616 
2025-07-12 04:23:32.906797: Current learning rate: 0.00423 
2025-07-12 04:24:40.874056: train_loss -0.9775 
2025-07-12 04:24:40.875459: val_loss -0.9533 
2025-07-12 04:24:40.876640: Pseudo dice [np.float32(0.9572)] 
2025-07-12 04:24:40.877514: Epoch time: 67.97 s 
2025-07-12 04:24:41.775164:  
2025-07-12 04:24:41.776465: Epoch 617 
2025-07-12 04:24:41.777445: Current learning rate: 0.00422 
2025-07-12 04:25:50.051380: train_loss -0.9763 
2025-07-12 04:25:50.053023: val_loss -0.9481 
2025-07-12 04:25:50.054025: Pseudo dice [np.float32(0.9528)] 
2025-07-12 04:25:50.055030: Epoch time: 68.28 s 
2025-07-12 04:25:50.954439:  
2025-07-12 04:25:50.955884: Epoch 618 
2025-07-12 04:25:50.956958: Current learning rate: 0.00421 
2025-07-12 04:26:58.971533: train_loss -0.9766 
2025-07-12 04:26:58.972783: val_loss -0.9523 
2025-07-12 04:26:58.973716: Pseudo dice [np.float32(0.9563)] 
2025-07-12 04:26:58.974613: Epoch time: 68.02 s 
2025-07-12 04:26:59.866647:  
2025-07-12 04:26:59.867950: Epoch 619 
2025-07-12 04:26:59.868911: Current learning rate: 0.0042 
2025-07-12 04:28:07.861979: train_loss -0.9754 
2025-07-12 04:28:07.863152: val_loss -0.9526 
2025-07-12 04:28:07.864112: Pseudo dice [np.float32(0.9569)] 
2025-07-12 04:28:07.865228: Epoch time: 68.0 s 
2025-07-12 04:28:08.764925:  
2025-07-12 04:28:08.766201: Epoch 620 
2025-07-12 04:28:08.767223: Current learning rate: 0.00419 
2025-07-12 04:29:16.740364: train_loss -0.9765 
2025-07-12 04:29:16.741662: val_loss -0.9505 
2025-07-12 04:29:16.742620: Pseudo dice [np.float32(0.955)] 
2025-07-12 04:29:16.743542: Epoch time: 67.98 s 
2025-07-12 04:29:17.616905:  
2025-07-12 04:29:17.619474: Epoch 621 
2025-07-12 04:29:17.620675: Current learning rate: 0.00418 
2025-07-12 04:30:25.928832: train_loss -0.9771 
2025-07-12 04:30:25.929935: val_loss -0.9526 
2025-07-12 04:30:25.930819: Pseudo dice [np.float32(0.9559)] 
2025-07-12 04:30:25.931727: Epoch time: 68.32 s 
2025-07-12 04:30:26.817920:  
2025-07-12 04:30:26.819412: Epoch 622 
2025-07-12 04:30:26.820353: Current learning rate: 0.00417 
2025-07-12 04:31:34.861999: train_loss -0.9757 
2025-07-12 04:31:34.863770: val_loss -0.9496 
2025-07-12 04:31:34.864810: Pseudo dice [np.float32(0.9543)] 
2025-07-12 04:31:34.866066: Epoch time: 68.05 s 
2025-07-12 04:31:35.787598:  
2025-07-12 04:31:35.788962: Epoch 623 
2025-07-12 04:31:35.790112: Current learning rate: 0.00416 
2025-07-12 04:32:43.777586: train_loss -0.9762 
2025-07-12 04:32:43.778661: val_loss -0.9492 
2025-07-12 04:32:43.779524: Pseudo dice [np.float32(0.9538)] 
2025-07-12 04:32:43.780668: Epoch time: 67.99 s 
2025-07-12 04:32:44.685247:  
2025-07-12 04:32:44.686720: Epoch 624 
2025-07-12 04:32:44.687689: Current learning rate: 0.00415 
2025-07-12 04:33:52.915419: train_loss -0.9768 
2025-07-12 04:33:52.917274: val_loss -0.9517 
2025-07-12 04:33:52.918170: Pseudo dice [np.float32(0.9556)] 
2025-07-12 04:33:52.919147: Epoch time: 68.23 s 
2025-07-12 04:33:53.810250:  
2025-07-12 04:33:53.811460: Epoch 625 
2025-07-12 04:33:53.812401: Current learning rate: 0.00414 
2025-07-12 04:35:01.747577: train_loss -0.9774 
2025-07-12 04:35:01.749002: val_loss -0.9521 
2025-07-12 04:35:01.750176: Pseudo dice [np.float32(0.9562)] 
2025-07-12 04:35:01.751042: Epoch time: 67.94 s 
2025-07-12 04:35:02.638180:  
2025-07-12 04:35:02.639414: Epoch 626 
2025-07-12 04:35:02.640509: Current learning rate: 0.00413 
2025-07-12 04:36:10.627205: train_loss -0.9771 
2025-07-12 04:36:10.628973: val_loss -0.9506 
2025-07-12 04:36:10.630028: Pseudo dice [np.float32(0.9549)] 
2025-07-12 04:36:10.631100: Epoch time: 67.99 s 
2025-07-12 04:36:11.533383:  
2025-07-12 04:36:11.534726: Epoch 627 
2025-07-12 04:36:11.535628: Current learning rate: 0.00412 
2025-07-12 04:37:19.597784: train_loss -0.9771 
2025-07-12 04:37:19.598960: val_loss -0.9504 
2025-07-12 04:37:19.599930: Pseudo dice [np.float32(0.9552)] 
2025-07-12 04:37:19.600847: Epoch time: 68.07 s 
2025-07-12 04:37:20.487448:  
2025-07-12 04:37:20.489036: Epoch 628 
2025-07-12 04:37:20.490040: Current learning rate: 0.00411 
2025-07-12 04:38:28.634505: train_loss -0.9778 
2025-07-12 04:38:28.635599: val_loss -0.9541 
2025-07-12 04:38:28.636586: Pseudo dice [np.float32(0.9577)] 
2025-07-12 04:38:28.637571: Epoch time: 68.15 s 
2025-07-12 04:38:29.530068:  
2025-07-12 04:38:29.531424: Epoch 629 
2025-07-12 04:38:29.532442: Current learning rate: 0.0041 
2025-07-12 04:39:37.619838: train_loss -0.9779 
2025-07-12 04:39:37.621057: val_loss -0.9516 
2025-07-12 04:39:37.621973: Pseudo dice [np.float32(0.9555)] 
2025-07-12 04:39:37.622936: Epoch time: 68.09 s 
2025-07-12 04:39:38.522649:  
2025-07-12 04:39:38.523998: Epoch 630 
2025-07-12 04:39:38.525023: Current learning rate: 0.00409 
2025-07-12 04:40:46.580066: train_loss -0.9788 
2025-07-12 04:40:46.581278: val_loss -0.9536 
2025-07-12 04:40:46.582238: Pseudo dice [np.float32(0.9577)] 
2025-07-12 04:40:46.583184: Epoch time: 68.06 s 
2025-07-12 04:40:47.478976:  
2025-07-12 04:40:47.480381: Epoch 631 
2025-07-12 04:40:47.481409: Current learning rate: 0.00408 
2025-07-12 04:41:55.735615: train_loss -0.9778 
2025-07-12 04:41:55.737168: val_loss -0.9515 
2025-07-12 04:41:55.738102: Pseudo dice [np.float32(0.9558)] 
2025-07-12 04:41:55.739168: Epoch time: 68.26 s 
2025-07-12 04:41:56.641458:  
2025-07-12 04:41:56.642716: Epoch 632 
2025-07-12 04:41:56.643702: Current learning rate: 0.00407 
2025-07-12 04:43:04.680085: train_loss -0.9776 
2025-07-12 04:43:04.681568: val_loss -0.9518 
2025-07-12 04:43:04.682471: Pseudo dice [np.float32(0.9553)] 
2025-07-12 04:43:04.683503: Epoch time: 68.04 s 
2025-07-12 04:43:05.573013:  
2025-07-12 04:43:05.574345: Epoch 633 
2025-07-12 04:43:05.575309: Current learning rate: 0.00406 
2025-07-12 04:44:13.643876: train_loss -0.9786 
2025-07-12 04:44:13.645119: val_loss -0.9479 
2025-07-12 04:44:13.646055: Pseudo dice [np.float32(0.9525)] 
2025-07-12 04:44:13.647063: Epoch time: 68.07 s 
2025-07-12 04:44:14.540450:  
2025-07-12 04:44:14.541981: Epoch 634 
2025-07-12 04:44:14.542929: Current learning rate: 0.00405 
2025-07-12 04:45:22.537085: train_loss -0.9772 
2025-07-12 04:45:22.538913: val_loss -0.9504 
2025-07-12 04:45:22.539908: Pseudo dice [np.float32(0.9545)] 
2025-07-12 04:45:22.540891: Epoch time: 68.0 s 
2025-07-12 04:45:23.454347:  
2025-07-12 04:45:23.455791: Epoch 635 
2025-07-12 04:45:23.456825: Current learning rate: 0.00404 
2025-07-12 04:46:31.650215: train_loss -0.978 
2025-07-12 04:46:31.651607: val_loss -0.9512 
2025-07-12 04:46:31.653056: Pseudo dice [np.float32(0.9555)] 
2025-07-12 04:46:31.654127: Epoch time: 68.2 s 
2025-07-12 04:46:32.544550:  
2025-07-12 04:46:32.545930: Epoch 636 
2025-07-12 04:46:32.546999: Current learning rate: 0.00403 
2025-07-12 04:47:40.547818: train_loss -0.9772 
2025-07-12 04:47:40.548945: val_loss -0.9499 
2025-07-12 04:47:40.549849: Pseudo dice [np.float32(0.9547)] 
2025-07-12 04:47:40.550802: Epoch time: 68.01 s 
2025-07-12 04:47:41.436483:  
2025-07-12 04:47:41.438015: Epoch 637 
2025-07-12 04:47:41.439056: Current learning rate: 0.00402 
2025-07-12 04:48:49.523569: train_loss -0.9764 
2025-07-12 04:48:49.524736: val_loss -0.9519 
2025-07-12 04:48:49.525845: Pseudo dice [np.float32(0.9557)] 
2025-07-12 04:48:49.526706: Epoch time: 68.09 s 
2025-07-12 04:48:50.407420:  
2025-07-12 04:48:50.409260: Epoch 638 
2025-07-12 04:48:50.410329: Current learning rate: 0.00401 
2025-07-12 04:49:58.618843: train_loss -0.9766 
2025-07-12 04:49:58.620265: val_loss -0.9505 
2025-07-12 04:49:58.621273: Pseudo dice [np.float32(0.9543)] 
2025-07-12 04:49:58.622601: Epoch time: 68.21 s 
2025-07-12 04:49:59.529247:  
2025-07-12 04:49:59.530585: Epoch 639 
2025-07-12 04:49:59.531505: Current learning rate: 0.004 
2025-07-12 04:51:07.521104: train_loss -0.9774 
2025-07-12 04:51:07.522795: val_loss -0.9502 
2025-07-12 04:51:07.523937: Pseudo dice [np.float32(0.9549)] 
2025-07-12 04:51:07.524922: Epoch time: 68.0 s 
2025-07-12 04:51:08.424538:  
2025-07-12 04:51:08.425916: Epoch 640 
2025-07-12 04:51:08.426996: Current learning rate: 0.00399 
2025-07-12 04:52:16.364818: train_loss -0.9755 
2025-07-12 04:52:16.366028: val_loss -0.954 
2025-07-12 04:52:16.367272: Pseudo dice [np.float32(0.958)] 
2025-07-12 04:52:16.368740: Epoch time: 67.94 s 
2025-07-12 04:52:17.353923:  
2025-07-12 04:52:17.355545: Epoch 641 
2025-07-12 04:52:17.356526: Current learning rate: 0.00398 
2025-07-12 04:53:25.616992: train_loss -0.9761 
2025-07-12 04:53:25.618764: val_loss -0.9532 
2025-07-12 04:53:25.619672: Pseudo dice [np.float32(0.9573)] 
2025-07-12 04:53:25.620613: Epoch time: 68.27 s 
2025-07-12 04:53:26.523566:  
2025-07-12 04:53:26.525107: Epoch 642 
2025-07-12 04:53:26.526089: Current learning rate: 0.00397 
2025-07-12 04:54:34.912893: train_loss -0.9782 
2025-07-12 04:54:34.914490: val_loss -0.9511 
2025-07-12 04:54:34.915564: Pseudo dice [np.float32(0.9555)] 
2025-07-12 04:54:34.916644: Epoch time: 68.39 s 
2025-07-12 04:54:35.820350:  
2025-07-12 04:54:35.821763: Epoch 643 
2025-07-12 04:54:35.822755: Current learning rate: 0.00396 
2025-07-12 04:55:44.004979: train_loss -0.9773 
2025-07-12 04:55:44.006134: val_loss -0.9536 
2025-07-12 04:55:44.007049: Pseudo dice [np.float32(0.9576)] 
2025-07-12 04:55:44.008086: Epoch time: 68.19 s 
2025-07-12 04:55:44.983687:  
2025-07-12 04:55:44.985170: Epoch 644 
2025-07-12 04:55:44.986140: Current learning rate: 0.00395 
2025-07-12 04:56:53.053648: train_loss -0.9784 
2025-07-12 04:56:53.054758: val_loss -0.9517 
2025-07-12 04:56:53.055726: Pseudo dice [np.float32(0.9554)] 
2025-07-12 04:56:53.056581: Epoch time: 68.07 s 
2025-07-12 04:56:53.936635:  
2025-07-12 04:56:53.938164: Epoch 645 
2025-07-12 04:56:53.939286: Current learning rate: 0.00394 
2025-07-12 04:58:02.076167: train_loss -0.9776 
2025-07-12 04:58:02.077590: val_loss -0.9507 
2025-07-12 04:58:02.078921: Pseudo dice [np.float32(0.9551)] 
2025-07-12 04:58:02.079912: Epoch time: 68.14 s 
2025-07-12 04:58:02.970127:  
2025-07-12 04:58:02.971636: Epoch 646 
2025-07-12 04:58:02.972667: Current learning rate: 0.00393 
2025-07-12 04:59:10.944668: train_loss -0.9764 
2025-07-12 04:59:10.945887: val_loss -0.9525 
2025-07-12 04:59:10.946771: Pseudo dice [np.float32(0.956)] 
2025-07-12 04:59:10.947762: Epoch time: 67.98 s 
2025-07-12 04:59:11.831653:  
2025-07-12 04:59:11.833120: Epoch 647 
2025-07-12 04:59:11.834124: Current learning rate: 0.00392 
2025-07-12 05:00:19.830038: train_loss -0.9778 
2025-07-12 05:00:19.831185: val_loss -0.9507 
2025-07-12 05:00:19.832114: Pseudo dice [np.float32(0.9549)] 
2025-07-12 05:00:19.833059: Epoch time: 68.0 s 
2025-07-12 05:00:20.716883:  
2025-07-12 05:00:20.718309: Epoch 648 
2025-07-12 05:00:20.719294: Current learning rate: 0.00391 
2025-07-12 05:01:28.851922: train_loss -0.9774 
2025-07-12 05:01:28.853510: val_loss -0.9489 
2025-07-12 05:01:28.854411: Pseudo dice [np.float32(0.9533)] 
2025-07-12 05:01:28.855405: Epoch time: 68.14 s 
2025-07-12 05:01:29.738230:  
2025-07-12 05:01:29.739564: Epoch 649 
2025-07-12 05:01:29.740474: Current learning rate: 0.0039 
2025-07-12 05:02:37.947243: train_loss -0.9778 
2025-07-12 05:02:37.948344: val_loss -0.9537 
2025-07-12 05:02:37.949367: Pseudo dice [np.float32(0.9576)] 
2025-07-12 05:02:37.950413: Epoch time: 68.21 s 
2025-07-12 05:02:39.987993:  
2025-07-12 05:02:39.989320: Epoch 650 
2025-07-12 05:02:39.990324: Current learning rate: 0.00389 
2025-07-12 05:03:47.859892: train_loss -0.9774 
2025-07-12 05:03:47.861016: val_loss -0.9526 
2025-07-12 05:03:47.862029: Pseudo dice [np.float32(0.9562)] 
2025-07-12 05:03:47.863094: Epoch time: 67.88 s 
2025-07-12 05:03:48.760219:  
2025-07-12 05:03:48.761812: Epoch 651 
2025-07-12 05:03:48.762754: Current learning rate: 0.00388 
2025-07-12 05:04:56.689981: train_loss -0.9769 
2025-07-12 05:04:56.691301: val_loss -0.9545 
2025-07-12 05:04:56.692169: Pseudo dice [np.float32(0.9578)] 
2025-07-12 05:04:56.693164: Epoch time: 67.93 s 
2025-07-12 05:04:57.573718:  
2025-07-12 05:04:57.575226: Epoch 652 
2025-07-12 05:04:57.576153: Current learning rate: 0.00387 
2025-07-12 05:06:05.613307: train_loss -0.9771 
2025-07-12 05:06:05.614757: val_loss -0.9531 
2025-07-12 05:06:05.615801: Pseudo dice [np.float32(0.957)] 
2025-07-12 05:06:05.617145: Epoch time: 68.04 s 
2025-07-12 05:06:06.510539:  
2025-07-12 05:06:06.511950: Epoch 653 
2025-07-12 05:06:06.512992: Current learning rate: 0.00386 
2025-07-12 05:07:14.462843: train_loss -0.9769 
2025-07-12 05:07:14.464081: val_loss -0.9532 
2025-07-12 05:07:14.464962: Pseudo dice [np.float32(0.9572)] 
2025-07-12 05:07:14.466050: Epoch time: 67.96 s 
2025-07-12 05:07:15.360792:  
2025-07-12 05:07:15.362182: Epoch 654 
2025-07-12 05:07:15.363201: Current learning rate: 0.00385 
2025-07-12 05:08:23.319940: train_loss -0.9778 
2025-07-12 05:08:23.321505: val_loss -0.9529 
2025-07-12 05:08:23.322437: Pseudo dice [np.float32(0.9567)] 
2025-07-12 05:08:23.323531: Epoch time: 67.96 s 
2025-07-12 05:08:23.324430: Yayy! New best EMA pseudo Dice: 0.9562000036239624 
2025-07-12 05:08:25.443079:  
2025-07-12 05:08:25.444629: Epoch 655 
2025-07-12 05:08:25.445729: Current learning rate: 0.00384 
2025-07-12 05:09:33.418832: train_loss -0.9775 
2025-07-12 05:09:33.420087: val_loss -0.9538 
2025-07-12 05:09:33.421106: Pseudo dice [np.float32(0.9577)] 
2025-07-12 05:09:33.421962: Epoch time: 67.98 s 
2025-07-12 05:09:33.422976: Yayy! New best EMA pseudo Dice: 0.9563000202178955 
2025-07-12 05:09:35.727254:  
2025-07-12 05:09:35.728703: Epoch 656 
2025-07-12 05:09:35.729800: Current learning rate: 0.00383 
2025-07-12 05:10:43.758824: train_loss -0.9774 
2025-07-12 05:10:43.759918: val_loss -0.9508 
2025-07-12 05:10:43.760796: Pseudo dice [np.float32(0.9554)] 
2025-07-12 05:10:43.761689: Epoch time: 68.03 s 
2025-07-12 05:10:44.630584:  
2025-07-12 05:10:44.631903: Epoch 657 
2025-07-12 05:10:44.632944: Current learning rate: 0.00382 
2025-07-12 05:11:52.815560: train_loss -0.9778 
2025-07-12 05:11:52.816908: val_loss -0.9548 
2025-07-12 05:11:52.817731: Pseudo dice [np.float32(0.9589)] 
2025-07-12 05:11:52.818583: Epoch time: 68.19 s 
2025-07-12 05:11:52.819482: Yayy! New best EMA pseudo Dice: 0.9564999938011169 
2025-07-12 05:11:55.038001:  
2025-07-12 05:11:55.039502: Epoch 658 
2025-07-12 05:11:55.040712: Current learning rate: 0.00381 
2025-07-12 05:13:03.077441: train_loss -0.9787 
2025-07-12 05:13:03.078632: val_loss -0.9543 
2025-07-12 05:13:03.079495: Pseudo dice [np.float32(0.9579)] 
2025-07-12 05:13:03.080475: Epoch time: 68.04 s 
2025-07-12 05:13:03.081417: Yayy! New best EMA pseudo Dice: 0.9567000269889832 
2025-07-12 05:13:05.419215:  
2025-07-12 05:13:05.420659: Epoch 659 
2025-07-12 05:13:05.421738: Current learning rate: 0.0038 
2025-07-12 05:14:13.364953: train_loss -0.9785 
2025-07-12 05:14:13.366238: val_loss -0.9535 
2025-07-12 05:14:13.367143: Pseudo dice [np.float32(0.9581)] 
2025-07-12 05:14:13.368228: Epoch time: 67.95 s 
2025-07-12 05:14:13.369281: Yayy! New best EMA pseudo Dice: 0.9567999839782715 
2025-07-12 05:14:15.693313:  
2025-07-12 05:14:15.694855: Epoch 660 
2025-07-12 05:14:15.695859: Current learning rate: 0.00379 
2025-07-12 05:15:23.698627: train_loss -0.9768 
2025-07-12 05:15:23.699858: val_loss -0.9475 
2025-07-12 05:15:23.701087: Pseudo dice [np.float32(0.9516)] 
2025-07-12 05:15:23.702334: Epoch time: 68.01 s 
2025-07-12 05:15:24.591106:  
2025-07-12 05:15:24.592585: Epoch 661 
2025-07-12 05:15:24.593711: Current learning rate: 0.00378 
2025-07-12 05:16:32.640043: train_loss -0.976 
2025-07-12 05:16:32.641320: val_loss -0.9537 
2025-07-12 05:16:32.642641: Pseudo dice [np.float32(0.9577)] 
2025-07-12 05:16:32.643497: Epoch time: 68.05 s 
2025-07-12 05:16:33.535466:  
2025-07-12 05:16:33.537060: Epoch 662 
2025-07-12 05:16:33.538077: Current learning rate: 0.00377 
2025-07-12 05:17:41.659971: train_loss -0.9779 
2025-07-12 05:17:41.661431: val_loss -0.952 
2025-07-12 05:17:41.662334: Pseudo dice [np.float32(0.9553)] 
2025-07-12 05:17:41.663188: Epoch time: 68.13 s 
2025-07-12 05:17:42.550684:  
2025-07-12 05:17:42.552159: Epoch 663 
2025-07-12 05:17:42.553157: Current learning rate: 0.00376 
2025-07-12 05:18:50.547115: train_loss -0.9788 
2025-07-12 05:18:50.548450: val_loss -0.951 
2025-07-12 05:18:50.549499: Pseudo dice [np.float32(0.9546)] 
2025-07-12 05:18:50.550505: Epoch time: 68.0 s 
2025-07-12 05:18:51.444312:  
2025-07-12 05:18:51.445558: Epoch 664 
2025-07-12 05:18:51.446667: Current learning rate: 0.00375 
2025-07-12 05:19:59.398495: train_loss -0.9784 
2025-07-12 05:19:59.399931: val_loss -0.9534 
2025-07-12 05:19:59.400836: Pseudo dice [np.float32(0.9568)] 
2025-07-12 05:19:59.401709: Epoch time: 67.96 s 
2025-07-12 05:20:00.286286:  
2025-07-12 05:20:00.287713: Epoch 665 
2025-07-12 05:20:00.288772: Current learning rate: 0.00374 
2025-07-12 05:21:08.355767: train_loss -0.9785 
2025-07-12 05:21:08.356969: val_loss -0.9541 
2025-07-12 05:21:08.357873: Pseudo dice [np.float32(0.9581)] 
2025-07-12 05:21:08.359088: Epoch time: 68.07 s 
2025-07-12 05:21:09.252224:  
2025-07-12 05:21:09.253525: Epoch 666 
2025-07-12 05:21:09.254478: Current learning rate: 0.00373 
2025-07-12 05:22:17.438413: train_loss -0.9781 
2025-07-12 05:22:17.439561: val_loss -0.9528 
2025-07-12 05:22:17.440567: Pseudo dice [np.float32(0.956)] 
2025-07-12 05:22:17.441878: Epoch time: 68.19 s 
2025-07-12 05:22:18.334886:  
2025-07-12 05:22:18.336103: Epoch 667 
2025-07-12 05:22:18.337124: Current learning rate: 0.00372 
2025-07-12 05:23:26.474878: train_loss -0.9778 
2025-07-12 05:23:26.475964: val_loss -0.9483 
2025-07-12 05:23:26.476960: Pseudo dice [np.float32(0.9517)] 
2025-07-12 05:23:26.477928: Epoch time: 68.14 s 
2025-07-12 05:23:27.372689:  
2025-07-12 05:23:27.374294: Epoch 668 
2025-07-12 05:23:27.375243: Current learning rate: 0.00371 
2025-07-12 05:24:35.395236: train_loss -0.9785 
2025-07-12 05:24:35.396440: val_loss -0.9516 
2025-07-12 05:24:35.397453: Pseudo dice [np.float32(0.9548)] 
2025-07-12 05:24:35.398485: Epoch time: 68.03 s 
2025-07-12 05:24:36.524792:  
2025-07-12 05:24:36.526315: Epoch 669 
2025-07-12 05:24:36.527333: Current learning rate: 0.0037 
2025-07-12 05:25:44.703481: train_loss -0.9791 
2025-07-12 05:25:44.704942: val_loss -0.9524 
2025-07-12 05:25:44.705936: Pseudo dice [np.float32(0.9565)] 
2025-07-12 05:25:44.706910: Epoch time: 68.18 s 
2025-07-12 05:25:45.603412:  
2025-07-12 05:25:45.604859: Epoch 670 
2025-07-12 05:25:45.605890: Current learning rate: 0.00369 
2025-07-12 05:26:53.647251: train_loss -0.9789 
2025-07-12 05:26:53.648688: val_loss -0.9516 
2025-07-12 05:26:53.649589: Pseudo dice [np.float32(0.9552)] 
2025-07-12 05:26:53.650586: Epoch time: 68.05 s 
2025-07-12 05:26:54.533020:  
2025-07-12 05:26:54.534309: Epoch 671 
2025-07-12 05:26:54.535324: Current learning rate: 0.00368 
2025-07-12 05:28:02.621526: train_loss -0.9778 
2025-07-12 05:28:02.622706: val_loss -0.9497 
2025-07-12 05:28:02.623626: Pseudo dice [np.float32(0.9539)] 
2025-07-12 05:28:02.624605: Epoch time: 68.09 s 
2025-07-12 05:28:03.532580:  
2025-07-12 05:28:03.533982: Epoch 672 
2025-07-12 05:28:03.535165: Current learning rate: 0.00367 
2025-07-12 05:29:11.892764: train_loss -0.9784 
2025-07-12 05:29:11.894055: val_loss -0.9512 
2025-07-12 05:29:11.895164: Pseudo dice [np.float32(0.9554)] 
2025-07-12 05:29:11.896060: Epoch time: 68.36 s 
2025-07-12 05:29:12.788335:  
2025-07-12 05:29:12.789870: Epoch 673 
2025-07-12 05:29:12.790911: Current learning rate: 0.00366 
2025-07-12 05:30:20.857000: train_loss -0.9775 
2025-07-12 05:30:20.858090: val_loss -0.9533 
2025-07-12 05:30:20.859049: Pseudo dice [np.float32(0.9564)] 
2025-07-12 05:30:20.860348: Epoch time: 68.07 s 
2025-07-12 05:30:21.760991:  
2025-07-12 05:30:21.762356: Epoch 674 
2025-07-12 05:30:21.763358: Current learning rate: 0.00365 
2025-07-12 05:31:29.835700: train_loss -0.9771 
2025-07-12 05:31:29.836945: val_loss -0.9533 
2025-07-12 05:31:29.837950: Pseudo dice [np.float32(0.9581)] 
2025-07-12 05:31:29.838839: Epoch time: 68.08 s 
2025-07-12 05:31:30.736626:  
2025-07-12 05:31:30.737911: Epoch 675 
2025-07-12 05:31:30.738950: Current learning rate: 0.00364 
2025-07-12 05:32:38.696370: train_loss -0.9768 
2025-07-12 05:32:38.697473: val_loss -0.949 
2025-07-12 05:32:38.698397: Pseudo dice [np.float32(0.9529)] 
2025-07-12 05:32:38.699560: Epoch time: 67.96 s 
2025-07-12 05:32:39.892663:  
2025-07-12 05:32:39.894237: Epoch 676 
2025-07-12 05:32:39.895205: Current learning rate: 0.00363 
2025-07-12 05:33:47.818940: train_loss -0.9766 
2025-07-12 05:33:47.820101: val_loss -0.9496 
2025-07-12 05:33:47.821019: Pseudo dice [np.float32(0.9536)] 
2025-07-12 05:33:47.822135: Epoch time: 67.93 s 
2025-07-12 05:33:48.713205:  
2025-07-12 05:33:48.714621: Epoch 677 
2025-07-12 05:33:48.715562: Current learning rate: 0.00362 
2025-07-12 05:34:56.676652: train_loss -0.9774 
2025-07-12 05:34:56.677988: val_loss -0.9518 
2025-07-12 05:34:56.678911: Pseudo dice [np.float32(0.9554)] 
2025-07-12 05:34:56.679866: Epoch time: 67.97 s 
2025-07-12 05:34:57.583421:  
2025-07-12 05:34:57.585273: Epoch 678 
2025-07-12 05:34:57.586347: Current learning rate: 0.00361 
2025-07-12 05:36:05.528035: train_loss -0.9784 
2025-07-12 05:36:05.529125: val_loss -0.9493 
2025-07-12 05:36:05.530191: Pseudo dice [np.float32(0.9531)] 
2025-07-12 05:36:05.531294: Epoch time: 67.95 s 
2025-07-12 05:36:06.436968:  
2025-07-12 05:36:06.438426: Epoch 679 
2025-07-12 05:36:06.439451: Current learning rate: 0.0036 
2025-07-12 05:37:14.571837: train_loss -0.9773 
2025-07-12 05:37:14.573111: val_loss -0.9535 
2025-07-12 05:37:14.574016: Pseudo dice [np.float32(0.9575)] 
2025-07-12 05:37:14.575110: Epoch time: 68.14 s 
2025-07-12 05:37:15.470491:  
2025-07-12 05:37:15.472063: Epoch 680 
2025-07-12 05:37:15.473078: Current learning rate: 0.00359 
2025-07-12 05:38:23.422865: train_loss -0.9781 
2025-07-12 05:38:23.424147: val_loss -0.9543 
2025-07-12 05:38:23.425063: Pseudo dice [np.float32(0.9588)] 
2025-07-12 05:38:23.425987: Epoch time: 67.96 s 
2025-07-12 05:38:24.311991:  
2025-07-12 05:38:24.313237: Epoch 681 
2025-07-12 05:38:24.314215: Current learning rate: 0.00358 
2025-07-12 05:39:32.302042: train_loss -0.9779 
2025-07-12 05:39:32.304213: val_loss -0.957 
2025-07-12 05:39:32.305116: Pseudo dice [np.float32(0.9615)] 
2025-07-12 05:39:32.305996: Epoch time: 67.99 s 
2025-07-12 05:39:33.199827:  
2025-07-12 05:39:33.201196: Epoch 682 
2025-07-12 05:39:33.202121: Current learning rate: 0.00357 
2025-07-12 05:40:41.139063: train_loss -0.9782 
2025-07-12 05:40:41.140283: val_loss -0.9507 
2025-07-12 05:40:41.141170: Pseudo dice [np.float32(0.9552)] 
2025-07-12 05:40:41.142177: Epoch time: 67.94 s 
2025-07-12 05:40:42.267613:  
2025-07-12 05:40:42.268754: Epoch 683 
2025-07-12 05:40:42.269720: Current learning rate: 0.00356 
2025-07-12 05:41:50.192505: train_loss -0.9782 
2025-07-12 05:41:50.194054: val_loss -0.9482 
2025-07-12 05:41:50.195085: Pseudo dice [np.float32(0.9531)] 
2025-07-12 05:41:50.196000: Epoch time: 67.93 s 
2025-07-12 05:41:51.104637:  
2025-07-12 05:41:51.106072: Epoch 684 
2025-07-12 05:41:51.107176: Current learning rate: 0.00355 
2025-07-12 05:42:59.017947: train_loss -0.9782 
2025-07-12 05:42:59.019031: val_loss -0.952 
2025-07-12 05:42:59.020028: Pseudo dice [np.float32(0.9557)] 
2025-07-12 05:42:59.021352: Epoch time: 67.92 s 
2025-07-12 05:42:59.921987:  
2025-07-12 05:42:59.923479: Epoch 685 
2025-07-12 05:42:59.924530: Current learning rate: 0.00354 
2025-07-12 05:44:07.805310: train_loss -0.978 
2025-07-12 05:44:07.806578: val_loss -0.9518 
2025-07-12 05:44:07.807566: Pseudo dice [np.float32(0.956)] 
2025-07-12 05:44:07.808578: Epoch time: 67.89 s 
2025-07-12 05:44:08.698938:  
2025-07-12 05:44:08.700300: Epoch 686 
2025-07-12 05:44:08.701191: Current learning rate: 0.00353 
2025-07-12 05:45:16.736332: train_loss -0.9786 
2025-07-12 05:45:16.737828: val_loss -0.9528 
2025-07-12 05:45:16.739067: Pseudo dice [np.float32(0.9566)] 
2025-07-12 05:45:16.740175: Epoch time: 68.04 s 
2025-07-12 05:45:17.637082:  
2025-07-12 05:45:17.638408: Epoch 687 
2025-07-12 05:45:17.639425: Current learning rate: 0.00352 
2025-07-12 05:46:25.694229: train_loss -0.9792 
2025-07-12 05:46:25.695497: val_loss -0.9528 
2025-07-12 05:46:25.696607: Pseudo dice [np.float32(0.9565)] 
2025-07-12 05:46:25.697704: Epoch time: 68.06 s 
2025-07-12 05:46:26.600341:  
2025-07-12 05:46:26.601942: Epoch 688 
2025-07-12 05:46:26.602968: Current learning rate: 0.00351 
2025-07-12 05:47:34.645754: train_loss -0.9765 
2025-07-12 05:47:34.646872: val_loss -0.9503 
2025-07-12 05:47:34.648101: Pseudo dice [np.float32(0.9555)] 
2025-07-12 05:47:34.649041: Epoch time: 68.05 s 
2025-07-12 05:47:35.540853:  
2025-07-12 05:47:35.542456: Epoch 689 
2025-07-12 05:47:35.543456: Current learning rate: 0.0035 
2025-07-12 05:48:43.397376: train_loss -0.9768 
2025-07-12 05:48:43.398468: val_loss -0.9494 
2025-07-12 05:48:43.399420: Pseudo dice [np.float32(0.9541)] 
2025-07-12 05:48:43.400654: Epoch time: 67.86 s 
2025-07-12 05:48:44.294343:  
2025-07-12 05:48:44.295741: Epoch 690 
2025-07-12 05:48:44.296851: Current learning rate: 0.00349 
2025-07-12 05:49:52.331104: train_loss -0.9777 
2025-07-12 05:49:52.332304: val_loss -0.9514 
2025-07-12 05:49:52.333279: Pseudo dice [np.float32(0.9555)] 
2025-07-12 05:49:52.334146: Epoch time: 68.04 s 
2025-07-12 05:49:53.230823:  
2025-07-12 05:49:53.232110: Epoch 691 
2025-07-12 05:49:53.233076: Current learning rate: 0.00348 
2025-07-12 05:51:01.063791: train_loss -0.9786 
2025-07-12 05:51:01.065006: val_loss -0.9534 
2025-07-12 05:51:01.065894: Pseudo dice [np.float32(0.958)] 
2025-07-12 05:51:01.066879: Epoch time: 67.84 s 
2025-07-12 05:51:01.967080:  
2025-07-12 05:51:01.968556: Epoch 692 
2025-07-12 05:51:01.969589: Current learning rate: 0.00346 
2025-07-12 05:52:09.973700: train_loss -0.978 
2025-07-12 05:52:09.974915: val_loss -0.9534 
2025-07-12 05:52:09.975901: Pseudo dice [np.float32(0.957)] 
2025-07-12 05:52:09.977023: Epoch time: 68.01 s 
2025-07-12 05:52:10.869124:  
2025-07-12 05:52:10.870852: Epoch 693 
2025-07-12 05:52:10.872062: Current learning rate: 0.00345 
2025-07-12 05:53:18.991043: train_loss -0.9788 
2025-07-12 05:53:18.992170: val_loss -0.9499 
2025-07-12 05:53:18.993101: Pseudo dice [np.float32(0.9536)] 
2025-07-12 05:53:18.994066: Epoch time: 68.13 s 
2025-07-12 05:53:19.891501:  
2025-07-12 05:53:19.893111: Epoch 694 
2025-07-12 05:53:19.894205: Current learning rate: 0.00344 
2025-07-12 05:54:27.936704: train_loss -0.9791 
2025-07-12 05:54:27.937827: val_loss -0.9526 
2025-07-12 05:54:27.938791: Pseudo dice [np.float32(0.9564)] 
2025-07-12 05:54:27.939856: Epoch time: 68.05 s 
2025-07-12 05:54:28.838978:  
2025-07-12 05:54:28.840446: Epoch 695 
2025-07-12 05:54:28.841515: Current learning rate: 0.00343 
2025-07-12 05:55:36.887341: train_loss -0.9791 
2025-07-12 05:55:36.888527: val_loss -0.9519 
2025-07-12 05:55:36.889442: Pseudo dice [np.float32(0.9559)] 
2025-07-12 05:55:36.890340: Epoch time: 68.05 s 
2025-07-12 05:55:37.798485:  
2025-07-12 05:55:37.800022: Epoch 696 
2025-07-12 05:55:37.801211: Current learning rate: 0.00342 
2025-07-12 05:56:45.844956: train_loss -0.9786 
2025-07-12 05:56:45.846075: val_loss -0.9505 
2025-07-12 05:56:45.846985: Pseudo dice [np.float32(0.9544)] 
2025-07-12 05:56:45.848032: Epoch time: 68.05 s 
2025-07-12 05:56:47.064477:  
2025-07-12 05:56:47.066109: Epoch 697 
2025-07-12 05:56:47.067216: Current learning rate: 0.00341 
2025-07-12 05:57:55.079849: train_loss -0.9784 
2025-07-12 05:57:55.081013: val_loss -0.9526 
2025-07-12 05:57:55.082062: Pseudo dice [np.float32(0.9562)] 
2025-07-12 05:57:55.083423: Epoch time: 68.02 s 
2025-07-12 05:57:55.989290:  
2025-07-12 05:57:55.990719: Epoch 698 
2025-07-12 05:57:55.991758: Current learning rate: 0.0034 
2025-07-12 05:59:03.884512: train_loss -0.9788 
2025-07-12 05:59:03.885662: val_loss -0.9544 
2025-07-12 05:59:03.886649: Pseudo dice [np.float32(0.9581)] 
2025-07-12 05:59:03.887580: Epoch time: 67.9 s 
2025-07-12 05:59:04.784294:  
2025-07-12 05:59:04.785880: Epoch 699 
2025-07-12 05:59:04.787079: Current learning rate: 0.00339 
2025-07-12 06:00:12.679697: train_loss -0.9791 
2025-07-12 06:00:12.680886: val_loss -0.9526 
2025-07-12 06:00:12.682009: Pseudo dice [np.float32(0.9564)] 
2025-07-12 06:00:12.682909: Epoch time: 67.9 s 
2025-07-12 06:00:14.661410:  
2025-07-12 06:00:14.662606: Epoch 700 
2025-07-12 06:00:14.663582: Current learning rate: 0.00338 
2025-07-12 06:01:22.640924: train_loss -0.9802 
2025-07-12 06:01:22.642210: val_loss -0.9532 
2025-07-12 06:01:22.643110: Pseudo dice [np.float32(0.9569)] 
2025-07-12 06:01:22.644047: Epoch time: 67.98 s 
2025-07-12 06:01:23.549457:  
2025-07-12 06:01:23.551007: Epoch 701 
2025-07-12 06:01:23.552014: Current learning rate: 0.00337 
2025-07-12 06:02:31.415181: train_loss -0.98 
2025-07-12 06:02:31.416419: val_loss -0.952 
2025-07-12 06:02:31.417366: Pseudo dice [np.float32(0.9559)] 
2025-07-12 06:02:31.418601: Epoch time: 67.87 s 
2025-07-12 06:02:32.318599:  
2025-07-12 06:02:32.320054: Epoch 702 
2025-07-12 06:02:32.321064: Current learning rate: 0.00336 
2025-07-12 06:03:40.530101: train_loss -0.9798 
2025-07-12 06:03:40.531157: val_loss -0.9554 
2025-07-12 06:03:40.532131: Pseudo dice [np.float32(0.9588)] 
2025-07-12 06:03:40.533070: Epoch time: 68.21 s 
2025-07-12 06:03:41.420154:  
2025-07-12 06:03:41.421670: Epoch 703 
2025-07-12 06:03:41.422713: Current learning rate: 0.00335 
2025-07-12 06:04:49.609521: train_loss -0.9792 
2025-07-12 06:04:49.610899: val_loss -0.9545 
2025-07-12 06:04:49.611803: Pseudo dice [np.float32(0.9579)] 
2025-07-12 06:04:49.612771: Epoch time: 68.19 s 
2025-07-12 06:04:50.508220:  
2025-07-12 06:04:50.509839: Epoch 704 
2025-07-12 06:04:50.510870: Current learning rate: 0.00334 
2025-07-12 06:05:58.460161: train_loss -0.9783 
2025-07-12 06:05:58.461388: val_loss -0.9483 
2025-07-12 06:05:58.462362: Pseudo dice [np.float32(0.9527)] 
2025-07-12 06:05:58.463276: Epoch time: 67.96 s 
2025-07-12 06:05:59.370778:  
2025-07-12 06:05:59.372023: Epoch 705 
2025-07-12 06:05:59.373122: Current learning rate: 0.00333 
2025-07-12 06:07:07.327382: train_loss -0.9788 
2025-07-12 06:07:07.328715: val_loss -0.9556 
2025-07-12 06:07:07.330041: Pseudo dice [np.float32(0.9592)] 
2025-07-12 06:07:07.331138: Epoch time: 67.96 s 
2025-07-12 06:07:08.239913:  
2025-07-12 06:07:08.241493: Epoch 706 
2025-07-12 06:07:08.242561: Current learning rate: 0.00332 
2025-07-12 06:08:16.325992: train_loss -0.9787 
2025-07-12 06:08:16.327325: val_loss -0.9516 
2025-07-12 06:08:16.328250: Pseudo dice [np.float32(0.9554)] 
2025-07-12 06:08:16.329134: Epoch time: 68.09 s 
2025-07-12 06:08:17.223114:  
2025-07-12 06:08:17.224638: Epoch 707 
2025-07-12 06:08:17.225649: Current learning rate: 0.00331 
2025-07-12 06:09:25.424659: train_loss -0.9785 
2025-07-12 06:09:25.425879: val_loss -0.9525 
2025-07-12 06:09:25.426867: Pseudo dice [np.float32(0.9561)] 
2025-07-12 06:09:25.427735: Epoch time: 68.2 s 
2025-07-12 06:09:26.322933:  
2025-07-12 06:09:26.324220: Epoch 708 
2025-07-12 06:09:26.325304: Current learning rate: 0.0033 
2025-07-12 06:10:34.318880: train_loss -0.9795 
2025-07-12 06:10:34.320205: val_loss -0.9532 
2025-07-12 06:10:34.321290: Pseudo dice [np.float32(0.9572)] 
2025-07-12 06:10:34.322215: Epoch time: 68.0 s 
2025-07-12 06:10:35.216980:  
2025-07-12 06:10:35.218682: Epoch 709 
2025-07-12 06:10:35.219823: Current learning rate: 0.00329 
2025-07-12 06:11:43.147369: train_loss -0.98 
2025-07-12 06:11:43.148544: val_loss -0.9526 
2025-07-12 06:11:43.149466: Pseudo dice [np.float32(0.9563)] 
2025-07-12 06:11:43.150341: Epoch time: 67.93 s 
2025-07-12 06:11:44.044007:  
2025-07-12 06:11:44.045326: Epoch 710 
2025-07-12 06:11:44.046244: Current learning rate: 0.00328 
2025-07-12 06:12:52.133412: train_loss -0.9799 
2025-07-12 06:12:52.134689: val_loss -0.9537 
2025-07-12 06:12:52.135821: Pseudo dice [np.float32(0.9572)] 
2025-07-12 06:12:52.136842: Epoch time: 68.09 s 
2025-07-12 06:12:53.043777:  
2025-07-12 06:12:53.045171: Epoch 711 
2025-07-12 06:12:53.046123: Current learning rate: 0.00327 
2025-07-12 06:14:00.988912: train_loss -0.9802 
2025-07-12 06:14:00.990407: val_loss -0.9528 
2025-07-12 06:14:00.991366: Pseudo dice [np.float32(0.9569)] 
2025-07-12 06:14:00.992435: Epoch time: 67.95 s 
2025-07-12 06:14:01.889714:  
2025-07-12 06:14:01.891212: Epoch 712 
2025-07-12 06:14:01.892262: Current learning rate: 0.00326 
2025-07-12 06:15:09.813348: train_loss -0.9791 
2025-07-12 06:15:09.814578: val_loss -0.9496 
2025-07-12 06:15:09.815709: Pseudo dice [np.float32(0.9535)] 
2025-07-12 06:15:09.816826: Epoch time: 67.93 s 
2025-07-12 06:15:10.727398:  
2025-07-12 06:15:10.728805: Epoch 713 
2025-07-12 06:15:10.729831: Current learning rate: 0.00325 
2025-07-12 06:16:18.626123: train_loss -0.9806 
2025-07-12 06:16:18.627285: val_loss -0.954 
2025-07-12 06:16:18.628124: Pseudo dice [np.float32(0.9579)] 
2025-07-12 06:16:18.629088: Epoch time: 67.9 s 
2025-07-12 06:16:19.529317:  
2025-07-12 06:16:19.530720: Epoch 714 
2025-07-12 06:16:19.531804: Current learning rate: 0.00324 
2025-07-12 06:17:27.576223: train_loss -0.9799 
2025-07-12 06:17:27.577667: val_loss -0.9564 
2025-07-12 06:17:27.578511: Pseudo dice [np.float32(0.9601)] 
2025-07-12 06:17:27.579477: Epoch time: 68.05 s 
2025-07-12 06:17:28.474787:  
2025-07-12 06:17:28.476136: Epoch 715 
2025-07-12 06:17:28.477073: Current learning rate: 0.00323 
2025-07-12 06:18:36.457329: train_loss -0.9799 
2025-07-12 06:18:36.458460: val_loss -0.9495 
2025-07-12 06:18:36.459390: Pseudo dice [np.float32(0.9531)] 
2025-07-12 06:18:36.460293: Epoch time: 67.99 s 
2025-07-12 06:18:37.364511:  
2025-07-12 06:18:37.366001: Epoch 716 
2025-07-12 06:18:37.366973: Current learning rate: 0.00322 
2025-07-12 06:19:45.440825: train_loss -0.9784 
2025-07-12 06:19:45.441944: val_loss -0.9533 
2025-07-12 06:19:45.442805: Pseudo dice [np.float32(0.9569)] 
2025-07-12 06:19:45.443743: Epoch time: 68.08 s 
2025-07-12 06:19:46.340633:  
2025-07-12 06:19:46.342335: Epoch 717 
2025-07-12 06:19:46.343404: Current learning rate: 0.00321 
2025-07-12 06:20:54.678352: train_loss -0.9783 
2025-07-12 06:20:54.679573: val_loss -0.9518 
2025-07-12 06:20:54.680473: Pseudo dice [np.float32(0.9554)] 
2025-07-12 06:20:54.681383: Epoch time: 68.34 s 
2025-07-12 06:20:55.579919:  
2025-07-12 06:20:55.581745: Epoch 718 
2025-07-12 06:20:55.582718: Current learning rate: 0.0032 
2025-07-12 06:22:03.607477: train_loss -0.9785 
2025-07-12 06:22:03.608725: val_loss -0.954 
2025-07-12 06:22:03.610142: Pseudo dice [np.float32(0.9578)] 
2025-07-12 06:22:03.611049: Epoch time: 68.03 s 
2025-07-12 06:22:04.503355:  
2025-07-12 06:22:04.504776: Epoch 719 
2025-07-12 06:22:04.505818: Current learning rate: 0.00319 
2025-07-12 06:23:12.610623: train_loss -0.9798 
2025-07-12 06:23:12.611989: val_loss -0.9525 
2025-07-12 06:23:12.613024: Pseudo dice [np.float32(0.9559)] 
2025-07-12 06:23:12.613963: Epoch time: 68.11 s 
2025-07-12 06:23:13.513042:  
2025-07-12 06:23:13.514578: Epoch 720 
2025-07-12 06:23:13.515791: Current learning rate: 0.00318 
2025-07-12 06:24:21.758838: train_loss -0.9798 
2025-07-12 06:24:21.760040: val_loss -0.9525 
2025-07-12 06:24:21.761073: Pseudo dice [np.float32(0.9563)] 
2025-07-12 06:24:21.762046: Epoch time: 68.25 s 
2025-07-12 06:24:22.885699:  
2025-07-12 06:24:22.887035: Epoch 721 
2025-07-12 06:24:22.888021: Current learning rate: 0.00317 
2025-07-12 06:25:31.031606: train_loss -0.9795 
2025-07-12 06:25:31.032814: val_loss -0.9489 
2025-07-12 06:25:31.033749: Pseudo dice [np.float32(0.9531)] 
2025-07-12 06:25:31.034956: Epoch time: 68.15 s 
2025-07-12 06:25:31.941615:  
2025-07-12 06:25:31.942799: Epoch 722 
2025-07-12 06:25:31.943820: Current learning rate: 0.00316 
2025-07-12 06:26:40.129026: train_loss -0.9788 
2025-07-12 06:26:40.130265: val_loss -0.9513 
2025-07-12 06:26:40.131145: Pseudo dice [np.float32(0.9548)] 
2025-07-12 06:26:40.132013: Epoch time: 68.19 s 
2025-07-12 06:26:41.036003:  
2025-07-12 06:26:41.037963: Epoch 723 
2025-07-12 06:26:41.038911: Current learning rate: 0.00315 
2025-07-12 06:27:49.107191: train_loss -0.9802 
2025-07-12 06:27:49.108396: val_loss -0.9534 
2025-07-12 06:27:49.109471: Pseudo dice [np.float32(0.9576)] 
2025-07-12 06:27:49.110389: Epoch time: 68.07 s 
2025-07-12 06:27:50.010391:  
2025-07-12 06:27:50.011703: Epoch 724 
2025-07-12 06:27:50.012632: Current learning rate: 0.00314 
2025-07-12 06:28:58.178552: train_loss -0.9803 
2025-07-12 06:28:58.184486: val_loss -0.9527 
2025-07-12 06:28:58.185602: Pseudo dice [np.float32(0.9568)] 
2025-07-12 06:28:58.186495: Epoch time: 68.17 s 
2025-07-12 06:28:59.100381:  
2025-07-12 06:28:59.101973: Epoch 725 
2025-07-12 06:28:59.103178: Current learning rate: 0.00313 
2025-07-12 06:30:07.208030: train_loss -0.9793 
2025-07-12 06:30:07.209146: val_loss -0.9531 
2025-07-12 06:30:07.210210: Pseudo dice [np.float32(0.9568)] 
2025-07-12 06:30:07.211092: Epoch time: 68.11 s 
2025-07-12 06:30:08.116855:  
2025-07-12 06:30:08.118104: Epoch 726 
2025-07-12 06:30:08.119100: Current learning rate: 0.00312 
2025-07-12 06:31:16.084818: train_loss -0.9803 
2025-07-12 06:31:16.086133: val_loss -0.954 
2025-07-12 06:31:16.087100: Pseudo dice [np.float32(0.958)] 
2025-07-12 06:31:16.088093: Epoch time: 67.97 s 
2025-07-12 06:31:16.992228:  
2025-07-12 06:31:16.993928: Epoch 727 
2025-07-12 06:31:16.994951: Current learning rate: 0.00311 
2025-07-12 06:32:24.929057: train_loss -0.9802 
2025-07-12 06:32:24.930159: val_loss -0.9538 
2025-07-12 06:32:24.931049: Pseudo dice [np.float32(0.9575)] 
2025-07-12 06:32:24.932218: Epoch time: 67.94 s 
2025-07-12 06:32:26.060366:  
2025-07-12 06:32:26.062016: Epoch 728 
2025-07-12 06:32:26.063061: Current learning rate: 0.0031 
2025-07-12 06:33:34.025139: train_loss -0.9797 
2025-07-12 06:33:34.026379: val_loss -0.952 
2025-07-12 06:33:34.027405: Pseudo dice [np.float32(0.9565)] 
2025-07-12 06:33:34.028339: Epoch time: 67.97 s 
2025-07-12 06:33:34.928984:  
2025-07-12 06:33:34.930261: Epoch 729 
2025-07-12 06:33:34.931241: Current learning rate: 0.00309 
2025-07-12 06:34:43.081395: train_loss -0.9799 
2025-07-12 06:34:43.082475: val_loss -0.9524 
2025-07-12 06:34:43.083515: Pseudo dice [np.float32(0.9567)] 
2025-07-12 06:34:43.084709: Epoch time: 68.16 s 
2025-07-12 06:34:43.964268:  
2025-07-12 06:34:43.965750: Epoch 730 
2025-07-12 06:34:43.966882: Current learning rate: 0.00308 
2025-07-12 06:35:51.913577: train_loss -0.9794 
2025-07-12 06:35:51.914812: val_loss -0.954 
2025-07-12 06:35:51.915700: Pseudo dice [np.float32(0.9582)] 
2025-07-12 06:35:51.916870: Epoch time: 67.95 s 
2025-07-12 06:35:52.812180:  
2025-07-12 06:35:52.813457: Epoch 731 
2025-07-12 06:35:52.814375: Current learning rate: 0.00307 
2025-07-12 06:37:00.931055: train_loss -0.979 
2025-07-12 06:37:00.932156: val_loss -0.9501 
2025-07-12 06:37:00.933066: Pseudo dice [np.float32(0.9545)] 
2025-07-12 06:37:00.934035: Epoch time: 68.12 s 
2025-07-12 06:37:01.843298:  
2025-07-12 06:37:01.844912: Epoch 732 
2025-07-12 06:37:01.845952: Current learning rate: 0.00306 
2025-07-12 06:38:10.154852: train_loss -0.9795 
2025-07-12 06:38:10.155946: val_loss -0.953 
2025-07-12 06:38:10.157120: Pseudo dice [np.float32(0.9573)] 
2025-07-12 06:38:10.158147: Epoch time: 68.31 s 
2025-07-12 06:38:11.072829:  
2025-07-12 06:38:11.074475: Epoch 733 
2025-07-12 06:38:11.075582: Current learning rate: 0.00305 
2025-07-12 06:39:19.126559: train_loss -0.9798 
2025-07-12 06:39:19.127723: val_loss -0.9543 
2025-07-12 06:39:19.128770: Pseudo dice [np.float32(0.958)] 
2025-07-12 06:39:19.129838: Epoch time: 68.06 s 
2025-07-12 06:39:20.024000:  
2025-07-12 06:39:20.025635: Epoch 734 
2025-07-12 06:39:20.026650: Current learning rate: 0.00304 
2025-07-12 06:40:28.140579: train_loss -0.9798 
2025-07-12 06:40:28.141769: val_loss -0.9515 
2025-07-12 06:40:28.142871: Pseudo dice [np.float32(0.9555)] 
2025-07-12 06:40:28.143906: Epoch time: 68.12 s 
2025-07-12 06:40:29.270645:  
2025-07-12 06:40:29.272400: Epoch 735 
2025-07-12 06:40:29.273407: Current learning rate: 0.00303 
2025-07-12 06:41:37.399913: train_loss -0.9802 
2025-07-12 06:41:37.401041: val_loss -0.9521 
2025-07-12 06:41:37.402328: Pseudo dice [np.float32(0.956)] 
2025-07-12 06:41:37.403193: Epoch time: 68.13 s 
2025-07-12 06:41:38.314015:  
2025-07-12 06:41:38.315357: Epoch 736 
2025-07-12 06:41:38.316335: Current learning rate: 0.00302 
2025-07-12 06:42:46.523184: train_loss -0.9803 
2025-07-12 06:42:46.524345: val_loss -0.953 
2025-07-12 06:42:46.525186: Pseudo dice [np.float32(0.9571)] 
2025-07-12 06:42:46.526049: Epoch time: 68.21 s 
2025-07-12 06:42:47.434235:  
2025-07-12 06:42:47.435724: Epoch 737 
2025-07-12 06:42:47.436609: Current learning rate: 0.00301 
2025-07-12 06:43:55.488398: train_loss -0.9802 
2025-07-12 06:43:55.489629: val_loss -0.9526 
2025-07-12 06:43:55.490493: Pseudo dice [np.float32(0.9563)] 
2025-07-12 06:43:55.491479: Epoch time: 68.06 s 
2025-07-12 06:43:56.385435:  
2025-07-12 06:43:56.387047: Epoch 738 
2025-07-12 06:43:56.388011: Current learning rate: 0.003 
2025-07-12 06:45:04.496244: train_loss -0.9804 
2025-07-12 06:45:04.498275: val_loss -0.9546 
2025-07-12 06:45:04.499211: Pseudo dice [np.float32(0.9586)] 
2025-07-12 06:45:04.500222: Epoch time: 68.11 s 
2025-07-12 06:45:05.401868:  
2025-07-12 06:45:05.403136: Epoch 739 
2025-07-12 06:45:05.404117: Current learning rate: 0.00299 
2025-07-12 06:46:13.394970: train_loss -0.9801 
2025-07-12 06:46:13.396088: val_loss -0.9492 
2025-07-12 06:46:13.396965: Pseudo dice [np.float32(0.9529)] 
2025-07-12 06:46:13.397887: Epoch time: 68.0 s 
2025-07-12 06:46:14.308001:  
2025-07-12 06:46:14.309123: Epoch 740 
2025-07-12 06:46:14.310220: Current learning rate: 0.00297 
2025-07-12 06:47:22.298954: train_loss -0.9801 
2025-07-12 06:47:22.300144: val_loss -0.9522 
2025-07-12 06:47:22.301174: Pseudo dice [np.float32(0.9562)] 
2025-07-12 06:47:22.302123: Epoch time: 67.99 s 
2025-07-12 06:47:23.200795:  
2025-07-12 06:47:23.202143: Epoch 741 
2025-07-12 06:47:23.203201: Current learning rate: 0.00296 
2025-07-12 06:48:31.215056: train_loss -0.9798 
2025-07-12 06:48:31.216279: val_loss -0.9543 
2025-07-12 06:48:31.217331: Pseudo dice [np.float32(0.9585)] 
2025-07-12 06:48:31.218341: Epoch time: 68.02 s 
2025-07-12 06:48:32.345747:  
2025-07-12 06:48:32.347230: Epoch 742 
2025-07-12 06:48:32.348213: Current learning rate: 0.00295 
2025-07-12 06:49:40.283203: train_loss -0.9803 
2025-07-12 06:49:40.284374: val_loss -0.9509 
2025-07-12 06:49:40.285238: Pseudo dice [np.float32(0.9544)] 
2025-07-12 06:49:40.286371: Epoch time: 67.94 s 
2025-07-12 06:49:41.170195:  
2025-07-12 06:49:41.171672: Epoch 743 
2025-07-12 06:49:41.172707: Current learning rate: 0.00294 
2025-07-12 06:50:49.170691: train_loss -0.9802 
2025-07-12 06:50:49.171918: val_loss -0.9492 
2025-07-12 06:50:49.172949: Pseudo dice [np.float32(0.9528)] 
2025-07-12 06:50:49.174220: Epoch time: 68.0 s 
2025-07-12 06:50:50.064775:  
2025-07-12 06:50:50.066084: Epoch 744 
2025-07-12 06:50:50.067151: Current learning rate: 0.00293 
2025-07-12 06:51:57.973491: train_loss -0.9806 
2025-07-12 06:51:57.974834: val_loss -0.9541 
2025-07-12 06:51:57.975842: Pseudo dice [np.float32(0.9577)] 
2025-07-12 06:51:57.976730: Epoch time: 67.91 s 
2025-07-12 06:51:58.885846:  
2025-07-12 06:51:58.887472: Epoch 745 
2025-07-12 06:51:58.888530: Current learning rate: 0.00292 
2025-07-12 06:53:07.023468: train_loss -0.9806 
2025-07-12 06:53:07.024867: val_loss -0.9538 
2025-07-12 06:53:07.026518: Pseudo dice [np.float32(0.958)] 
2025-07-12 06:53:07.027661: Epoch time: 68.14 s 
2025-07-12 06:53:07.922344:  
2025-07-12 06:53:07.923852: Epoch 746 
2025-07-12 06:53:07.924901: Current learning rate: 0.00291 
2025-07-12 06:54:16.273779: train_loss -0.9795 
2025-07-12 06:54:16.274924: val_loss -0.9545 
2025-07-12 06:54:16.275855: Pseudo dice [np.float32(0.9586)] 
2025-07-12 06:54:16.276872: Epoch time: 68.35 s 
2025-07-12 06:54:17.180668:  
2025-07-12 06:54:17.182084: Epoch 747 
2025-07-12 06:54:17.183088: Current learning rate: 0.0029 
2025-07-12 06:55:25.324669: train_loss -0.9795 
2025-07-12 06:55:25.325840: val_loss -0.9531 
2025-07-12 06:55:25.326799: Pseudo dice [np.float32(0.9565)] 
2025-07-12 06:55:25.327733: Epoch time: 68.15 s 
2025-07-12 06:55:26.227248:  
2025-07-12 06:55:26.228797: Epoch 748 
2025-07-12 06:55:26.229992: Current learning rate: 0.00289 
2025-07-12 06:56:34.348229: train_loss -0.9801 
2025-07-12 06:56:34.349472: val_loss -0.9493 
2025-07-12 06:56:34.350528: Pseudo dice [np.float32(0.9532)] 
2025-07-12 06:56:34.351372: Epoch time: 68.12 s 
2025-07-12 06:56:35.484028:  
2025-07-12 06:56:35.485426: Epoch 749 
2025-07-12 06:56:35.486437: Current learning rate: 0.00288 
2025-07-12 06:57:43.697325: train_loss -0.9788 
2025-07-12 06:57:43.698545: val_loss -0.9519 
2025-07-12 06:57:43.699780: Pseudo dice [np.float32(0.9558)] 
2025-07-12 06:57:43.700981: Epoch time: 68.22 s 
2025-07-12 06:57:46.320741:  
2025-07-12 06:57:46.322682: Epoch 750 
2025-07-12 06:57:46.323855: Current learning rate: 0.00287 
2025-07-12 06:58:54.480727: train_loss -0.9805 
2025-07-12 06:58:54.481857: val_loss -0.952 
2025-07-12 06:58:54.482786: Pseudo dice [np.float32(0.9558)] 
2025-07-12 06:58:54.483742: Epoch time: 68.16 s 
2025-07-12 06:58:55.391304:  
2025-07-12 06:58:55.392642: Epoch 751 
2025-07-12 06:58:55.393614: Current learning rate: 0.00286 
2025-07-12 07:00:03.412300: train_loss -0.9803 
2025-07-12 07:00:03.413516: val_loss -0.9509 
2025-07-12 07:00:03.414503: Pseudo dice [np.float32(0.955)] 
2025-07-12 07:00:03.415598: Epoch time: 68.02 s 
2025-07-12 07:00:04.314305:  
2025-07-12 07:00:04.315643: Epoch 752 
2025-07-12 07:00:04.316607: Current learning rate: 0.00285 
2025-07-12 07:01:12.549627: train_loss -0.9798 
2025-07-12 07:01:12.550900: val_loss -0.9498 
2025-07-12 07:01:12.551844: Pseudo dice [np.float32(0.9539)] 
2025-07-12 07:01:12.552747: Epoch time: 68.24 s 
2025-07-12 07:01:13.460753:  
2025-07-12 07:01:13.461921: Epoch 753 
2025-07-12 07:01:13.462962: Current learning rate: 0.00284 
2025-07-12 07:02:21.475087: train_loss -0.9794 
2025-07-12 07:02:21.476418: val_loss -0.954 
2025-07-12 07:02:21.477431: Pseudo dice [np.float32(0.958)] 
2025-07-12 07:02:21.478389: Epoch time: 68.02 s 
2025-07-12 07:02:22.376464:  
2025-07-12 07:02:22.377883: Epoch 754 
2025-07-12 07:02:22.378899: Current learning rate: 0.00283 
2025-07-12 07:03:30.397305: train_loss -0.9801 
2025-07-12 07:03:30.398602: val_loss -0.9516 
2025-07-12 07:03:30.399526: Pseudo dice [np.float32(0.9556)] 
2025-07-12 07:03:30.400640: Epoch time: 68.02 s 
2025-07-12 07:03:31.311139:  
2025-07-12 07:03:31.312625: Epoch 755 
2025-07-12 07:03:31.313819: Current learning rate: 0.00282 
2025-07-12 07:04:39.246924: train_loss -0.9799 
2025-07-12 07:04:39.248329: val_loss -0.9504 
2025-07-12 07:04:39.249395: Pseudo dice [np.float32(0.9542)] 
2025-07-12 07:04:39.250338: Epoch time: 67.94 s 
2025-07-12 07:04:40.386674:  
2025-07-12 07:04:40.388097: Epoch 756 
2025-07-12 07:04:40.389145: Current learning rate: 0.00281 
2025-07-12 07:05:48.434788: train_loss -0.9807 
2025-07-12 07:05:48.436020: val_loss -0.952 
2025-07-12 07:05:48.436922: Pseudo dice [np.float32(0.9555)] 
2025-07-12 07:05:48.437934: Epoch time: 68.05 s 
2025-07-12 07:05:49.355978:  
2025-07-12 07:05:49.358021: Epoch 757 
2025-07-12 07:05:49.359578: Current learning rate: 0.0028 
2025-07-12 07:06:57.644855: train_loss -0.9796 
2025-07-12 07:06:57.646131: val_loss -0.9507 
2025-07-12 07:06:57.647072: Pseudo dice [np.float32(0.9547)] 
2025-07-12 07:06:57.648260: Epoch time: 68.29 s 
2025-07-12 07:06:58.565527:  
2025-07-12 07:06:58.566823: Epoch 758 
2025-07-12 07:06:58.567768: Current learning rate: 0.00279 
2025-07-12 07:08:06.693336: train_loss -0.9783 
2025-07-12 07:08:06.694463: val_loss -0.9532 
2025-07-12 07:08:06.695806: Pseudo dice [np.float32(0.9569)] 
2025-07-12 07:08:06.697208: Epoch time: 68.13 s 
2025-07-12 07:08:07.591673:  
2025-07-12 07:08:07.593171: Epoch 759 
2025-07-12 07:08:07.594223: Current learning rate: 0.00278 
2025-07-12 07:09:15.816903: train_loss -0.978 
2025-07-12 07:09:15.818122: val_loss -0.9504 
2025-07-12 07:09:15.819196: Pseudo dice [np.float32(0.9547)] 
2025-07-12 07:09:15.820200: Epoch time: 68.23 s 
2025-07-12 07:09:16.750848:  
2025-07-12 07:09:16.753505: Epoch 760 
2025-07-12 07:09:16.754600: Current learning rate: 0.00277 
2025-07-12 07:10:24.693808: train_loss -0.9787 
2025-07-12 07:10:24.695117: val_loss -0.9512 
2025-07-12 07:10:24.696347: Pseudo dice [np.float32(0.9543)] 
2025-07-12 07:10:24.697273: Epoch time: 67.95 s 
2025-07-12 07:10:25.597793:  
2025-07-12 07:10:25.599315: Epoch 761 
2025-07-12 07:10:25.600308: Current learning rate: 0.00276 
2025-07-12 07:11:33.449381: train_loss -0.9794 
2025-07-12 07:11:33.450672: val_loss -0.9501 
2025-07-12 07:11:33.451532: Pseudo dice [np.float32(0.954)] 
2025-07-12 07:11:33.452456: Epoch time: 67.86 s 
2025-07-12 07:11:34.345513:  
2025-07-12 07:11:34.346973: Epoch 762 
2025-07-12 07:11:34.348008: Current learning rate: 0.00275 
2025-07-12 07:12:42.303642: train_loss -0.9798 
2025-07-12 07:12:42.304913: val_loss -0.9526 
2025-07-12 07:12:42.305968: Pseudo dice [np.float32(0.9562)] 
2025-07-12 07:12:42.306900: Epoch time: 67.96 s 
2025-07-12 07:12:43.437650:  
2025-07-12 07:12:43.439132: Epoch 763 
2025-07-12 07:12:43.440094: Current learning rate: 0.00274 
2025-07-12 07:13:51.430916: train_loss -0.9793 
2025-07-12 07:13:51.432307: val_loss -0.953 
2025-07-12 07:13:51.433853: Pseudo dice [np.float32(0.9574)] 
2025-07-12 07:13:51.434913: Epoch time: 68.0 s 
2025-07-12 07:13:52.344267:  
2025-07-12 07:13:52.345764: Epoch 764 
2025-07-12 07:13:52.346885: Current learning rate: 0.00273 
2025-07-12 07:15:00.363646: train_loss -0.9798 
2025-07-12 07:15:00.364871: val_loss -0.951 
2025-07-12 07:15:00.366102: Pseudo dice [np.float32(0.9557)] 
2025-07-12 07:15:00.367208: Epoch time: 68.02 s 
2025-07-12 07:15:01.282386:  
2025-07-12 07:15:01.283802: Epoch 765 
2025-07-12 07:15:01.284816: Current learning rate: 0.00272 
2025-07-12 07:16:09.140816: train_loss -0.98 
2025-07-12 07:16:09.141987: val_loss -0.9511 
2025-07-12 07:16:09.143067: Pseudo dice [np.float32(0.9545)] 
2025-07-12 07:16:09.143990: Epoch time: 67.86 s 
2025-07-12 07:16:10.047787:  
2025-07-12 07:16:10.049241: Epoch 766 
2025-07-12 07:16:10.050345: Current learning rate: 0.00271 
2025-07-12 07:17:17.985001: train_loss -0.9791 
2025-07-12 07:17:17.986241: val_loss -0.9522 
2025-07-12 07:17:17.987231: Pseudo dice [np.float32(0.9561)] 
2025-07-12 07:17:17.988636: Epoch time: 67.94 s 
2025-07-12 07:17:18.884009:  
2025-07-12 07:17:18.885316: Epoch 767 
2025-07-12 07:17:18.886234: Current learning rate: 0.0027 
2025-07-12 07:18:26.736462: train_loss -0.9799 
2025-07-12 07:18:26.737585: val_loss -0.9503 
2025-07-12 07:18:26.738475: Pseudo dice [np.float32(0.9549)] 
2025-07-12 07:18:26.739612: Epoch time: 67.86 s 
2025-07-12 07:18:27.648964:  
2025-07-12 07:18:27.650497: Epoch 768 
2025-07-12 07:18:27.651438: Current learning rate: 0.00268 
2025-07-12 07:19:35.610382: train_loss -0.9805 
2025-07-12 07:19:35.611563: val_loss -0.9534 
2025-07-12 07:19:35.612417: Pseudo dice [np.float32(0.9572)] 
2025-07-12 07:19:35.613278: Epoch time: 67.96 s 
2025-07-12 07:19:36.518084:  
2025-07-12 07:19:36.519557: Epoch 769 
2025-07-12 07:19:36.520807: Current learning rate: 0.00267 
2025-07-12 07:20:44.588015: train_loss -0.9796 
2025-07-12 07:20:44.589130: val_loss -0.9475 
2025-07-12 07:20:44.590072: Pseudo dice [np.float32(0.9514)] 
2025-07-12 07:20:44.590970: Epoch time: 68.07 s 
2025-07-12 07:20:45.502904:  
2025-07-12 07:20:45.504345: Epoch 770 
2025-07-12 07:20:45.505396: Current learning rate: 0.00266 
2025-07-12 07:21:53.424146: train_loss -0.9796 
2025-07-12 07:21:53.425366: val_loss -0.9524 
2025-07-12 07:21:53.426237: Pseudo dice [np.float32(0.9557)] 
2025-07-12 07:21:53.427261: Epoch time: 67.92 s 
2025-07-12 07:21:54.315809:  
2025-07-12 07:21:54.316995: Epoch 771 
2025-07-12 07:21:54.318019: Current learning rate: 0.00265 
2025-07-12 07:23:02.271778: train_loss -0.9794 
2025-07-12 07:23:02.273002: val_loss -0.9529 
2025-07-12 07:23:02.273972: Pseudo dice [np.float32(0.9565)] 
2025-07-12 07:23:02.274914: Epoch time: 67.96 s 
2025-07-12 07:23:03.177287:  
2025-07-12 07:23:03.178888: Epoch 772 
2025-07-12 07:23:03.179862: Current learning rate: 0.00264 
2025-07-12 07:24:11.066696: train_loss -0.9802 
2025-07-12 07:24:11.067831: val_loss -0.9524 
2025-07-12 07:24:11.068671: Pseudo dice [np.float32(0.9559)] 
2025-07-12 07:24:11.069612: Epoch time: 67.89 s 
2025-07-12 07:24:11.987302:  
2025-07-12 07:24:11.988777: Epoch 773 
2025-07-12 07:24:11.989762: Current learning rate: 0.00263 
2025-07-12 07:25:19.912724: train_loss -0.9797 
2025-07-12 07:25:19.913929: val_loss -0.9478 
2025-07-12 07:25:19.914929: Pseudo dice [np.float32(0.9521)] 
2025-07-12 07:25:19.915919: Epoch time: 67.93 s 
2025-07-12 07:25:20.826786:  
2025-07-12 07:25:20.828086: Epoch 774 
2025-07-12 07:25:20.829089: Current learning rate: 0.00262 
2025-07-12 07:26:28.650597: train_loss -0.9795 
2025-07-12 07:26:28.652196: val_loss -0.9542 
2025-07-12 07:26:28.653445: Pseudo dice [np.float32(0.9577)] 
2025-07-12 07:26:28.654470: Epoch time: 67.83 s 
2025-07-12 07:26:29.557909:  
2025-07-12 07:26:29.559218: Epoch 775 
2025-07-12 07:26:29.560274: Current learning rate: 0.00261 
2025-07-12 07:27:37.357308: train_loss -0.9797 
2025-07-12 07:27:37.358634: val_loss -0.9484 
2025-07-12 07:27:37.359663: Pseudo dice [np.float32(0.9519)] 
2025-07-12 07:27:37.360618: Epoch time: 67.8 s 
2025-07-12 07:27:38.265792:  
2025-07-12 07:27:38.267185: Epoch 776 
2025-07-12 07:27:38.268128: Current learning rate: 0.0026 
2025-07-12 07:28:46.413051: train_loss -0.9793 
2025-07-12 07:28:46.414395: val_loss -0.9515 
2025-07-12 07:28:46.415509: Pseudo dice [np.float32(0.9551)] 
2025-07-12 07:28:46.416652: Epoch time: 68.15 s 
2025-07-12 07:28:47.316461:  
2025-07-12 07:28:47.317831: Epoch 777 
2025-07-12 07:28:47.318843: Current learning rate: 0.00259 
2025-07-12 07:29:55.231599: train_loss -0.9797 
2025-07-12 07:29:55.232730: val_loss -0.9521 
2025-07-12 07:29:55.233652: Pseudo dice [np.float32(0.9564)] 
2025-07-12 07:29:55.234606: Epoch time: 67.92 s 
2025-07-12 07:29:56.137254:  
2025-07-12 07:29:56.138901: Epoch 778 
2025-07-12 07:29:56.139966: Current learning rate: 0.00258 
2025-07-12 07:31:04.082151: train_loss -0.9799 
2025-07-12 07:31:04.083295: val_loss -0.9478 
2025-07-12 07:31:04.084210: Pseudo dice [np.float32(0.9516)] 
2025-07-12 07:31:04.085208: Epoch time: 67.95 s 
2025-07-12 07:31:05.002368:  
2025-07-12 07:31:05.003881: Epoch 779 
2025-07-12 07:31:05.004931: Current learning rate: 0.00257 
2025-07-12 07:32:12.972943: train_loss -0.9803 
2025-07-12 07:32:12.974107: val_loss -0.9523 
2025-07-12 07:32:12.975080: Pseudo dice [np.float32(0.9561)] 
2025-07-12 07:32:12.975945: Epoch time: 67.97 s 
2025-07-12 07:32:13.884499:  
2025-07-12 07:32:13.886031: Epoch 780 
2025-07-12 07:32:13.887099: Current learning rate: 0.00256 
2025-07-12 07:33:21.949660: train_loss -0.9805 
2025-07-12 07:33:21.951001: val_loss -0.9503 
2025-07-12 07:33:21.951894: Pseudo dice [np.float32(0.9537)] 
2025-07-12 07:33:21.952823: Epoch time: 68.07 s 
2025-07-12 07:33:22.855289:  
2025-07-12 07:33:22.856822: Epoch 781 
2025-07-12 07:33:22.857757: Current learning rate: 0.00255 
2025-07-12 07:34:30.818500: train_loss -0.9805 
2025-07-12 07:34:30.820163: val_loss -0.9531 
2025-07-12 07:34:30.821233: Pseudo dice [np.float32(0.9562)] 
2025-07-12 07:34:30.822290: Epoch time: 67.97 s 
2025-07-12 07:34:31.735790:  
2025-07-12 07:34:31.737401: Epoch 782 
2025-07-12 07:34:31.738431: Current learning rate: 0.00254 
2025-07-12 07:35:39.747810: train_loss -0.9807 
2025-07-12 07:35:39.748870: val_loss -0.949 
2025-07-12 07:35:39.749895: Pseudo dice [np.float32(0.9525)] 
2025-07-12 07:35:39.750805: Epoch time: 68.02 s 
2025-07-12 07:35:40.659793:  
2025-07-12 07:35:40.661238: Epoch 783 
2025-07-12 07:35:40.662272: Current learning rate: 0.00253 
2025-07-12 07:36:48.799755: train_loss -0.9804 
2025-07-12 07:36:48.800852: val_loss -0.9534 
2025-07-12 07:36:48.801772: Pseudo dice [np.float32(0.9568)] 
2025-07-12 07:36:48.802711: Epoch time: 68.14 s 
2025-07-12 07:36:49.718769:  
2025-07-12 07:36:49.720255: Epoch 784 
2025-07-12 07:36:49.721294: Current learning rate: 0.00252 
2025-07-12 07:37:57.606083: train_loss -0.98 
2025-07-12 07:37:57.607310: val_loss -0.956 
2025-07-12 07:37:57.608529: Pseudo dice [np.float32(0.9594)] 
2025-07-12 07:37:57.609475: Epoch time: 67.89 s 
2025-07-12 07:37:58.519273:  
2025-07-12 07:37:58.520705: Epoch 785 
2025-07-12 07:37:58.521652: Current learning rate: 0.00251 
2025-07-12 07:39:06.502804: train_loss -0.9806 
2025-07-12 07:39:06.504149: val_loss -0.9472 
2025-07-12 07:39:06.505170: Pseudo dice [np.float32(0.9513)] 
2025-07-12 07:39:06.506145: Epoch time: 67.99 s 
2025-07-12 07:39:07.420053:  
2025-07-12 07:39:07.421662: Epoch 786 
2025-07-12 07:39:07.422758: Current learning rate: 0.0025 
2025-07-12 07:40:15.385225: train_loss -0.981 
2025-07-12 07:40:15.386731: val_loss -0.9488 
2025-07-12 07:40:15.387768: Pseudo dice [np.float32(0.9528)] 
2025-07-12 07:40:15.389118: Epoch time: 67.97 s 
2025-07-12 07:40:16.524548:  
2025-07-12 07:40:16.526154: Epoch 787 
2025-07-12 07:40:16.527135: Current learning rate: 0.00249 
2025-07-12 07:41:24.499454: train_loss -0.98 
2025-07-12 07:41:24.500546: val_loss -0.9501 
2025-07-12 07:41:24.501723: Pseudo dice [np.float32(0.9545)] 
2025-07-12 07:41:24.502704: Epoch time: 67.98 s 
2025-07-12 07:41:25.406330:  
2025-07-12 07:41:25.407973: Epoch 788 
2025-07-12 07:41:25.409006: Current learning rate: 0.00248 
2025-07-12 07:42:33.287422: train_loss -0.9793 
2025-07-12 07:42:33.288695: val_loss -0.952 
2025-07-12 07:42:33.289580: Pseudo dice [np.float32(0.9549)] 
2025-07-12 07:42:33.290518: Epoch time: 67.88 s 
2025-07-12 07:42:34.207172:  
2025-07-12 07:42:34.208524: Epoch 789 
2025-07-12 07:42:34.209586: Current learning rate: 0.00247 
2025-07-12 07:43:42.198552: train_loss -0.9793 
2025-07-12 07:43:42.199650: val_loss -0.9514 
2025-07-12 07:43:42.200545: Pseudo dice [np.float32(0.9548)] 
2025-07-12 07:43:42.201500: Epoch time: 67.99 s 
2025-07-12 07:43:43.105849:  
2025-07-12 07:43:43.107144: Epoch 790 
2025-07-12 07:43:43.108078: Current learning rate: 0.00245 
2025-07-12 07:44:51.251550: train_loss -0.9795 
2025-07-12 07:44:51.252704: val_loss -0.9544 
2025-07-12 07:44:51.253723: Pseudo dice [np.float32(0.9578)] 
2025-07-12 07:44:51.254797: Epoch time: 68.15 s 
2025-07-12 07:44:52.153901:  
2025-07-12 07:44:52.155789: Epoch 791 
2025-07-12 07:44:52.156824: Current learning rate: 0.00244 
2025-07-12 07:46:00.171000: train_loss -0.9799 
2025-07-12 07:46:00.172153: val_loss -0.9517 
2025-07-12 07:46:00.173099: Pseudo dice [np.float32(0.9551)] 
2025-07-12 07:46:00.174270: Epoch time: 68.02 s 
2025-07-12 07:46:01.082269:  
2025-07-12 07:46:01.083874: Epoch 792 
2025-07-12 07:46:01.084907: Current learning rate: 0.00243 
2025-07-12 07:47:09.079711: train_loss -0.9802 
2025-07-12 07:47:09.081079: val_loss -0.9506 
2025-07-12 07:47:09.082088: Pseudo dice [np.float32(0.9542)] 
2025-07-12 07:47:09.083334: Epoch time: 68.0 s 
2025-07-12 07:47:09.985459:  
2025-07-12 07:47:09.986581: Epoch 793 
2025-07-12 07:47:09.987553: Current learning rate: 0.00242 
2025-07-12 07:48:18.050634: train_loss -0.9801 
2025-07-12 07:48:18.051898: val_loss -0.9539 
2025-07-12 07:48:18.052807: Pseudo dice [np.float32(0.9571)] 
2025-07-12 07:48:18.053873: Epoch time: 68.07 s 
2025-07-12 07:48:19.200485:  
2025-07-12 07:48:19.201927: Epoch 794 
2025-07-12 07:48:19.202923: Current learning rate: 0.00241 
2025-07-12 07:49:27.367222: train_loss -0.9808 
2025-07-12 07:49:27.368526: val_loss -0.9535 
2025-07-12 07:49:27.369954: Pseudo dice [np.float32(0.9574)] 
2025-07-12 07:49:27.370943: Epoch time: 68.17 s 
2025-07-12 07:49:28.300780:  
2025-07-12 07:49:28.301988: Epoch 795 
2025-07-12 07:49:28.303110: Current learning rate: 0.0024 
2025-07-12 07:50:36.298402: train_loss -0.9805 
2025-07-12 07:50:36.300431: val_loss -0.952 
2025-07-12 07:50:36.301676: Pseudo dice [np.float32(0.9552)] 
2025-07-12 07:50:36.302543: Epoch time: 68.0 s 
2025-07-12 07:50:37.209231:  
2025-07-12 07:50:37.210868: Epoch 796 
2025-07-12 07:50:37.211909: Current learning rate: 0.00239 
2025-07-12 07:51:45.229256: train_loss -0.9807 
2025-07-12 07:51:45.230396: val_loss -0.9524 
2025-07-12 07:51:45.231256: Pseudo dice [np.float32(0.9559)] 
2025-07-12 07:51:45.232187: Epoch time: 68.02 s 
2025-07-12 07:51:46.138932:  
2025-07-12 07:51:46.140353: Epoch 797 
2025-07-12 07:51:46.141367: Current learning rate: 0.00238 
2025-07-12 07:52:54.298196: train_loss -0.9812 
2025-07-12 07:52:54.299376: val_loss -0.9553 
2025-07-12 07:52:54.300370: Pseudo dice [np.float32(0.9592)] 
2025-07-12 07:52:54.301357: Epoch time: 68.16 s 
2025-07-12 07:52:55.217755:  
2025-07-12 07:52:55.218911: Epoch 798 
2025-07-12 07:52:55.219799: Current learning rate: 0.00237 
2025-07-12 07:54:03.238018: train_loss -0.9808 
2025-07-12 07:54:03.239202: val_loss -0.9502 
2025-07-12 07:54:03.240121: Pseudo dice [np.float32(0.9538)] 
2025-07-12 07:54:03.241069: Epoch time: 68.02 s 
2025-07-12 07:54:04.157496:  
2025-07-12 07:54:04.158671: Epoch 799 
2025-07-12 07:54:04.159616: Current learning rate: 0.00236 
2025-07-12 07:55:12.197123: train_loss -0.9804 
2025-07-12 07:55:12.198290: val_loss -0.9529 
2025-07-12 07:55:12.199159: Pseudo dice [np.float32(0.9564)] 
2025-07-12 07:55:12.200301: Epoch time: 68.04 s 
2025-07-12 07:55:14.190425:  
2025-07-12 07:55:14.191690: Epoch 800 
2025-07-12 07:55:14.192727: Current learning rate: 0.00235 
2025-07-12 07:56:22.347021: train_loss -0.9805 
2025-07-12 07:56:22.348275: val_loss -0.9515 
2025-07-12 07:56:22.349153: Pseudo dice [np.float32(0.9547)] 
2025-07-12 07:56:22.350375: Epoch time: 68.16 s 
2025-07-12 07:56:23.258645:  
2025-07-12 07:56:23.260052: Epoch 801 
2025-07-12 07:56:23.261051: Current learning rate: 0.00234 
2025-07-12 07:57:31.222530: train_loss -0.9807 
2025-07-12 07:57:31.223793: val_loss -0.9533 
2025-07-12 07:57:31.224810: Pseudo dice [np.float32(0.9569)] 
2025-07-12 07:57:31.225804: Epoch time: 67.97 s 
2025-07-12 07:57:32.133541:  
2025-07-12 07:57:32.134923: Epoch 802 
2025-07-12 07:57:32.135962: Current learning rate: 0.00233 
2025-07-12 07:58:40.061005: train_loss -0.9811 
2025-07-12 07:58:40.062640: val_loss -0.9559 
2025-07-12 07:58:40.063555: Pseudo dice [np.float32(0.9591)] 
2025-07-12 07:58:40.064663: Epoch time: 67.93 s 
2025-07-12 07:58:40.967732:  
2025-07-12 07:58:40.968870: Epoch 803 
2025-07-12 07:58:40.969873: Current learning rate: 0.00232 
2025-07-12 07:59:48.922127: train_loss -0.9808 
2025-07-12 07:59:48.923237: val_loss -0.9518 
2025-07-12 07:59:48.924196: Pseudo dice [np.float32(0.9551)] 
2025-07-12 07:59:48.925264: Epoch time: 67.96 s 
2025-07-12 07:59:50.064039:  
2025-07-12 07:59:50.065541: Epoch 804 
2025-07-12 07:59:50.066460: Current learning rate: 0.00231 
2025-07-12 08:00:57.988550: train_loss -0.9807 
2025-07-12 08:00:57.989767: val_loss -0.951 
2025-07-12 08:00:57.990881: Pseudo dice [np.float32(0.955)] 
2025-07-12 08:00:57.992126: Epoch time: 67.93 s 
2025-07-12 08:00:58.904942:  
2025-07-12 08:00:58.906309: Epoch 805 
2025-07-12 08:00:58.907342: Current learning rate: 0.0023 
2025-07-12 08:02:06.834553: train_loss -0.9811 
2025-07-12 08:02:06.835710: val_loss -0.9548 
2025-07-12 08:02:06.836779: Pseudo dice [np.float32(0.9576)] 
2025-07-12 08:02:06.837732: Epoch time: 67.93 s 
2025-07-12 08:02:07.743709:  
2025-07-12 08:02:07.744978: Epoch 806 
2025-07-12 08:02:07.745895: Current learning rate: 0.00229 
2025-07-12 08:03:15.676601: train_loss -0.9812 
2025-07-12 08:03:15.677801: val_loss -0.9523 
2025-07-12 08:03:15.678859: Pseudo dice [np.float32(0.9553)] 
2025-07-12 08:03:15.679776: Epoch time: 67.94 s 
2025-07-12 08:03:16.592818:  
2025-07-12 08:03:16.594299: Epoch 807 
2025-07-12 08:03:16.595212: Current learning rate: 0.00228 
2025-07-12 08:04:24.595151: train_loss -0.9811 
2025-07-12 08:04:24.596257: val_loss -0.9545 
2025-07-12 08:04:24.597215: Pseudo dice [np.float32(0.9578)] 
2025-07-12 08:04:24.598048: Epoch time: 68.01 s 
2025-07-12 08:04:25.513261:  
2025-07-12 08:04:25.514872: Epoch 808 
2025-07-12 08:04:25.516005: Current learning rate: 0.00226 
2025-07-12 08:05:33.374683: train_loss -0.981 
2025-07-12 08:05:33.375948: val_loss -0.9519 
2025-07-12 08:05:33.377017: Pseudo dice [np.float32(0.9552)] 
2025-07-12 08:05:33.378061: Epoch time: 67.86 s 
2025-07-12 08:05:34.265468:  
2025-07-12 08:05:34.266967: Epoch 809 
2025-07-12 08:05:34.268037: Current learning rate: 0.00225 
2025-07-12 08:06:42.079481: train_loss -0.9806 
2025-07-12 08:06:42.080774: val_loss -0.9549 
2025-07-12 08:06:42.081609: Pseudo dice [np.float32(0.9584)] 
2025-07-12 08:06:42.082573: Epoch time: 67.82 s 
2025-07-12 08:06:42.999304:  
2025-07-12 08:06:43.000715: Epoch 810 
2025-07-12 08:06:43.001743: Current learning rate: 0.00224 
2025-07-12 08:07:50.822014: train_loss -0.9812 
2025-07-12 08:07:50.823204: val_loss -0.9547 
2025-07-12 08:07:50.824174: Pseudo dice [np.float32(0.9586)] 
2025-07-12 08:07:50.825058: Epoch time: 67.83 s 
2025-07-12 08:07:51.967485:  
2025-07-12 08:07:51.968929: Epoch 811 
2025-07-12 08:07:51.969909: Current learning rate: 0.00223 
2025-07-12 08:08:59.805359: train_loss -0.9811 
2025-07-12 08:08:59.806852: val_loss -0.9519 
2025-07-12 08:08:59.807816: Pseudo dice [np.float32(0.9553)] 
2025-07-12 08:08:59.808885: Epoch time: 67.84 s 
2025-07-12 08:09:00.715959:  
2025-07-12 08:09:00.717327: Epoch 812 
2025-07-12 08:09:00.718302: Current learning rate: 0.00222 
2025-07-12 08:10:08.559063: train_loss -0.981 
2025-07-12 08:10:08.560124: val_loss -0.9548 
2025-07-12 08:10:08.561218: Pseudo dice [np.float32(0.9583)] 
2025-07-12 08:10:08.562272: Epoch time: 67.85 s 
2025-07-12 08:10:09.472068:  
2025-07-12 08:10:09.473484: Epoch 813 
2025-07-12 08:10:09.474445: Current learning rate: 0.00221 
2025-07-12 08:11:17.332213: train_loss -0.9805 
2025-07-12 08:11:17.333425: val_loss -0.9552 
2025-07-12 08:11:17.334360: Pseudo dice [np.float32(0.959)] 
2025-07-12 08:11:17.335222: Epoch time: 67.86 s 
2025-07-12 08:11:17.336075: Yayy! New best EMA pseudo Dice: 0.9567999839782715 
2025-07-12 08:11:19.517251:  
2025-07-12 08:11:19.518615: Epoch 814 
2025-07-12 08:11:19.519602: Current learning rate: 0.0022 
2025-07-12 08:12:27.530472: train_loss -0.9807 
2025-07-12 08:12:27.531707: val_loss -0.9519 
2025-07-12 08:12:27.532838: Pseudo dice [np.float32(0.9551)] 
2025-07-12 08:12:27.533904: Epoch time: 68.02 s 
2025-07-12 08:12:28.451773:  
2025-07-12 08:12:28.453237: Epoch 815 
2025-07-12 08:12:28.454285: Current learning rate: 0.00219 
2025-07-12 08:13:36.466968: train_loss -0.9807 
2025-07-12 08:13:36.468142: val_loss -0.9517 
2025-07-12 08:13:36.469121: Pseudo dice [np.float32(0.9551)] 
2025-07-12 08:13:36.470020: Epoch time: 68.02 s 
2025-07-12 08:13:37.397369:  
2025-07-12 08:13:37.399105: Epoch 816 
2025-07-12 08:13:37.400240: Current learning rate: 0.00218 
2025-07-12 08:14:45.462094: train_loss -0.9817 
2025-07-12 08:14:45.463607: val_loss -0.9522 
2025-07-12 08:14:45.464527: Pseudo dice [np.float32(0.9554)] 
2025-07-12 08:14:45.465607: Epoch time: 68.07 s 
2025-07-12 08:14:46.378081:  
2025-07-12 08:14:46.379817: Epoch 817 
2025-07-12 08:14:46.381023: Current learning rate: 0.00217 
2025-07-12 08:15:54.541971: train_loss -0.9812 
2025-07-12 08:15:54.543187: val_loss -0.9524 
2025-07-12 08:15:54.544425: Pseudo dice [np.float32(0.9558)] 
2025-07-12 08:15:54.545308: Epoch time: 68.17 s 
2025-07-12 08:15:55.458821:  
2025-07-12 08:15:55.460546: Epoch 818 
2025-07-12 08:15:55.461582: Current learning rate: 0.00216 
2025-07-12 08:17:03.569538: train_loss -0.9806 
2025-07-12 08:17:03.570786: val_loss -0.952 
2025-07-12 08:17:03.572057: Pseudo dice [np.float32(0.9556)] 
2025-07-12 08:17:03.573021: Epoch time: 68.11 s 
2025-07-12 08:17:04.494361:  
2025-07-12 08:17:04.495872: Epoch 819 
2025-07-12 08:17:04.496805: Current learning rate: 0.00215 
2025-07-12 08:18:12.485934: train_loss -0.9813 
2025-07-12 08:18:12.487171: val_loss -0.9556 
2025-07-12 08:18:12.488076: Pseudo dice [np.float32(0.9596)] 
2025-07-12 08:18:12.489011: Epoch time: 68.0 s 
2025-07-12 08:18:13.387766:  
2025-07-12 08:18:13.389100: Epoch 820 
2025-07-12 08:18:13.390217: Current learning rate: 0.00214 
2025-07-12 08:19:21.380471: train_loss -0.981 
2025-07-12 08:19:21.381606: val_loss -0.9518 
2025-07-12 08:19:21.382687: Pseudo dice [np.float32(0.9558)] 
2025-07-12 08:19:21.383829: Epoch time: 68.0 s 
2025-07-12 08:19:22.258394:  
2025-07-12 08:19:22.259934: Epoch 821 
2025-07-12 08:19:22.261018: Current learning rate: 0.00213 
2025-07-12 08:20:30.382668: train_loss -0.9802 
2025-07-12 08:20:30.384501: val_loss -0.9524 
2025-07-12 08:20:30.385478: Pseudo dice [np.float32(0.9557)] 
2025-07-12 08:20:30.386543: Epoch time: 68.13 s 
2025-07-12 08:20:31.265605:  
2025-07-12 08:20:31.267028: Epoch 822 
2025-07-12 08:20:31.268137: Current learning rate: 0.00212 
2025-07-12 08:21:39.316323: train_loss -0.9813 
2025-07-12 08:21:39.317537: val_loss -0.9535 
2025-07-12 08:21:39.318432: Pseudo dice [np.float32(0.957)] 
2025-07-12 08:21:39.319371: Epoch time: 68.05 s 
2025-07-12 08:21:40.196915:  
2025-07-12 08:21:40.198289: Epoch 823 
2025-07-12 08:21:40.199266: Current learning rate: 0.0021 
2025-07-12 08:22:48.182438: train_loss -0.9813 
2025-07-12 08:22:48.183615: val_loss -0.9533 
2025-07-12 08:22:48.184488: Pseudo dice [np.float32(0.9577)] 
2025-07-12 08:22:48.185469: Epoch time: 67.99 s 
2025-07-12 08:22:49.060870:  
2025-07-12 08:22:49.062118: Epoch 824 
2025-07-12 08:22:49.062999: Current learning rate: 0.00209 
2025-07-12 08:23:57.160495: train_loss -0.9813 
2025-07-12 08:23:57.161604: val_loss -0.9544 
2025-07-12 08:23:57.162733: Pseudo dice [np.float32(0.9578)] 
2025-07-12 08:23:57.163614: Epoch time: 68.1 s 
2025-07-12 08:23:58.041097:  
2025-07-12 08:23:58.042652: Epoch 825 
2025-07-12 08:23:58.043672: Current learning rate: 0.00208 
2025-07-12 08:25:06.030627: train_loss -0.9809 
2025-07-12 08:25:06.031865: val_loss -0.952 
2025-07-12 08:25:06.032924: Pseudo dice [np.float32(0.9562)] 
2025-07-12 08:25:06.033903: Epoch time: 67.99 s 
2025-07-12 08:25:06.916350:  
2025-07-12 08:25:06.917856: Epoch 826 
2025-07-12 08:25:06.918804: Current learning rate: 0.00207 
2025-07-12 08:26:14.781482: train_loss -0.9811 
2025-07-12 08:26:14.782790: val_loss -0.9523 
2025-07-12 08:26:14.783674: Pseudo dice [np.float32(0.9553)] 
2025-07-12 08:26:14.784613: Epoch time: 67.87 s 
2025-07-12 08:26:15.675092:  
2025-07-12 08:26:15.676594: Epoch 827 
2025-07-12 08:26:15.677601: Current learning rate: 0.00206 
2025-07-12 08:27:23.625164: train_loss -0.9817 
2025-07-12 08:27:23.626528: val_loss -0.9557 
2025-07-12 08:27:23.627552: Pseudo dice [np.float32(0.9593)] 
2025-07-12 08:27:23.628749: Epoch time: 67.95 s 
2025-07-12 08:27:24.514890:  
2025-07-12 08:27:24.516256: Epoch 828 
2025-07-12 08:27:24.517312: Current learning rate: 0.00205 
2025-07-12 08:28:32.589459: train_loss -0.981 
2025-07-12 08:28:32.590698: val_loss -0.9521 
2025-07-12 08:28:32.591658: Pseudo dice [np.float32(0.9561)] 
2025-07-12 08:28:32.592603: Epoch time: 68.08 s 
2025-07-12 08:28:33.471360:  
2025-07-12 08:28:33.472798: Epoch 829 
2025-07-12 08:28:33.473749: Current learning rate: 0.00204 
2025-07-12 08:29:41.399988: train_loss -0.9818 
2025-07-12 08:29:41.401103: val_loss -0.9546 
2025-07-12 08:29:41.402151: Pseudo dice [np.float32(0.9574)] 
2025-07-12 08:29:41.403055: Epoch time: 67.93 s 
2025-07-12 08:29:42.307220:  
2025-07-12 08:29:42.308674: Epoch 830 
2025-07-12 08:29:42.309628: Current learning rate: 0.00203 
2025-07-12 08:30:50.436833: train_loss -0.9818 
2025-07-12 08:30:50.438149: val_loss -0.9541 
2025-07-12 08:30:50.439111: Pseudo dice [np.float32(0.9574)] 
2025-07-12 08:30:50.440134: Epoch time: 68.13 s 
2025-07-12 08:30:50.441057: Yayy! New best EMA pseudo Dice: 0.9569000005722046 
2025-07-12 08:30:52.543624:  
2025-07-12 08:30:52.544916: Epoch 831 
2025-07-12 08:30:52.545985: Current learning rate: 0.00202 
2025-07-12 08:32:00.886514: train_loss -0.9817 
2025-07-12 08:32:00.887897: val_loss -0.9517 
2025-07-12 08:32:00.889127: Pseudo dice [np.float32(0.9555)] 
2025-07-12 08:32:00.890036: Epoch time: 68.35 s 
2025-07-12 08:32:01.778673:  
2025-07-12 08:32:01.780260: Epoch 832 
2025-07-12 08:32:01.781556: Current learning rate: 0.00201 
2025-07-12 08:33:09.908006: train_loss -0.9814 
2025-07-12 08:33:09.909097: val_loss -0.9527 
2025-07-12 08:33:09.910156: Pseudo dice [np.float32(0.9562)] 
2025-07-12 08:33:09.911278: Epoch time: 68.13 s 
2025-07-12 08:33:10.794862:  
2025-07-12 08:33:10.796179: Epoch 833 
2025-07-12 08:33:10.797187: Current learning rate: 0.002 
2025-07-12 08:34:18.983791: train_loss -0.9818 
2025-07-12 08:34:18.984995: val_loss -0.9541 
2025-07-12 08:34:18.985957: Pseudo dice [np.float32(0.9575)] 
2025-07-12 08:34:18.986912: Epoch time: 68.19 s 
2025-07-12 08:34:19.864765:  
2025-07-12 08:34:19.866032: Epoch 834 
2025-07-12 08:34:19.867077: Current learning rate: 0.00199 
2025-07-12 08:35:27.984511: train_loss -0.9825 
2025-07-12 08:35:27.985836: val_loss -0.9515 
2025-07-12 08:35:27.986974: Pseudo dice [np.float32(0.9551)] 
2025-07-12 08:35:27.987936: Epoch time: 68.12 s 
2025-07-12 08:35:29.088778:  
2025-07-12 08:35:29.090096: Epoch 835 
2025-07-12 08:35:29.091078: Current learning rate: 0.00198 
2025-07-12 08:36:37.328004: train_loss -0.9818 
2025-07-12 08:36:37.329136: val_loss -0.9532 
2025-07-12 08:36:37.330010: Pseudo dice [np.float32(0.9559)] 
2025-07-12 08:36:37.330897: Epoch time: 68.24 s 
2025-07-12 08:36:38.205510:  
2025-07-12 08:36:38.206804: Epoch 836 
2025-07-12 08:36:38.207770: Current learning rate: 0.00196 
2025-07-12 08:37:46.147871: train_loss -0.9818 
2025-07-12 08:37:46.149031: val_loss -0.9559 
2025-07-12 08:37:46.149939: Pseudo dice [np.float32(0.959)] 
2025-07-12 08:37:46.150919: Epoch time: 67.95 s 
2025-07-12 08:37:47.025217:  
2025-07-12 08:37:47.026681: Epoch 837 
2025-07-12 08:37:47.027680: Current learning rate: 0.00195 
2025-07-12 08:38:55.040271: train_loss -0.9818 
2025-07-12 08:38:55.041482: val_loss -0.9543 
2025-07-12 08:38:55.042390: Pseudo dice [np.float32(0.9573)] 
2025-07-12 08:38:55.043339: Epoch time: 68.02 s 
2025-07-12 08:38:55.925394:  
2025-07-12 08:38:55.926800: Epoch 838 
2025-07-12 08:38:55.927757: Current learning rate: 0.00194 
2025-07-12 08:40:04.003991: train_loss -0.982 
2025-07-12 08:40:04.005253: val_loss -0.9506 
2025-07-12 08:40:04.006126: Pseudo dice [np.float32(0.9541)] 
2025-07-12 08:40:04.007026: Epoch time: 68.08 s 
2025-07-12 08:40:04.884737:  
2025-07-12 08:40:04.886101: Epoch 839 
2025-07-12 08:40:04.887027: Current learning rate: 0.00193 
2025-07-12 08:41:12.794631: train_loss -0.9826 
2025-07-12 08:41:12.795816: val_loss -0.9525 
2025-07-12 08:41:12.796752: Pseudo dice [np.float32(0.9562)] 
2025-07-12 08:41:12.797843: Epoch time: 67.91 s 
2025-07-12 08:41:13.675906:  
2025-07-12 08:41:13.677096: Epoch 840 
2025-07-12 08:41:13.678174: Current learning rate: 0.00192 
2025-07-12 08:42:21.474168: train_loss -0.9818 
2025-07-12 08:42:21.475389: val_loss -0.9506 
2025-07-12 08:42:21.476275: Pseudo dice [np.float32(0.954)] 
2025-07-12 08:42:21.477481: Epoch time: 67.8 s 
2025-07-12 08:42:22.359901:  
2025-07-12 08:42:22.361205: Epoch 841 
2025-07-12 08:42:22.362229: Current learning rate: 0.00191 
2025-07-12 08:43:30.125520: train_loss -0.9819 
2025-07-12 08:43:30.126791: val_loss -0.9546 
2025-07-12 08:43:30.127686: Pseudo dice [np.float32(0.9576)] 
2025-07-12 08:43:30.128584: Epoch time: 67.77 s 
2025-07-12 08:43:31.011566:  
2025-07-12 08:43:31.012989: Epoch 842 
2025-07-12 08:43:31.014185: Current learning rate: 0.0019 
2025-07-12 08:44:39.054124: train_loss -0.9817 
2025-07-12 08:44:39.055385: val_loss -0.9508 
2025-07-12 08:44:39.056292: Pseudo dice [np.float32(0.9552)] 
2025-07-12 08:44:39.057363: Epoch time: 68.05 s 
2025-07-12 08:44:39.941511:  
2025-07-12 08:44:39.942955: Epoch 843 
2025-07-12 08:44:39.944097: Current learning rate: 0.00189 
2025-07-12 08:45:47.791103: train_loss -0.9816 
2025-07-12 08:45:47.792217: val_loss -0.952 
2025-07-12 08:45:47.793450: Pseudo dice [np.float32(0.9557)] 
2025-07-12 08:45:47.794280: Epoch time: 67.85 s 
2025-07-12 08:45:48.673668:  
2025-07-12 08:45:48.675102: Epoch 844 
2025-07-12 08:45:48.676145: Current learning rate: 0.00188 
2025-07-12 08:46:56.546666: train_loss -0.9814 
2025-07-12 08:46:56.547805: val_loss -0.9544 
2025-07-12 08:46:56.548774: Pseudo dice [np.float32(0.9575)] 
2025-07-12 08:46:56.549671: Epoch time: 67.88 s 
2025-07-12 08:46:57.426504:  
2025-07-12 08:46:57.428002: Epoch 845 
2025-07-12 08:46:57.429217: Current learning rate: 0.00187 
2025-07-12 08:48:05.440503: train_loss -0.9823 
2025-07-12 08:48:05.441728: val_loss -0.9506 
2025-07-12 08:48:05.443041: Pseudo dice [np.float32(0.9538)] 
2025-07-12 08:48:05.444010: Epoch time: 68.02 s 
2025-07-12 08:48:06.330244:  
2025-07-12 08:48:06.331918: Epoch 846 
2025-07-12 08:48:06.332995: Current learning rate: 0.00186 
2025-07-12 08:49:14.203074: train_loss -0.9815 
2025-07-12 08:49:14.204496: val_loss -0.9535 
2025-07-12 08:49:14.205359: Pseudo dice [np.float32(0.9573)] 
2025-07-12 08:49:14.206255: Epoch time: 67.88 s 
2025-07-12 08:49:15.066043:  
2025-07-12 08:49:15.067490: Epoch 847 
2025-07-12 08:49:15.068471: Current learning rate: 0.00185 
2025-07-12 08:50:22.932628: train_loss -0.982 
2025-07-12 08:50:22.933983: val_loss -0.9516 
2025-07-12 08:50:22.935183: Pseudo dice [np.float32(0.955)] 
2025-07-12 08:50:22.936491: Epoch time: 67.87 s 
2025-07-12 08:50:23.816840:  
2025-07-12 08:50:23.818368: Epoch 848 
2025-07-12 08:50:23.819415: Current learning rate: 0.00184 
2025-07-12 08:51:31.705513: train_loss -0.9821 
2025-07-12 08:51:31.706758: val_loss -0.9513 
2025-07-12 08:51:31.707680: Pseudo dice [np.float32(0.9552)] 
2025-07-12 08:51:31.708701: Epoch time: 67.89 s 
2025-07-12 08:51:32.590193:  
2025-07-12 08:51:32.591681: Epoch 849 
2025-07-12 08:51:32.592660: Current learning rate: 0.00182 
2025-07-12 08:52:40.685625: train_loss -0.9816 
2025-07-12 08:52:40.686721: val_loss -0.9546 
2025-07-12 08:52:40.687666: Pseudo dice [np.float32(0.9579)] 
2025-07-12 08:52:40.688674: Epoch time: 68.1 s 
2025-07-12 08:52:42.665967:  
2025-07-12 08:52:42.667256: Epoch 850 
2025-07-12 08:52:42.668251: Current learning rate: 0.00181 
2025-07-12 08:53:50.505517: train_loss -0.9812 
2025-07-12 08:53:50.507048: val_loss -0.9551 
2025-07-12 08:53:50.508102: Pseudo dice [np.float32(0.9588)] 
2025-07-12 08:53:50.509127: Epoch time: 67.84 s 
2025-07-12 08:53:51.373357:  
2025-07-12 08:53:51.374919: Epoch 851 
2025-07-12 08:53:51.375971: Current learning rate: 0.0018 
2025-07-12 08:54:59.307940: train_loss -0.9815 
2025-07-12 08:54:59.309389: val_loss -0.9534 
2025-07-12 08:54:59.310436: Pseudo dice [np.float32(0.9565)] 
2025-07-12 08:54:59.311459: Epoch time: 67.94 s 
2025-07-12 08:55:00.180400:  
2025-07-12 08:55:00.181889: Epoch 852 
2025-07-12 08:55:00.182931: Current learning rate: 0.00179 
2025-07-12 08:56:08.223963: train_loss -0.9817 
2025-07-12 08:56:08.225891: val_loss -0.9551 
2025-07-12 08:56:08.227024: Pseudo dice [np.float32(0.9591)] 
2025-07-12 08:56:08.228117: Epoch time: 68.05 s 
2025-07-12 08:56:09.098737:  
2025-07-12 08:56:09.100199: Epoch 853 
2025-07-12 08:56:09.101190: Current learning rate: 0.00178 
2025-07-12 08:57:17.070045: train_loss -0.9816 
2025-07-12 08:57:17.071153: val_loss -0.9538 
2025-07-12 08:57:17.072049: Pseudo dice [np.float32(0.9579)] 
2025-07-12 08:57:17.072998: Epoch time: 67.97 s 
2025-07-12 08:57:17.937693:  
2025-07-12 08:57:17.939026: Epoch 854 
2025-07-12 08:57:17.940010: Current learning rate: 0.00177 
2025-07-12 08:58:25.819558: train_loss -0.9807 
2025-07-12 08:58:25.820639: val_loss -0.9479 
2025-07-12 08:58:25.821604: Pseudo dice [np.float32(0.9517)] 
2025-07-12 08:58:25.822594: Epoch time: 67.89 s 
2025-07-12 08:58:26.689193:  
2025-07-12 08:58:26.690762: Epoch 855 
2025-07-12 08:58:26.691736: Current learning rate: 0.00176 
2025-07-12 08:59:34.605078: train_loss -0.9811 
2025-07-12 08:59:34.606307: val_loss -0.9493 
2025-07-12 08:59:34.607758: Pseudo dice [np.float32(0.9532)] 
2025-07-12 08:59:34.608670: Epoch time: 67.92 s 
2025-07-12 08:59:35.715648:  
2025-07-12 08:59:35.717308: Epoch 856 
2025-07-12 08:59:35.718310: Current learning rate: 0.00175 
2025-07-12 09:00:43.716650: train_loss -0.9816 
2025-07-12 09:00:43.717970: val_loss -0.9548 
2025-07-12 09:00:43.719037: Pseudo dice [np.float32(0.9575)] 
2025-07-12 09:00:43.719946: Epoch time: 68.0 s 
2025-07-12 09:00:44.586895:  
2025-07-12 09:00:44.588179: Epoch 857 
2025-07-12 09:00:44.589180: Current learning rate: 0.00174 
2025-07-12 09:01:52.554675: train_loss -0.9814 
2025-07-12 09:01:52.556030: val_loss -0.9556 
2025-07-12 09:01:52.557143: Pseudo dice [np.float32(0.9583)] 
2025-07-12 09:01:52.558166: Epoch time: 67.97 s 
2025-07-12 09:01:53.431807:  
2025-07-12 09:01:53.433549: Epoch 858 
2025-07-12 09:01:53.434659: Current learning rate: 0.00173 
2025-07-12 09:03:01.643445: train_loss -0.9816 
2025-07-12 09:03:01.644713: val_loss -0.95 
2025-07-12 09:03:01.646091: Pseudo dice [np.float32(0.9537)] 
2025-07-12 09:03:01.647098: Epoch time: 68.22 s 
2025-07-12 09:03:02.604024:  
2025-07-12 09:03:02.605390: Epoch 859 
2025-07-12 09:03:02.606349: Current learning rate: 0.00172 
2025-07-12 09:04:10.674672: train_loss -0.9819 
2025-07-12 09:04:10.675735: val_loss -0.9526 
2025-07-12 09:04:10.676588: Pseudo dice [np.float32(0.9559)] 
2025-07-12 09:04:10.677485: Epoch time: 68.07 s 
2025-07-12 09:04:11.545493:  
2025-07-12 09:04:11.547071: Epoch 860 
2025-07-12 09:04:11.547997: Current learning rate: 0.0017 
2025-07-12 09:05:19.481277: train_loss -0.9813 
2025-07-12 09:05:19.482390: val_loss -0.9539 
2025-07-12 09:05:19.483303: Pseudo dice [np.float32(0.9569)] 
2025-07-12 09:05:19.484269: Epoch time: 67.94 s 
2025-07-12 09:05:20.353142:  
2025-07-12 09:05:20.354473: Epoch 861 
2025-07-12 09:05:20.355366: Current learning rate: 0.00169 
2025-07-12 09:06:28.274749: train_loss -0.9812 
2025-07-12 09:06:28.275884: val_loss -0.956 
2025-07-12 09:06:28.276899: Pseudo dice [np.float32(0.9593)] 
2025-07-12 09:06:28.277966: Epoch time: 67.93 s 
2025-07-12 09:06:29.145373:  
2025-07-12 09:06:29.146565: Epoch 862 
2025-07-12 09:06:29.147539: Current learning rate: 0.00168 
2025-07-12 09:07:37.078870: train_loss -0.9815 
2025-07-12 09:07:37.080094: val_loss -0.9536 
2025-07-12 09:07:37.081074: Pseudo dice [np.float32(0.9573)] 
2025-07-12 09:07:37.082055: Epoch time: 67.94 s 
2025-07-12 09:07:37.959158:  
2025-07-12 09:07:37.960387: Epoch 863 
2025-07-12 09:07:37.961302: Current learning rate: 0.00167 
2025-07-12 09:08:46.035015: train_loss -0.9811 
2025-07-12 09:08:46.036207: val_loss -0.955 
2025-07-12 09:08:46.037215: Pseudo dice [np.float32(0.9583)] 
2025-07-12 09:08:46.038193: Epoch time: 68.08 s 
2025-07-12 09:08:46.911391:  
2025-07-12 09:08:46.913005: Epoch 864 
2025-07-12 09:08:46.914056: Current learning rate: 0.00166 
2025-07-12 09:09:54.835203: train_loss -0.9822 
2025-07-12 09:09:54.836455: val_loss -0.9543 
2025-07-12 09:09:54.837698: Pseudo dice [np.float32(0.9581)] 
2025-07-12 09:09:54.838637: Epoch time: 67.93 s 
2025-07-12 09:09:55.720417:  
2025-07-12 09:09:55.721944: Epoch 865 
2025-07-12 09:09:55.723136: Current learning rate: 0.00165 
2025-07-12 09:11:03.755290: train_loss -0.9818 
2025-07-12 09:11:03.756547: val_loss -0.9523 
2025-07-12 09:11:03.757723: Pseudo dice [np.float32(0.9555)] 
2025-07-12 09:11:03.758611: Epoch time: 68.04 s 
2025-07-12 09:11:04.640798:  
2025-07-12 09:11:04.642266: Epoch 866 
2025-07-12 09:11:04.643212: Current learning rate: 0.00164 
2025-07-12 09:12:12.968280: train_loss -0.9824 
2025-07-12 09:12:12.969581: val_loss -0.9533 
2025-07-12 09:12:12.970600: Pseudo dice [np.float32(0.9569)] 
2025-07-12 09:12:12.971525: Epoch time: 68.33 s 
2025-07-12 09:12:13.848639:  
2025-07-12 09:12:13.850042: Epoch 867 
2025-07-12 09:12:13.851205: Current learning rate: 0.00163 
2025-07-12 09:13:21.842797: train_loss -0.9824 
2025-07-12 09:13:21.843949: val_loss -0.9523 
2025-07-12 09:13:21.844852: Pseudo dice [np.float32(0.9555)] 
2025-07-12 09:13:21.845822: Epoch time: 68.0 s 
2025-07-12 09:13:22.726367:  
2025-07-12 09:13:22.727581: Epoch 868 
2025-07-12 09:13:22.728595: Current learning rate: 0.00162 
2025-07-12 09:14:30.663887: train_loss -0.9822 
2025-07-12 09:14:30.665162: val_loss -0.954 
2025-07-12 09:14:30.666039: Pseudo dice [np.float32(0.9576)] 
2025-07-12 09:14:30.666996: Epoch time: 67.94 s 
2025-07-12 09:14:31.539011:  
2025-07-12 09:14:31.540538: Epoch 869 
2025-07-12 09:14:31.541478: Current learning rate: 0.00161 
2025-07-12 09:15:39.483076: train_loss -0.9825 
2025-07-12 09:15:39.484202: val_loss -0.9517 
2025-07-12 09:15:39.485063: Pseudo dice [np.float32(0.9554)] 
2025-07-12 09:15:39.485962: Epoch time: 67.95 s 
2025-07-12 09:15:40.349166:  
2025-07-12 09:15:40.350451: Epoch 870 
2025-07-12 09:15:40.351482: Current learning rate: 0.00159 
2025-07-12 09:16:48.415650: train_loss -0.9828 
2025-07-12 09:16:48.416670: val_loss -0.9533 
2025-07-12 09:16:48.417579: Pseudo dice [np.float32(0.9568)] 
2025-07-12 09:16:48.418489: Epoch time: 68.07 s 
2025-07-12 09:16:49.285285:  
2025-07-12 09:16:49.286705: Epoch 871 
2025-07-12 09:16:49.287677: Current learning rate: 0.00158 
2025-07-12 09:17:57.209212: train_loss -0.9814 
2025-07-12 09:17:57.210312: val_loss -0.9514 
2025-07-12 09:17:57.211553: Pseudo dice [np.float32(0.9544)] 
2025-07-12 09:17:57.212610: Epoch time: 67.93 s 
2025-07-12 09:17:58.092840:  
2025-07-12 09:17:58.094286: Epoch 872 
2025-07-12 09:17:58.095223: Current learning rate: 0.00157 
2025-07-12 09:19:06.035170: train_loss -0.9817 
2025-07-12 09:19:06.036488: val_loss -0.9531 
2025-07-12 09:19:06.037458: Pseudo dice [np.float32(0.9567)] 
2025-07-12 09:19:06.038506: Epoch time: 67.95 s 
2025-07-12 09:19:06.903444:  
2025-07-12 09:19:06.904740: Epoch 873 
2025-07-12 09:19:06.905679: Current learning rate: 0.00156 
2025-07-12 09:20:14.957110: train_loss -0.9822 
2025-07-12 09:20:14.958601: val_loss -0.9482 
2025-07-12 09:20:14.959476: Pseudo dice [np.float32(0.9524)] 
2025-07-12 09:20:14.960501: Epoch time: 68.06 s 
2025-07-12 09:20:15.836699:  
2025-07-12 09:20:15.838141: Epoch 874 
2025-07-12 09:20:15.839269: Current learning rate: 0.00155 
2025-07-12 09:21:23.765164: train_loss -0.9818 
2025-07-12 09:21:23.766345: val_loss -0.9504 
2025-07-12 09:21:23.767554: Pseudo dice [np.float32(0.9538)] 
2025-07-12 09:21:23.768494: Epoch time: 67.93 s 
2025-07-12 09:21:24.646742:  
2025-07-12 09:21:24.648457: Epoch 875 
2025-07-12 09:21:24.649721: Current learning rate: 0.00154 
2025-07-12 09:22:32.530758: train_loss -0.9827 
2025-07-12 09:22:32.531922: val_loss -0.953 
2025-07-12 09:22:32.532870: Pseudo dice [np.float32(0.9566)] 
2025-07-12 09:22:32.533811: Epoch time: 67.89 s 
2025-07-12 09:22:33.398035:  
2025-07-12 09:22:33.399252: Epoch 876 
2025-07-12 09:22:33.400174: Current learning rate: 0.00153 
2025-07-12 09:23:41.375828: train_loss -0.9824 
2025-07-12 09:23:41.376955: val_loss -0.9535 
2025-07-12 09:23:41.377863: Pseudo dice [np.float32(0.9561)] 
2025-07-12 09:23:41.378809: Epoch time: 67.98 s 
2025-07-12 09:23:42.248262:  
2025-07-12 09:23:42.249606: Epoch 877 
2025-07-12 09:23:42.250576: Current learning rate: 0.00152 
2025-07-12 09:24:50.254450: train_loss -0.9818 
2025-07-12 09:24:50.255724: val_loss -0.9513 
2025-07-12 09:24:50.256744: Pseudo dice [np.float32(0.9551)] 
2025-07-12 09:24:50.257696: Epoch time: 68.01 s 
2025-07-12 09:24:51.126440:  
2025-07-12 09:24:51.127840: Epoch 878 
2025-07-12 09:24:51.128839: Current learning rate: 0.00151 
2025-07-12 09:25:59.007292: train_loss -0.9826 
2025-07-12 09:25:59.008496: val_loss -0.9521 
2025-07-12 09:25:59.009707: Pseudo dice [np.float32(0.9553)] 
2025-07-12 09:25:59.010901: Epoch time: 67.88 s 
2025-07-12 09:25:59.881109:  
2025-07-12 09:25:59.882522: Epoch 879 
2025-07-12 09:25:59.883691: Current learning rate: 0.00149 
2025-07-12 09:27:07.737611: train_loss -0.982 
2025-07-12 09:27:07.738687: val_loss -0.9516 
2025-07-12 09:27:07.739573: Pseudo dice [np.float32(0.9546)] 
2025-07-12 09:27:07.740561: Epoch time: 67.86 s 
2025-07-12 09:27:08.607835:  
2025-07-12 09:27:08.609370: Epoch 880 
2025-07-12 09:27:08.610402: Current learning rate: 0.00148 
2025-07-12 09:28:16.446171: train_loss -0.9817 
2025-07-12 09:28:16.447524: val_loss -0.9524 
2025-07-12 09:28:16.448501: Pseudo dice [np.float32(0.956)] 
2025-07-12 09:28:16.449533: Epoch time: 67.84 s 
2025-07-12 09:28:17.544286:  
2025-07-12 09:28:17.545881: Epoch 881 
2025-07-12 09:28:17.546828: Current learning rate: 0.00147 
2025-07-12 09:29:25.428534: train_loss -0.9827 
2025-07-12 09:29:25.429673: val_loss -0.9504 
2025-07-12 09:29:25.430673: Pseudo dice [np.float32(0.9532)] 
2025-07-12 09:29:25.431831: Epoch time: 67.89 s 
2025-07-12 09:29:26.307495:  
2025-07-12 09:29:26.309083: Epoch 882 
2025-07-12 09:29:26.310092: Current learning rate: 0.00146 
2025-07-12 09:30:34.216012: train_loss -0.9832 
2025-07-12 09:30:34.217064: val_loss -0.9521 
2025-07-12 09:30:34.218050: Pseudo dice [np.float32(0.9556)] 
2025-07-12 09:30:34.219026: Epoch time: 67.91 s 
2025-07-12 09:30:35.103264:  
2025-07-12 09:30:35.104705: Epoch 883 
2025-07-12 09:30:35.105649: Current learning rate: 0.00145 
2025-07-12 09:31:42.897604: train_loss -0.9825 
2025-07-12 09:31:42.898790: val_loss -0.9514 
2025-07-12 09:31:42.899688: Pseudo dice [np.float32(0.9552)] 
2025-07-12 09:31:42.900679: Epoch time: 67.8 s 
2025-07-12 09:31:43.774031:  
2025-07-12 09:31:43.775509: Epoch 884 
2025-07-12 09:31:43.776682: Current learning rate: 0.00144 
2025-07-12 09:32:51.749669: train_loss -0.9826 
2025-07-12 09:32:51.750946: val_loss -0.9545 
2025-07-12 09:32:51.751903: Pseudo dice [np.float32(0.9579)] 
2025-07-12 09:32:51.752827: Epoch time: 67.98 s 
2025-07-12 09:32:52.636237:  
2025-07-12 09:32:52.637737: Epoch 885 
2025-07-12 09:32:52.638777: Current learning rate: 0.00143 
2025-07-12 09:34:00.470319: train_loss -0.982 
2025-07-12 09:34:00.471470: val_loss -0.9522 
2025-07-12 09:34:00.472511: Pseudo dice [np.float32(0.9559)] 
2025-07-12 09:34:00.473818: Epoch time: 67.84 s 
2025-07-12 09:34:01.342303:  
2025-07-12 09:34:01.343662: Epoch 886 
2025-07-12 09:34:01.344736: Current learning rate: 0.00142 
2025-07-12 09:35:09.235306: train_loss -0.9824 
2025-07-12 09:35:09.236768: val_loss -0.9555 
2025-07-12 09:35:09.237698: Pseudo dice [np.float32(0.9591)] 
2025-07-12 09:35:09.238850: Epoch time: 67.9 s 
2025-07-12 09:35:10.107178:  
2025-07-12 09:35:10.108489: Epoch 887 
2025-07-12 09:35:10.109478: Current learning rate: 0.00141 
2025-07-12 09:36:18.036966: train_loss -0.9827 
2025-07-12 09:36:18.038215: val_loss -0.9522 
2025-07-12 09:36:18.039099: Pseudo dice [np.float32(0.9558)] 
2025-07-12 09:36:18.040158: Epoch time: 67.93 s 
2025-07-12 09:36:18.913773:  
2025-07-12 09:36:18.915028: Epoch 888 
2025-07-12 09:36:18.916066: Current learning rate: 0.00139 
2025-07-12 09:37:26.990535: train_loss -0.9831 
2025-07-12 09:37:26.991830: val_loss -0.9532 
2025-07-12 09:37:26.992749: Pseudo dice [np.float32(0.9564)] 
2025-07-12 09:37:26.993752: Epoch time: 68.08 s 
2025-07-12 09:37:27.846475:  
2025-07-12 09:37:27.847786: Epoch 889 
2025-07-12 09:37:27.848651: Current learning rate: 0.00138 
2025-07-12 09:38:35.888755: train_loss -0.982 
2025-07-12 09:38:35.889844: val_loss -0.9523 
2025-07-12 09:38:35.890943: Pseudo dice [np.float32(0.9561)] 
2025-07-12 09:38:35.891943: Epoch time: 68.05 s 
2025-07-12 09:38:36.760760:  
2025-07-12 09:38:36.762424: Epoch 890 
2025-07-12 09:38:36.763484: Current learning rate: 0.00137 
2025-07-12 09:39:44.695652: train_loss -0.9827 
2025-07-12 09:39:44.696720: val_loss -0.9535 
2025-07-12 09:39:44.697594: Pseudo dice [np.float32(0.9573)] 
2025-07-12 09:39:44.698680: Epoch time: 67.94 s 
2025-07-12 09:39:45.574511:  
2025-07-12 09:39:45.575978: Epoch 891 
2025-07-12 09:39:45.576974: Current learning rate: 0.00136 
2025-07-12 09:40:53.645318: train_loss -0.9827 
2025-07-12 09:40:53.646639: val_loss -0.9535 
2025-07-12 09:40:53.647633: Pseudo dice [np.float32(0.9565)] 
2025-07-12 09:40:53.648624: Epoch time: 68.07 s 
2025-07-12 09:40:54.522355:  
2025-07-12 09:40:54.523759: Epoch 892 
2025-07-12 09:40:54.524806: Current learning rate: 0.00135 
2025-07-12 09:42:02.464129: train_loss -0.9824 
2025-07-12 09:42:02.465253: val_loss -0.9502 
2025-07-12 09:42:02.466177: Pseudo dice [np.float32(0.9537)] 
2025-07-12 09:42:02.467365: Epoch time: 67.95 s 
2025-07-12 09:42:03.349096:  
2025-07-12 09:42:03.350404: Epoch 893 
2025-07-12 09:42:03.351428: Current learning rate: 0.00134 
2025-07-12 09:43:11.247739: train_loss -0.9819 
2025-07-12 09:43:11.248878: val_loss -0.9523 
2025-07-12 09:43:11.249842: Pseudo dice [np.float32(0.956)] 
2025-07-12 09:43:11.250788: Epoch time: 67.9 s 
2025-07-12 09:43:12.122920:  
2025-07-12 09:43:12.124448: Epoch 894 
2025-07-12 09:43:12.125475: Current learning rate: 0.00133 
2025-07-12 09:44:20.100965: train_loss -0.9824 
2025-07-12 09:44:20.102331: val_loss -0.9526 
2025-07-12 09:44:20.103329: Pseudo dice [np.float32(0.9557)] 
2025-07-12 09:44:20.104190: Epoch time: 67.98 s 
2025-07-12 09:44:20.974764:  
2025-07-12 09:44:20.976161: Epoch 895 
2025-07-12 09:44:20.977147: Current learning rate: 0.00132 
2025-07-12 09:45:29.096162: train_loss -0.9832 
2025-07-12 09:45:29.097308: val_loss -0.9502 
2025-07-12 09:45:29.098397: Pseudo dice [np.float32(0.954)] 
2025-07-12 09:45:29.099271: Epoch time: 68.13 s 
2025-07-12 09:45:29.973113:  
2025-07-12 09:45:29.974393: Epoch 896 
2025-07-12 09:45:29.975399: Current learning rate: 0.0013 
2025-07-12 09:46:37.897674: train_loss -0.9823 
2025-07-12 09:46:37.899186: val_loss -0.9523 
2025-07-12 09:46:37.900265: Pseudo dice [np.float32(0.9558)] 
2025-07-12 09:46:37.901292: Epoch time: 67.93 s 
2025-07-12 09:46:38.774296:  
2025-07-12 09:46:38.775616: Epoch 897 
2025-07-12 09:46:38.776595: Current learning rate: 0.00129 
2025-07-12 09:47:46.765321: train_loss -0.9825 
2025-07-12 09:47:46.766641: val_loss -0.9524 
2025-07-12 09:47:46.767743: Pseudo dice [np.float32(0.9558)] 
2025-07-12 09:47:46.768606: Epoch time: 67.99 s 
2025-07-12 09:47:47.640279:  
2025-07-12 09:47:47.641781: Epoch 898 
2025-07-12 09:47:47.642899: Current learning rate: 0.00128 
2025-07-12 09:48:55.604633: train_loss -0.9822 
2025-07-12 09:48:55.605825: val_loss -0.9535 
2025-07-12 09:48:55.606764: Pseudo dice [np.float32(0.9569)] 
2025-07-12 09:48:55.607612: Epoch time: 67.97 s 
2025-07-12 09:48:56.473716:  
2025-07-12 09:48:56.474959: Epoch 899 
2025-07-12 09:48:56.475968: Current learning rate: 0.00127 
2025-07-12 09:50:04.304323: train_loss -0.9826 
2025-07-12 09:50:04.305532: val_loss -0.9505 
2025-07-12 09:50:04.306571: Pseudo dice [np.float32(0.9547)] 
2025-07-12 09:50:04.307594: Epoch time: 67.83 s 
2025-07-12 09:50:06.273509:  
2025-07-12 09:50:06.275005: Epoch 900 
2025-07-12 09:50:06.275944: Current learning rate: 0.00126 
2025-07-12 09:51:14.124827: train_loss -0.9819 
2025-07-12 09:51:14.125940: val_loss -0.9514 
2025-07-12 09:51:14.126889: Pseudo dice [np.float32(0.955)] 
2025-07-12 09:51:14.127904: Epoch time: 67.85 s 
2025-07-12 09:51:14.994842:  
2025-07-12 09:51:14.996177: Epoch 901 
2025-07-12 09:51:14.997142: Current learning rate: 0.00125 
2025-07-12 09:52:22.845884: train_loss -0.9823 
2025-07-12 09:52:22.847056: val_loss -0.9518 
2025-07-12 09:52:22.848054: Pseudo dice [np.float32(0.9554)] 
2025-07-12 09:52:22.848943: Epoch time: 67.85 s 
2025-07-12 09:52:23.950284:  
2025-07-12 09:52:23.951633: Epoch 902 
2025-07-12 09:52:23.952616: Current learning rate: 0.00124 
2025-07-12 09:53:31.925689: train_loss -0.983 
2025-07-12 09:53:31.926768: val_loss -0.9503 
2025-07-12 09:53:31.927725: Pseudo dice [np.float32(0.9541)] 
2025-07-12 09:53:31.928659: Epoch time: 67.98 s 
2025-07-12 09:53:32.803920:  
2025-07-12 09:53:32.805522: Epoch 903 
2025-07-12 09:53:32.806581: Current learning rate: 0.00122 
2025-07-12 09:54:40.826816: train_loss -0.9832 
2025-07-12 09:54:40.828001: val_loss -0.9513 
2025-07-12 09:54:40.828924: Pseudo dice [np.float32(0.9549)] 
2025-07-12 09:54:40.829866: Epoch time: 68.03 s 
2025-07-12 09:54:41.712045:  
2025-07-12 09:54:41.713445: Epoch 904 
2025-07-12 09:54:41.714513: Current learning rate: 0.00121 
2025-07-12 09:55:49.723979: train_loss -0.9825 
2025-07-12 09:55:49.725258: val_loss -0.9525 
2025-07-12 09:55:49.726394: Pseudo dice [np.float32(0.9559)] 
2025-07-12 09:55:49.727395: Epoch time: 68.02 s 
2025-07-12 09:55:50.597479:  
2025-07-12 09:55:50.598942: Epoch 905 
2025-07-12 09:55:50.600019: Current learning rate: 0.0012 
2025-07-12 09:56:58.878229: train_loss -0.983 
2025-07-12 09:56:58.879373: val_loss -0.9499 
2025-07-12 09:56:58.880414: Pseudo dice [np.float32(0.953)] 
2025-07-12 09:56:58.881414: Epoch time: 68.28 s 
2025-07-12 09:56:59.753651:  
2025-07-12 09:56:59.755081: Epoch 906 
2025-07-12 09:56:59.756100: Current learning rate: 0.00119 
2025-07-12 09:58:07.846757: train_loss -0.9829 
2025-07-12 09:58:07.847833: val_loss -0.9502 
2025-07-12 09:58:07.848681: Pseudo dice [np.float32(0.9544)] 
2025-07-12 09:58:07.849945: Epoch time: 68.1 s 
2025-07-12 09:58:08.725049:  
2025-07-12 09:58:08.726397: Epoch 907 
2025-07-12 09:58:08.727420: Current learning rate: 0.00118 
2025-07-12 09:59:16.755423: train_loss -0.9828 
2025-07-12 09:59:16.756495: val_loss -0.9543 
2025-07-12 09:59:16.758190: Pseudo dice [np.float32(0.9577)] 
2025-07-12 09:59:16.759129: Epoch time: 68.03 s 
2025-07-12 09:59:17.632853:  
2025-07-12 09:59:17.634081: Epoch 908 
2025-07-12 09:59:17.635025: Current learning rate: 0.00117 
2025-07-12 10:00:25.587551: train_loss -0.9827 
2025-07-12 10:00:25.588667: val_loss -0.9545 
2025-07-12 10:00:25.589557: Pseudo dice [np.float32(0.9574)] 
2025-07-12 10:00:25.590539: Epoch time: 67.96 s 
2025-07-12 10:00:26.446372:  
2025-07-12 10:00:26.447945: Epoch 909 
2025-07-12 10:00:26.449044: Current learning rate: 0.00116 
2025-07-12 10:01:34.545060: train_loss -0.9826 
2025-07-12 10:01:34.546901: val_loss -0.9513 
2025-07-12 10:01:34.547831: Pseudo dice [np.float32(0.9548)] 
2025-07-12 10:01:34.548646: Epoch time: 68.1 s 
2025-07-12 10:01:35.425024:  
2025-07-12 10:01:35.426461: Epoch 910 
2025-07-12 10:01:35.427480: Current learning rate: 0.00115 
2025-07-12 10:02:43.404205: train_loss -0.9828 
2025-07-12 10:02:43.405439: val_loss -0.9506 
2025-07-12 10:02:43.406467: Pseudo dice [np.float32(0.9538)] 
2025-07-12 10:02:43.407514: Epoch time: 67.98 s 
2025-07-12 10:02:44.276940:  
2025-07-12 10:02:44.278310: Epoch 911 
2025-07-12 10:02:44.279428: Current learning rate: 0.00113 
2025-07-12 10:03:52.259345: train_loss -0.9832 
2025-07-12 10:03:52.260487: val_loss -0.9519 
2025-07-12 10:03:52.261487: Pseudo dice [np.float32(0.9554)] 
2025-07-12 10:03:52.262440: Epoch time: 67.99 s 
2025-07-12 10:03:53.130626:  
2025-07-12 10:03:53.131986: Epoch 912 
2025-07-12 10:03:53.132946: Current learning rate: 0.00112 
2025-07-12 10:05:01.348040: train_loss -0.9828 
2025-07-12 10:05:01.349170: val_loss -0.9525 
2025-07-12 10:05:01.350090: Pseudo dice [np.float32(0.956)] 
2025-07-12 10:05:01.350923: Epoch time: 68.22 s 
2025-07-12 10:05:02.219348:  
2025-07-12 10:05:02.220813: Epoch 913 
2025-07-12 10:05:02.221842: Current learning rate: 0.00111 
2025-07-12 10:06:10.320944: train_loss -0.9825 
2025-07-12 10:06:10.322113: val_loss -0.9544 
2025-07-12 10:06:10.323057: Pseudo dice [np.float32(0.9572)] 
2025-07-12 10:06:10.324040: Epoch time: 68.11 s 
2025-07-12 10:06:11.195426:  
2025-07-12 10:06:11.196668: Epoch 914 
2025-07-12 10:06:11.197585: Current learning rate: 0.0011 
2025-07-12 10:07:19.258803: train_loss -0.9832 
2025-07-12 10:07:19.260024: val_loss -0.9546 
2025-07-12 10:07:19.260912: Pseudo dice [np.float32(0.9579)] 
2025-07-12 10:07:19.261875: Epoch time: 68.07 s 
2025-07-12 10:07:20.130250:  
2025-07-12 10:07:20.131581: Epoch 915 
2025-07-12 10:07:20.132496: Current learning rate: 0.00109 
2025-07-12 10:08:28.133013: train_loss -0.9831 
2025-07-12 10:08:28.134227: val_loss -0.9516 
2025-07-12 10:08:28.135098: Pseudo dice [np.float32(0.9552)] 
2025-07-12 10:08:28.136172: Epoch time: 68.01 s 
2025-07-12 10:08:29.014732:  
2025-07-12 10:08:29.016161: Epoch 916 
2025-07-12 10:08:29.017092: Current learning rate: 0.00108 
2025-07-12 10:09:37.184469: train_loss -0.983 
2025-07-12 10:09:37.185653: val_loss -0.9521 
2025-07-12 10:09:37.186566: Pseudo dice [np.float32(0.955)] 
2025-07-12 10:09:37.187709: Epoch time: 68.17 s 
2025-07-12 10:09:38.066258:  
2025-07-12 10:09:38.067435: Epoch 917 
2025-07-12 10:09:38.068481: Current learning rate: 0.00106 
2025-07-12 10:10:46.044784: train_loss -0.9837 
2025-07-12 10:10:46.045939: val_loss -0.9525 
2025-07-12 10:10:46.047147: Pseudo dice [np.float32(0.9556)] 
2025-07-12 10:10:46.048052: Epoch time: 67.98 s 
2025-07-12 10:10:46.925869:  
2025-07-12 10:10:46.927099: Epoch 918 
2025-07-12 10:10:46.928234: Current learning rate: 0.00105 
2025-07-12 10:11:54.868379: train_loss -0.9832 
2025-07-12 10:11:54.869525: val_loss -0.9507 
2025-07-12 10:11:54.870434: Pseudo dice [np.float32(0.9542)] 
2025-07-12 10:11:54.871565: Epoch time: 67.95 s 
2025-07-12 10:11:55.740214:  
2025-07-12 10:11:55.741919: Epoch 919 
2025-07-12 10:11:55.742947: Current learning rate: 0.00104 
2025-07-12 10:13:03.676759: train_loss -0.9832 
2025-07-12 10:13:03.677879: val_loss -0.9512 
2025-07-12 10:13:03.679051: Pseudo dice [np.float32(0.9543)] 
2025-07-12 10:13:03.680336: Epoch time: 67.94 s 
2025-07-12 10:13:04.774525:  
2025-07-12 10:13:04.775943: Epoch 920 
2025-07-12 10:13:04.776988: Current learning rate: 0.00103 
2025-07-12 10:14:12.523747: train_loss -0.9829 
2025-07-12 10:14:12.524872: val_loss -0.9503 
2025-07-12 10:14:12.525824: Pseudo dice [np.float32(0.9542)] 
2025-07-12 10:14:12.526799: Epoch time: 67.75 s 
2025-07-12 10:14:13.413666:  
2025-07-12 10:14:13.415258: Epoch 921 
2025-07-12 10:14:13.416324: Current learning rate: 0.00102 
2025-07-12 10:15:21.155155: train_loss -0.9832 
2025-07-12 10:15:21.156664: val_loss -0.9534 
2025-07-12 10:15:21.157779: Pseudo dice [np.float32(0.9566)] 
2025-07-12 10:15:21.158783: Epoch time: 67.75 s 
2025-07-12 10:15:22.026802:  
2025-07-12 10:15:22.028513: Epoch 922 
2025-07-12 10:15:22.029533: Current learning rate: 0.00101 
2025-07-12 10:16:29.756075: train_loss -0.9832 
2025-07-12 10:16:29.757639: val_loss -0.952 
2025-07-12 10:16:29.758671: Pseudo dice [np.float32(0.9554)] 
2025-07-12 10:16:29.759609: Epoch time: 67.73 s 
2025-07-12 10:16:30.627378:  
2025-07-12 10:16:30.628937: Epoch 923 
2025-07-12 10:16:30.629948: Current learning rate: 0.001 
2025-07-12 10:17:38.491093: train_loss -0.983 
2025-07-12 10:17:38.492213: val_loss -0.9513 
2025-07-12 10:17:38.493571: Pseudo dice [np.float32(0.9552)] 
2025-07-12 10:17:38.494504: Epoch time: 67.87 s 
2025-07-12 10:17:39.365197:  
2025-07-12 10:17:39.366515: Epoch 924 
2025-07-12 10:17:39.367483: Current learning rate: 0.00098 
2025-07-12 10:18:47.326058: train_loss -0.9831 
2025-07-12 10:18:47.327199: val_loss -0.9517 
2025-07-12 10:18:47.328112: Pseudo dice [np.float32(0.954)] 
2025-07-12 10:18:47.329215: Epoch time: 67.96 s 
2025-07-12 10:18:48.197126:  
2025-07-12 10:18:48.198825: Epoch 925 
2025-07-12 10:18:48.199740: Current learning rate: 0.00097 
2025-07-12 10:19:56.195518: train_loss -0.9831 
2025-07-12 10:19:56.196824: val_loss -0.9524 
2025-07-12 10:19:56.197829: Pseudo dice [np.float32(0.9553)] 
2025-07-12 10:19:56.198880: Epoch time: 68.0 s 
2025-07-12 10:19:57.072092:  
2025-07-12 10:19:57.073534: Epoch 926 
2025-07-12 10:19:57.074556: Current learning rate: 0.00096 
2025-07-12 10:21:04.974031: train_loss -0.9835 
2025-07-12 10:21:04.975164: val_loss -0.9523 
2025-07-12 10:21:04.976064: Pseudo dice [np.float32(0.9551)] 
2025-07-12 10:21:04.977119: Epoch time: 67.91 s 
2025-07-12 10:21:05.849546:  
2025-07-12 10:21:05.851112: Epoch 927 
2025-07-12 10:21:05.852116: Current learning rate: 0.00095 
2025-07-12 10:22:13.940669: train_loss -0.9833 
2025-07-12 10:22:13.942006: val_loss -0.9511 
2025-07-12 10:22:13.942902: Pseudo dice [np.float32(0.9543)] 
2025-07-12 10:22:13.943945: Epoch time: 68.09 s 
2025-07-12 10:22:14.808939:  
2025-07-12 10:22:14.810460: Epoch 928 
2025-07-12 10:22:14.811367: Current learning rate: 0.00094 
2025-07-12 10:23:22.829762: train_loss -0.9832 
2025-07-12 10:23:22.830925: val_loss -0.9504 
2025-07-12 10:23:22.832036: Pseudo dice [np.float32(0.9537)] 
2025-07-12 10:23:22.833080: Epoch time: 68.02 s 
2025-07-12 10:23:23.703755:  
2025-07-12 10:23:23.705272: Epoch 929 
2025-07-12 10:23:23.706359: Current learning rate: 0.00092 
2025-07-12 10:24:31.495318: train_loss -0.9838 
2025-07-12 10:24:31.496618: val_loss -0.9527 
2025-07-12 10:24:31.498045: Pseudo dice [np.float32(0.9557)] 
2025-07-12 10:24:31.499327: Epoch time: 67.8 s 
2025-07-12 10:24:32.364850:  
2025-07-12 10:24:32.366140: Epoch 930 
2025-07-12 10:24:32.367176: Current learning rate: 0.00091 
2025-07-12 10:25:40.524411: train_loss -0.9832 
2025-07-12 10:25:40.525698: val_loss -0.9525 
2025-07-12 10:25:40.526547: Pseudo dice [np.float32(0.9553)] 
2025-07-12 10:25:40.527512: Epoch time: 68.16 s 
2025-07-12 10:25:41.402233:  
2025-07-12 10:25:41.403805: Epoch 931 
2025-07-12 10:25:41.404833: Current learning rate: 0.0009 
2025-07-12 10:26:49.331393: train_loss -0.9835 
2025-07-12 10:26:49.332494: val_loss -0.9508 
2025-07-12 10:26:49.333487: Pseudo dice [np.float32(0.9537)] 
2025-07-12 10:26:49.334391: Epoch time: 67.93 s 
2025-07-12 10:26:50.205271:  
2025-07-12 10:26:50.206659: Epoch 932 
2025-07-12 10:26:50.207676: Current learning rate: 0.00089 
2025-07-12 10:27:58.127901: train_loss -0.9829 
2025-07-12 10:27:58.129071: val_loss -0.9518 
2025-07-12 10:27:58.129974: Pseudo dice [np.float32(0.9553)] 
2025-07-12 10:27:58.131093: Epoch time: 67.93 s 
2025-07-12 10:27:59.011616:  
2025-07-12 10:27:59.012974: Epoch 933 
2025-07-12 10:27:59.014125: Current learning rate: 0.00088 
2025-07-12 10:29:07.163317: train_loss -0.9835 
2025-07-12 10:29:07.164841: val_loss -0.9493 
2025-07-12 10:29:07.165780: Pseudo dice [np.float32(0.9529)] 
2025-07-12 10:29:07.166825: Epoch time: 68.16 s 
2025-07-12 10:29:08.042179:  
2025-07-12 10:29:08.043678: Epoch 934 
2025-07-12 10:29:08.044651: Current learning rate: 0.00087 
2025-07-12 10:30:16.082521: train_loss -0.983 
2025-07-12 10:30:16.083655: val_loss -0.954 
2025-07-12 10:30:16.084799: Pseudo dice [np.float32(0.9567)] 
2025-07-12 10:30:16.085791: Epoch time: 68.04 s 
2025-07-12 10:30:16.965305:  
2025-07-12 10:30:16.966542: Epoch 935 
2025-07-12 10:30:16.967530: Current learning rate: 0.00085 
2025-07-12 10:31:24.830562: train_loss -0.9831 
2025-07-12 10:31:24.831940: val_loss -0.9525 
2025-07-12 10:31:24.832814: Pseudo dice [np.float32(0.9555)] 
2025-07-12 10:31:24.833732: Epoch time: 67.87 s 
2025-07-12 10:31:25.698707:  
2025-07-12 10:31:25.700466: Epoch 936 
2025-07-12 10:31:25.701572: Current learning rate: 0.00084 
2025-07-12 10:32:33.500343: train_loss -0.9832 
2025-07-12 10:32:33.501629: val_loss -0.9502 
2025-07-12 10:32:33.502767: Pseudo dice [np.float32(0.9529)] 
2025-07-12 10:32:33.503711: Epoch time: 67.81 s 
2025-07-12 10:32:34.376141:  
2025-07-12 10:32:34.377737: Epoch 937 
2025-07-12 10:32:34.378717: Current learning rate: 0.00083 
2025-07-12 10:33:42.375504: train_loss -0.9839 
2025-07-12 10:33:42.376603: val_loss -0.9525 
2025-07-12 10:33:42.377687: Pseudo dice [np.float32(0.9558)] 
2025-07-12 10:33:42.378564: Epoch time: 68.0 s 
2025-07-12 10:33:43.245073:  
2025-07-12 10:33:43.246310: Epoch 938 
2025-07-12 10:33:43.247375: Current learning rate: 0.00082 
2025-07-12 10:34:51.189069: train_loss -0.983 
2025-07-12 10:34:51.190201: val_loss -0.951 
2025-07-12 10:34:51.191076: Pseudo dice [np.float32(0.9543)] 
2025-07-12 10:34:51.191959: Epoch time: 67.95 s 
2025-07-12 10:34:52.065341:  
2025-07-12 10:34:52.066898: Epoch 939 
2025-07-12 10:34:52.067938: Current learning rate: 0.00081 
2025-07-12 10:35:59.988107: train_loss -0.984 
2025-07-12 10:35:59.989251: val_loss -0.9517 
2025-07-12 10:35:59.990325: Pseudo dice [np.float32(0.9549)] 
2025-07-12 10:35:59.991175: Epoch time: 67.93 s 
2025-07-12 10:36:00.865910:  
2025-07-12 10:36:00.867486: Epoch 940 
2025-07-12 10:36:00.868702: Current learning rate: 0.00079 
2025-07-12 10:37:08.796587: train_loss -0.9835 
2025-07-12 10:37:08.797685: val_loss -0.9504 
2025-07-12 10:37:08.798598: Pseudo dice [np.float32(0.9535)] 
2025-07-12 10:37:08.799499: Epoch time: 67.93 s 
2025-07-12 10:37:09.671373:  
2025-07-12 10:37:09.672678: Epoch 941 
2025-07-12 10:37:09.673626: Current learning rate: 0.00078 
2025-07-12 10:38:17.632192: train_loss -0.983 
2025-07-12 10:38:17.633296: val_loss -0.9554 
2025-07-12 10:38:17.634434: Pseudo dice [np.float32(0.9579)] 
2025-07-12 10:38:17.635695: Epoch time: 67.96 s 
2025-07-12 10:38:18.504194:  
2025-07-12 10:38:18.505520: Epoch 942 
2025-07-12 10:38:18.506491: Current learning rate: 0.00077 
2025-07-12 10:39:26.261353: train_loss -0.9841 
2025-07-12 10:39:26.262542: val_loss -0.9537 
2025-07-12 10:39:26.263437: Pseudo dice [np.float32(0.9563)] 
2025-07-12 10:39:26.264328: Epoch time: 67.76 s 
2025-07-12 10:39:27.134698:  
2025-07-12 10:39:27.136198: Epoch 943 
2025-07-12 10:39:27.137181: Current learning rate: 0.00076 
2025-07-12 10:40:34.847772: train_loss -0.9834 
2025-07-12 10:40:34.848892: val_loss -0.9541 
2025-07-12 10:40:34.849886: Pseudo dice [np.float32(0.9578)] 
2025-07-12 10:40:34.850993: Epoch time: 67.72 s 
2025-07-12 10:40:35.729542:  
2025-07-12 10:40:35.730963: Epoch 944 
2025-07-12 10:40:35.731973: Current learning rate: 0.00075 
2025-07-12 10:41:43.528692: train_loss -0.9838 
2025-07-12 10:41:43.529943: val_loss -0.9538 
2025-07-12 10:41:43.531081: Pseudo dice [np.float32(0.9564)] 
2025-07-12 10:41:43.532002: Epoch time: 67.8 s 
2025-07-12 10:41:44.385117:  
2025-07-12 10:41:44.386668: Epoch 945 
2025-07-12 10:41:44.387699: Current learning rate: 0.00074 
2025-07-12 10:42:52.137030: train_loss -0.9838 
2025-07-12 10:42:52.138109: val_loss -0.9533 
2025-07-12 10:42:52.138991: Pseudo dice [np.float32(0.9565)] 
2025-07-12 10:42:52.139982: Epoch time: 67.76 s 
2025-07-12 10:42:53.006489:  
2025-07-12 10:42:53.008041: Epoch 946 
2025-07-12 10:42:53.009008: Current learning rate: 0.00072 
2025-07-12 10:44:00.705232: train_loss -0.984 
2025-07-12 10:44:00.706456: val_loss -0.9523 
2025-07-12 10:44:00.707626: Pseudo dice [np.float32(0.956)] 
2025-07-12 10:44:00.708607: Epoch time: 67.7 s 
2025-07-12 10:44:01.587127:  
2025-07-12 10:44:01.588660: Epoch 947 
2025-07-12 10:44:01.589571: Current learning rate: 0.00071 
2025-07-12 10:45:09.364574: train_loss -0.9843 
2025-07-12 10:45:09.365829: val_loss -0.953 
2025-07-12 10:45:09.366832: Pseudo dice [np.float32(0.9558)] 
2025-07-12 10:45:09.367876: Epoch time: 67.78 s 
2025-07-12 10:45:10.255455:  
2025-07-12 10:45:10.257013: Epoch 948 
2025-07-12 10:45:10.258080: Current learning rate: 0.0007 
2025-07-12 10:46:17.999564: train_loss -0.9841 
2025-07-12 10:46:18.000818: val_loss -0.9539 
2025-07-12 10:46:18.001738: Pseudo dice [np.float32(0.957)] 
2025-07-12 10:46:18.002714: Epoch time: 67.75 s 
2025-07-12 10:46:18.867369:  
2025-07-12 10:46:18.868683: Epoch 949 
2025-07-12 10:46:18.869771: Current learning rate: 0.00069 
2025-07-12 10:47:26.583405: train_loss -0.9836 
2025-07-12 10:47:26.584970: val_loss -0.9533 
2025-07-12 10:47:26.585918: Pseudo dice [np.float32(0.9562)] 
2025-07-12 10:47:26.586794: Epoch time: 67.72 s 
2025-07-12 10:47:28.689840:  
2025-07-12 10:47:28.691134: Epoch 950 
2025-07-12 10:47:28.692260: Current learning rate: 0.00067 
2025-07-12 10:48:36.312095: train_loss -0.9838 
2025-07-12 10:48:36.313227: val_loss -0.9508 
2025-07-12 10:48:36.314122: Pseudo dice [np.float32(0.9533)] 
2025-07-12 10:48:36.315699: Epoch time: 67.63 s 
2025-07-12 10:48:37.190953:  
2025-07-12 10:48:37.192416: Epoch 951 
2025-07-12 10:48:37.193417: Current learning rate: 0.00066 
2025-07-12 10:49:45.098509: train_loss -0.9843 
2025-07-12 10:49:45.099592: val_loss -0.9524 
2025-07-12 10:49:45.100595: Pseudo dice [np.float32(0.9553)] 
2025-07-12 10:49:45.101534: Epoch time: 67.91 s 
2025-07-12 10:49:45.976600:  
2025-07-12 10:49:45.977998: Epoch 952 
2025-07-12 10:49:45.979141: Current learning rate: 0.00065 
2025-07-12 10:50:53.751443: train_loss -0.9836 
2025-07-12 10:50:53.752540: val_loss -0.9526 
2025-07-12 10:50:53.753499: Pseudo dice [np.float32(0.9552)] 
2025-07-12 10:50:53.754379: Epoch time: 67.78 s 
2025-07-12 10:50:54.625204:  
2025-07-12 10:50:54.627104: Epoch 953 
2025-07-12 10:50:54.628138: Current learning rate: 0.00064 
2025-07-12 10:52:02.532851: train_loss -0.9837 
2025-07-12 10:52:02.534008: val_loss -0.9532 
2025-07-12 10:52:02.535208: Pseudo dice [np.float32(0.9561)] 
2025-07-12 10:52:02.536228: Epoch time: 67.91 s 
2025-07-12 10:52:03.412314:  
2025-07-12 10:52:03.414149: Epoch 954 
2025-07-12 10:52:03.415210: Current learning rate: 0.00063 
2025-07-12 10:53:11.228111: train_loss -0.9844 
2025-07-12 10:53:11.229339: val_loss -0.9539 
2025-07-12 10:53:11.230260: Pseudo dice [np.float32(0.9576)] 
2025-07-12 10:53:11.231183: Epoch time: 67.82 s 
2025-07-12 10:53:12.102888:  
2025-07-12 10:53:12.104363: Epoch 955 
2025-07-12 10:53:12.105339: Current learning rate: 0.00061 
2025-07-12 10:54:19.988000: train_loss -0.9836 
2025-07-12 10:54:19.989314: val_loss -0.9536 
2025-07-12 10:54:19.990206: Pseudo dice [np.float32(0.9566)] 
2025-07-12 10:54:19.991212: Epoch time: 67.89 s 
2025-07-12 10:54:20.876470:  
2025-07-12 10:54:20.878154: Epoch 956 
2025-07-12 10:54:20.879123: Current learning rate: 0.0006 
2025-07-12 10:55:28.706839: train_loss -0.9843 
2025-07-12 10:55:28.708194: val_loss -0.9542 
2025-07-12 10:55:28.709087: Pseudo dice [np.float32(0.9575)] 
2025-07-12 10:55:28.710098: Epoch time: 67.83 s 
2025-07-12 10:55:29.590831:  
2025-07-12 10:55:29.592188: Epoch 957 
2025-07-12 10:55:29.593134: Current learning rate: 0.00059 
2025-07-12 10:56:37.390401: train_loss -0.984 
2025-07-12 10:56:37.391535: val_loss -0.9518 
2025-07-12 10:56:37.392497: Pseudo dice [np.float32(0.9545)] 
2025-07-12 10:56:37.393663: Epoch time: 67.8 s 
2025-07-12 10:56:38.274979:  
2025-07-12 10:56:38.276729: Epoch 958 
2025-07-12 10:56:38.277781: Current learning rate: 0.00058 
2025-07-12 10:57:46.481018: train_loss -0.9833 
2025-07-12 10:57:46.482329: val_loss -0.9511 
2025-07-12 10:57:46.483208: Pseudo dice [np.float32(0.9541)] 
2025-07-12 10:57:46.484093: Epoch time: 68.21 s 
2025-07-12 10:57:47.360348:  
2025-07-12 10:57:47.362041: Epoch 959 
2025-07-12 10:57:47.363070: Current learning rate: 0.00056 
2025-07-12 10:58:55.268077: train_loss -0.984 
2025-07-12 10:58:55.269202: val_loss -0.9527 
2025-07-12 10:58:55.270328: Pseudo dice [np.float32(0.9559)] 
2025-07-12 10:58:55.271752: Epoch time: 67.91 s 
2025-07-12 10:58:56.156231:  
2025-07-12 10:58:56.157920: Epoch 960 
2025-07-12 10:58:56.158925: Current learning rate: 0.00055 
2025-07-12 11:00:04.196999: train_loss -0.9839 
2025-07-12 11:00:04.198185: val_loss -0.9531 
2025-07-12 11:00:04.199067: Pseudo dice [np.float32(0.9559)] 
2025-07-12 11:00:04.200085: Epoch time: 68.04 s 
2025-07-12 11:00:05.079243:  
2025-07-12 11:00:05.081254: Epoch 961 
2025-07-12 11:00:05.082677: Current learning rate: 0.00054 
2025-07-12 11:01:13.029831: train_loss -0.984 
2025-07-12 11:01:13.031029: val_loss -0.9497 
2025-07-12 11:01:13.031939: Pseudo dice [np.float32(0.9532)] 
2025-07-12 11:01:13.032830: Epoch time: 67.95 s 
2025-07-12 11:01:13.923528:  
2025-07-12 11:01:13.925141: Epoch 962 
2025-07-12 11:01:13.926138: Current learning rate: 0.00053 
2025-07-12 11:02:21.870054: train_loss -0.9839 
2025-07-12 11:02:21.871214: val_loss -0.9539 
2025-07-12 11:02:21.872179: Pseudo dice [np.float32(0.9567)] 
2025-07-12 11:02:21.873191: Epoch time: 67.95 s 
2025-07-12 11:02:22.765637:  
2025-07-12 11:02:22.767191: Epoch 963 
2025-07-12 11:02:22.768223: Current learning rate: 0.00051 
2025-07-12 11:03:30.610140: train_loss -0.9841 
2025-07-12 11:03:30.611262: val_loss -0.9539 
2025-07-12 11:03:30.612160: Pseudo dice [np.float32(0.9563)] 
2025-07-12 11:03:30.613124: Epoch time: 67.85 s 
2025-07-12 11:03:31.471619:  
2025-07-12 11:03:31.473114: Epoch 964 
2025-07-12 11:03:31.474008: Current learning rate: 0.0005 
2025-07-12 11:04:39.276946: train_loss -0.9841 
2025-07-12 11:04:39.278777: val_loss -0.954 
2025-07-12 11:04:39.279701: Pseudo dice [np.float32(0.9572)] 
2025-07-12 11:04:39.280639: Epoch time: 67.81 s 
2025-07-12 11:04:40.156802:  
2025-07-12 11:04:40.158294: Epoch 965 
2025-07-12 11:04:40.159422: Current learning rate: 0.00049 
2025-07-12 11:05:48.028969: train_loss -0.9838 
2025-07-12 11:05:48.030168: val_loss -0.9517 
2025-07-12 11:05:48.031625: Pseudo dice [np.float32(0.9547)] 
2025-07-12 11:05:48.032597: Epoch time: 67.88 s 
2025-07-12 11:05:48.916787:  
2025-07-12 11:05:48.918679: Epoch 966 
2025-07-12 11:05:48.919820: Current learning rate: 0.00048 
2025-07-12 11:06:56.664404: train_loss -0.9835 
2025-07-12 11:06:56.666350: val_loss -0.9512 
2025-07-12 11:06:56.667715: Pseudo dice [np.float32(0.9539)] 
2025-07-12 11:06:56.668960: Epoch time: 67.75 s 
2025-07-12 11:06:57.554498:  
2025-07-12 11:06:57.556035: Epoch 967 
2025-07-12 11:06:57.557209: Current learning rate: 0.00046 
2025-07-12 11:08:05.366987: train_loss -0.9839 
2025-07-12 11:08:05.368187: val_loss -0.9547 
2025-07-12 11:08:05.369094: Pseudo dice [np.float32(0.9578)] 
2025-07-12 11:08:05.370196: Epoch time: 67.82 s 
2025-07-12 11:08:06.269887:  
2025-07-12 11:08:06.271714: Epoch 968 
2025-07-12 11:08:06.272716: Current learning rate: 0.00045 
2025-07-12 11:09:14.190996: train_loss -0.9842 
2025-07-12 11:09:14.192146: val_loss -0.9518 
2025-07-12 11:09:14.193094: Pseudo dice [np.float32(0.955)] 
2025-07-12 11:09:14.194333: Epoch time: 67.92 s 
2025-07-12 11:09:15.076441:  
2025-07-12 11:09:15.078055: Epoch 969 
2025-07-12 11:09:15.079115: Current learning rate: 0.00044 
2025-07-12 11:10:23.376716: train_loss -0.9841 
2025-07-12 11:10:23.377898: val_loss -0.954 
2025-07-12 11:10:23.379334: Pseudo dice [np.float32(0.9567)] 
2025-07-12 11:10:23.380286: Epoch time: 68.3 s 
2025-07-12 11:10:24.248996:  
2025-07-12 11:10:24.250492: Epoch 970 
2025-07-12 11:10:24.251634: Current learning rate: 0.00043 
2025-07-12 11:11:32.170005: train_loss -0.9837 
2025-07-12 11:11:32.171280: val_loss -0.95 
2025-07-12 11:11:32.172347: Pseudo dice [np.float32(0.9526)] 
2025-07-12 11:11:32.173243: Epoch time: 67.92 s 
2025-07-12 11:11:33.056723:  
2025-07-12 11:11:33.058655: Epoch 971 
2025-07-12 11:11:33.060201: Current learning rate: 0.00041 
2025-07-12 11:12:41.005355: train_loss -0.9839 
2025-07-12 11:12:41.006572: val_loss -0.9529 
2025-07-12 11:12:41.007543: Pseudo dice [np.float32(0.9566)] 
2025-07-12 11:12:41.008835: Epoch time: 67.95 s 
2025-07-12 11:12:41.894884:  
2025-07-12 11:12:41.896772: Epoch 972 
2025-07-12 11:12:41.897831: Current learning rate: 0.0004 
2025-07-12 11:13:49.993796: train_loss -0.9838 
2025-07-12 11:13:49.995124: val_loss -0.9539 
2025-07-12 11:13:49.995992: Pseudo dice [np.float32(0.9574)] 
2025-07-12 11:13:49.996902: Epoch time: 68.1 s 
2025-07-12 11:13:50.872828:  
2025-07-12 11:13:50.874991: Epoch 973 
2025-07-12 11:13:50.876101: Current learning rate: 0.00039 
2025-07-12 11:14:58.842680: train_loss -0.9843 
2025-07-12 11:14:58.844095: val_loss -0.9539 
2025-07-12 11:14:58.845298: Pseudo dice [np.float32(0.9568)] 
2025-07-12 11:14:58.846340: Epoch time: 67.97 s 
2025-07-12 11:14:59.732774:  
2025-07-12 11:14:59.734542: Epoch 974 
2025-07-12 11:14:59.735709: Current learning rate: 0.00037 
2025-07-12 11:16:07.765013: train_loss -0.9845 
2025-07-12 11:16:07.766275: val_loss -0.9502 
2025-07-12 11:16:07.767380: Pseudo dice [np.float32(0.954)] 
2025-07-12 11:16:07.768817: Epoch time: 68.04 s 
2025-07-12 11:16:08.631594:  
2025-07-12 11:16:08.633044: Epoch 975 
2025-07-12 11:16:08.634007: Current learning rate: 0.00036 
2025-07-12 11:17:16.716953: train_loss -0.9842 
2025-07-12 11:17:16.718182: val_loss -0.9538 
2025-07-12 11:17:16.719351: Pseudo dice [np.float32(0.9569)] 
2025-07-12 11:17:16.720506: Epoch time: 68.09 s 
2025-07-12 11:17:17.605002:  
2025-07-12 11:17:17.606480: Epoch 976 
2025-07-12 11:17:17.607744: Current learning rate: 0.00035 
2025-07-12 11:18:25.797287: train_loss -0.9837 
2025-07-12 11:18:25.798402: val_loss -0.9516 
2025-07-12 11:18:25.799371: Pseudo dice [np.float32(0.9546)] 
2025-07-12 11:18:25.800239: Epoch time: 68.2 s 
2025-07-12 11:18:26.693270:  
2025-07-12 11:18:26.694982: Epoch 977 
2025-07-12 11:18:26.696256: Current learning rate: 0.00034 
2025-07-12 11:19:34.816251: train_loss -0.9842 
2025-07-12 11:19:34.817871: val_loss -0.9532 
2025-07-12 11:19:34.818920: Pseudo dice [np.float32(0.9563)] 
2025-07-12 11:19:34.819874: Epoch time: 68.13 s 
2025-07-12 11:19:35.697493:  
2025-07-12 11:19:35.699382: Epoch 978 
2025-07-12 11:19:35.700588: Current learning rate: 0.00032 
2025-07-12 11:20:44.020274: train_loss -0.9842 
2025-07-12 11:20:44.021495: val_loss -0.952 
2025-07-12 11:20:44.023291: Pseudo dice [np.float32(0.955)] 
2025-07-12 11:20:44.024181: Epoch time: 68.33 s 
2025-07-12 11:20:44.918809:  
2025-07-12 11:20:44.920516: Epoch 979 
2025-07-12 11:20:44.921702: Current learning rate: 0.00031 
2025-07-12 11:21:53.357241: train_loss -0.9839 
2025-07-12 11:21:53.358287: val_loss -0.9541 
2025-07-12 11:21:53.359398: Pseudo dice [np.float32(0.957)] 
2025-07-12 11:21:53.360744: Epoch time: 68.44 s 
2025-07-12 11:21:54.235147:  
2025-07-12 11:21:54.237116: Epoch 980 
2025-07-12 11:21:54.238399: Current learning rate: 0.0003 
2025-07-12 11:23:02.527671: train_loss -0.9837 
2025-07-12 11:23:02.528794: val_loss -0.9514 
2025-07-12 11:23:02.529815: Pseudo dice [np.float32(0.9549)] 
2025-07-12 11:23:02.530744: Epoch time: 68.3 s 
2025-07-12 11:23:03.426012:  
2025-07-12 11:23:03.427906: Epoch 981 
2025-07-12 11:23:03.429159: Current learning rate: 0.00028 
2025-07-12 11:24:11.742013: train_loss -0.9843 
2025-07-12 11:24:11.743151: val_loss -0.9534 
2025-07-12 11:24:11.744060: Pseudo dice [np.float32(0.9564)] 
2025-07-12 11:24:11.744942: Epoch time: 68.32 s 
2025-07-12 11:24:12.640894:  
2025-07-12 11:24:12.642797: Epoch 982 
2025-07-12 11:24:12.643946: Current learning rate: 0.00027 
2025-07-12 11:25:21.070921: train_loss -0.9844 
2025-07-12 11:25:21.072047: val_loss -0.9536 
2025-07-12 11:25:21.073128: Pseudo dice [np.float32(0.9569)] 
2025-07-12 11:25:21.074115: Epoch time: 68.43 s 
2025-07-12 11:25:21.961018:  
2025-07-12 11:25:21.962685: Epoch 983 
2025-07-12 11:25:21.963742: Current learning rate: 0.00026 
2025-07-12 11:26:30.259271: train_loss -0.984 
2025-07-12 11:26:30.260674: val_loss -0.9506 
2025-07-12 11:26:30.261629: Pseudo dice [np.float32(0.9544)] 
2025-07-12 11:26:30.262541: Epoch time: 68.3 s 
2025-07-12 11:26:31.159464:  
2025-07-12 11:26:31.161125: Epoch 984 
2025-07-12 11:26:31.162305: Current learning rate: 0.00024 
2025-07-12 11:27:39.159408: train_loss -0.9838 
2025-07-12 11:27:39.160752: val_loss -0.9544 
2025-07-12 11:27:39.161963: Pseudo dice [np.float32(0.9578)] 
2025-07-12 11:27:39.163093: Epoch time: 68.0 s 
2025-07-12 11:27:40.050431:  
2025-07-12 11:27:40.052118: Epoch 985 
2025-07-12 11:27:40.053127: Current learning rate: 0.00023 
2025-07-12 11:28:48.193694: train_loss -0.9845 
2025-07-12 11:28:48.194866: val_loss -0.9517 
2025-07-12 11:28:48.195768: Pseudo dice [np.float32(0.9551)] 
2025-07-12 11:28:48.196740: Epoch time: 68.15 s 
2025-07-12 11:28:49.094678:  
2025-07-12 11:28:49.096659: Epoch 986 
2025-07-12 11:28:49.097865: Current learning rate: 0.00021 
2025-07-12 11:29:57.289518: train_loss -0.9838 
2025-07-12 11:29:57.290629: val_loss -0.9548 
2025-07-12 11:29:57.291504: Pseudo dice [np.float32(0.958)] 
2025-07-12 11:29:57.292649: Epoch time: 68.2 s 
2025-07-12 11:29:58.172964:  
2025-07-12 11:29:58.174764: Epoch 987 
2025-07-12 11:29:58.176051: Current learning rate: 0.0002 
2025-07-12 11:31:06.409636: train_loss -0.9846 
2025-07-12 11:31:06.410772: val_loss -0.9543 
2025-07-12 11:31:06.411792: Pseudo dice [np.float32(0.957)] 
2025-07-12 11:31:06.412791: Epoch time: 68.24 s 
2025-07-12 11:31:07.300934:  
2025-07-12 11:31:07.302196: Epoch 988 
2025-07-12 11:31:07.303170: Current learning rate: 0.00019 
2025-07-12 11:32:15.348473: train_loss -0.9848 
2025-07-12 11:32:15.349826: val_loss -0.952 
2025-07-12 11:32:15.350951: Pseudo dice [np.float32(0.9549)] 
2025-07-12 11:32:15.352012: Epoch time: 68.05 s 
2025-07-12 11:32:16.240478:  
2025-07-12 11:32:16.242707: Epoch 989 
2025-07-12 11:32:16.243757: Current learning rate: 0.00017 
2025-07-12 11:33:24.190694: train_loss -0.9846 
2025-07-12 11:33:24.191849: val_loss -0.9505 
2025-07-12 11:33:24.192795: Pseudo dice [np.float32(0.9537)] 
2025-07-12 11:33:24.193712: Epoch time: 67.95 s 
2025-07-12 11:33:25.082728:  
2025-07-12 11:33:25.084376: Epoch 990 
2025-07-12 11:33:25.085465: Current learning rate: 0.00016 
2025-07-12 11:34:33.359621: train_loss -0.9851 
2025-07-12 11:34:33.360696: val_loss -0.9558 
2025-07-12 11:34:33.361641: Pseudo dice [np.float32(0.9587)] 
2025-07-12 11:34:33.362641: Epoch time: 68.28 s 
2025-07-12 11:34:34.246680:  
2025-07-12 11:34:34.248162: Epoch 991 
2025-07-12 11:34:34.249205: Current learning rate: 0.00014 
2025-07-12 11:35:42.391545: train_loss -0.9838 
2025-07-12 11:35:42.392663: val_loss -0.9551 
2025-07-12 11:35:42.393731: Pseudo dice [np.float32(0.9578)] 
2025-07-12 11:35:42.394883: Epoch time: 68.15 s 
2025-07-12 11:35:43.280414:  
2025-07-12 11:35:43.282118: Epoch 992 
2025-07-12 11:35:43.283218: Current learning rate: 0.00013 
2025-07-12 11:36:51.412010: train_loss -0.9843 
2025-07-12 11:36:51.413436: val_loss -0.954 
2025-07-12 11:36:51.414399: Pseudo dice [np.float32(0.9567)] 
2025-07-12 11:36:51.415296: Epoch time: 68.14 s 
2025-07-12 11:36:52.303580:  
2025-07-12 11:36:52.305125: Epoch 993 
2025-07-12 11:36:52.306143: Current learning rate: 0.00011 
2025-07-12 11:38:00.502906: train_loss -0.9842 
2025-07-12 11:38:00.504313: val_loss -0.9506 
2025-07-12 11:38:00.505471: Pseudo dice [np.float32(0.9534)] 
2025-07-12 11:38:00.506428: Epoch time: 68.2 s 
2025-07-12 11:38:01.393181:  
2025-07-12 11:38:01.394728: Epoch 994 
2025-07-12 11:38:01.395819: Current learning rate: 0.0001 
2025-07-12 11:39:09.480965: train_loss -0.9842 
2025-07-12 11:39:09.482166: val_loss -0.9512 
2025-07-12 11:39:09.483487: Pseudo dice [np.float32(0.9542)] 
2025-07-12 11:39:09.484433: Epoch time: 68.09 s 
2025-07-12 11:39:10.363782:  
2025-07-12 11:39:10.365535: Epoch 995 
2025-07-12 11:39:10.366613: Current learning rate: 8e-05 
2025-07-12 11:40:18.495100: train_loss -0.9843 
2025-07-12 11:40:18.496239: val_loss -0.9555 
2025-07-12 11:40:18.497272: Pseudo dice [np.float32(0.9578)] 
2025-07-12 11:40:18.498177: Epoch time: 68.13 s 
2025-07-12 11:40:19.389452:  
2025-07-12 11:40:19.391088: Epoch 996 
2025-07-12 11:40:19.392151: Current learning rate: 7e-05 
2025-07-12 11:41:27.467467: train_loss -0.9841 
2025-07-12 11:41:27.468588: val_loss -0.9486 
2025-07-12 11:41:27.469542: Pseudo dice [np.float32(0.9522)] 
2025-07-12 11:41:27.470479: Epoch time: 68.08 s 
2025-07-12 11:41:28.360011:  
2025-07-12 11:41:28.361333: Epoch 997 
2025-07-12 11:41:28.362357: Current learning rate: 5e-05 
2025-07-12 11:42:36.710867: train_loss -0.9842 
2025-07-12 11:42:36.712118: val_loss -0.9539 
2025-07-12 11:42:36.713106: Pseudo dice [np.float32(0.9568)] 
2025-07-12 11:42:36.714118: Epoch time: 68.35 s 
2025-07-12 11:42:37.612276:  
2025-07-12 11:42:37.613796: Epoch 998 
2025-07-12 11:42:37.614839: Current learning rate: 4e-05 
2025-07-12 11:43:45.627753: train_loss -0.9842 
2025-07-12 11:43:45.628812: val_loss -0.9518 
2025-07-12 11:43:45.629698: Pseudo dice [np.float32(0.9546)] 
2025-07-12 11:43:45.630692: Epoch time: 68.02 s 
2025-07-12 11:43:46.499205:  
2025-07-12 11:43:46.500888: Epoch 999 
2025-07-12 11:43:46.501983: Current learning rate: 2e-05 
2025-07-12 11:44:54.495229: train_loss -0.984 
2025-07-12 11:44:54.496488: val_loss -0.9546 
2025-07-12 11:44:54.497445: Pseudo dice [np.float32(0.9579)] 
2025-07-12 11:44:54.498369: Epoch time: 68.0 s 
2025-07-12 11:44:56.517637: Training done. 
2025-07-12 11:44:56.566949: Using splits from existing split file: /data/nnUNet_preprocessed/Dataset010_PTB_all_energies_1mm_no_background_alldata/splits_final.json 
2025-07-12 11:44:56.573236: The split file contains 5 splits. 
2025-07-12 11:44:56.574213: Desired fold for training: 1 
2025-07-12 11:44:56.575139: This split has 1440 training and 360 validation cases. 
2025-07-12 11:44:56.581172: predicting PTB_all_energies_1mm_no_background_0007 
2025-07-12 11:44:56.588268: PTB_all_energies_1mm_no_background_0007, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.178397: predicting PTB_all_energies_1mm_no_background_0008 
2025-07-12 11:45:05.185409: PTB_all_energies_1mm_no_background_0008, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.235288: predicting PTB_all_energies_1mm_no_background_0011 
2025-07-12 11:45:05.241611: PTB_all_energies_1mm_no_background_0011, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.288956: predicting PTB_all_energies_1mm_no_background_0016 
2025-07-12 11:45:05.295583: PTB_all_energies_1mm_no_background_0016, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.343663: predicting PTB_all_energies_1mm_no_background_0020 
2025-07-12 11:45:05.350745: PTB_all_energies_1mm_no_background_0020, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.400907: predicting PTB_all_energies_1mm_no_background_0025 
2025-07-12 11:45:05.408194: PTB_all_energies_1mm_no_background_0025, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.462322: predicting PTB_all_energies_1mm_no_background_0026 
2025-07-12 11:45:05.468741: PTB_all_energies_1mm_no_background_0026, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.520892: predicting PTB_all_energies_1mm_no_background_0030 
2025-07-12 11:45:05.527967: PTB_all_energies_1mm_no_background_0030, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.575328: predicting PTB_all_energies_1mm_no_background_0033 
2025-07-12 11:45:05.581997: PTB_all_energies_1mm_no_background_0033, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.631191: predicting PTB_all_energies_1mm_no_background_0041 
2025-07-12 11:45:05.637591: PTB_all_energies_1mm_no_background_0041, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.685046: predicting PTB_all_energies_1mm_no_background_0051 
2025-07-12 11:45:05.691664: PTB_all_energies_1mm_no_background_0051, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.738666: predicting PTB_all_energies_1mm_no_background_0053 
2025-07-12 11:45:05.745043: PTB_all_energies_1mm_no_background_0053, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.792231: predicting PTB_all_energies_1mm_no_background_0056 
2025-07-12 11:45:05.798485: PTB_all_energies_1mm_no_background_0056, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.846014: predicting PTB_all_energies_1mm_no_background_0062 
2025-07-12 11:45:05.851983: PTB_all_energies_1mm_no_background_0062, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.904281: predicting PTB_all_energies_1mm_no_background_0065 
2025-07-12 11:45:05.911076: PTB_all_energies_1mm_no_background_0065, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:05.961919: predicting PTB_all_energies_1mm_no_background_0066 
2025-07-12 11:45:05.968490: PTB_all_energies_1mm_no_background_0066, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.015460: predicting PTB_all_energies_1mm_no_background_0068 
2025-07-12 11:45:06.022006: PTB_all_energies_1mm_no_background_0068, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.072239: predicting PTB_all_energies_1mm_no_background_0069 
2025-07-12 11:45:06.078335: PTB_all_energies_1mm_no_background_0069, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.125389: predicting PTB_all_energies_1mm_no_background_0083 
2025-07-12 11:45:06.131619: PTB_all_energies_1mm_no_background_0083, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.178313: predicting PTB_all_energies_1mm_no_background_0088 
2025-07-12 11:45:06.184557: PTB_all_energies_1mm_no_background_0088, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.230914: predicting PTB_all_energies_1mm_no_background_0091 
2025-07-12 11:45:06.236786: PTB_all_energies_1mm_no_background_0091, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.283980: predicting PTB_all_energies_1mm_no_background_0098 
2025-07-12 11:45:06.290846: PTB_all_energies_1mm_no_background_0098, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.339971: predicting PTB_all_energies_1mm_no_background_0099 
2025-07-12 11:45:06.346912: PTB_all_energies_1mm_no_background_0099, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.397438: predicting PTB_all_energies_1mm_no_background_0100 
2025-07-12 11:45:06.403916: PTB_all_energies_1mm_no_background_0100, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.451920: predicting PTB_all_energies_1mm_no_background_0115 
2025-07-12 11:45:06.458113: PTB_all_energies_1mm_no_background_0115, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.508260: predicting PTB_all_energies_1mm_no_background_0119 
2025-07-12 11:45:06.515333: PTB_all_energies_1mm_no_background_0119, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.562234: predicting PTB_all_energies_1mm_no_background_0124 
2025-07-12 11:45:06.568560: PTB_all_energies_1mm_no_background_0124, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.615808: predicting PTB_all_energies_1mm_no_background_0128 
2025-07-12 11:45:06.622789: PTB_all_energies_1mm_no_background_0128, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.669792: predicting PTB_all_energies_1mm_no_background_0129 
2025-07-12 11:45:06.675718: PTB_all_energies_1mm_no_background_0129, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.721726: predicting PTB_all_energies_1mm_no_background_0139 
2025-07-12 11:45:06.728268: PTB_all_energies_1mm_no_background_0139, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.777683: predicting PTB_all_energies_1mm_no_background_0144 
2025-07-12 11:45:06.784080: PTB_all_energies_1mm_no_background_0144, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.835817: predicting PTB_all_energies_1mm_no_background_0145 
2025-07-12 11:45:06.842515: PTB_all_energies_1mm_no_background_0145, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.889728: predicting PTB_all_energies_1mm_no_background_0153 
2025-07-12 11:45:06.896234: PTB_all_energies_1mm_no_background_0153, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.946013: predicting PTB_all_energies_1mm_no_background_0168 
2025-07-12 11:45:06.952458: PTB_all_energies_1mm_no_background_0168, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:06.999878: predicting PTB_all_energies_1mm_no_background_0174 
2025-07-12 11:45:07.006416: PTB_all_energies_1mm_no_background_0174, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.053361: predicting PTB_all_energies_1mm_no_background_0176 
2025-07-12 11:45:07.059895: PTB_all_energies_1mm_no_background_0176, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.106328: predicting PTB_all_energies_1mm_no_background_0178 
2025-07-12 11:45:07.112340: PTB_all_energies_1mm_no_background_0178, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.158763: predicting PTB_all_energies_1mm_no_background_0180 
2025-07-12 11:45:07.165045: PTB_all_energies_1mm_no_background_0180, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.215008: predicting PTB_all_energies_1mm_no_background_0193 
2025-07-12 11:45:07.222987: PTB_all_energies_1mm_no_background_0193, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.272267: predicting PTB_all_energies_1mm_no_background_0199 
2025-07-12 11:45:07.278678: PTB_all_energies_1mm_no_background_0199, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.324998: predicting PTB_all_energies_1mm_no_background_0201 
2025-07-12 11:45:07.331222: PTB_all_energies_1mm_no_background_0201, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.381586: predicting PTB_all_energies_1mm_no_background_0216 
2025-07-12 11:45:07.387869: PTB_all_energies_1mm_no_background_0216, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.438717: predicting PTB_all_energies_1mm_no_background_0221 
2025-07-12 11:45:07.445312: PTB_all_energies_1mm_no_background_0221, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.492920: predicting PTB_all_energies_1mm_no_background_0222 
2025-07-12 11:45:07.499603: PTB_all_energies_1mm_no_background_0222, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.546918: predicting PTB_all_energies_1mm_no_background_0224 
2025-07-12 11:45:07.553121: PTB_all_energies_1mm_no_background_0224, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.605997: predicting PTB_all_energies_1mm_no_background_0235 
2025-07-12 11:45:07.612496: PTB_all_energies_1mm_no_background_0235, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.664995: predicting PTB_all_energies_1mm_no_background_0240 
2025-07-12 11:45:07.671660: PTB_all_energies_1mm_no_background_0240, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.731865: predicting PTB_all_energies_1mm_no_background_0244 
2025-07-12 11:45:07.738187: PTB_all_energies_1mm_no_background_0244, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.785955: predicting PTB_all_energies_1mm_no_background_0251 
2025-07-12 11:45:07.792525: PTB_all_energies_1mm_no_background_0251, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.847973: predicting PTB_all_energies_1mm_no_background_0254 
2025-07-12 11:45:07.854726: PTB_all_energies_1mm_no_background_0254, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.902988: predicting PTB_all_energies_1mm_no_background_0266 
2025-07-12 11:45:07.909332: PTB_all_energies_1mm_no_background_0266, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:07.960619: predicting PTB_all_energies_1mm_no_background_0278 
2025-07-12 11:45:07.967130: PTB_all_energies_1mm_no_background_0278, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.013922: predicting PTB_all_energies_1mm_no_background_0282 
2025-07-12 11:45:08.019892: PTB_all_energies_1mm_no_background_0282, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.065887: predicting PTB_all_energies_1mm_no_background_0304 
2025-07-12 11:45:08.071966: PTB_all_energies_1mm_no_background_0304, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.122630: predicting PTB_all_energies_1mm_no_background_0305 
2025-07-12 11:45:08.129519: PTB_all_energies_1mm_no_background_0305, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.184347: predicting PTB_all_energies_1mm_no_background_0309 
2025-07-12 11:45:08.190652: PTB_all_energies_1mm_no_background_0309, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.240170: predicting PTB_all_energies_1mm_no_background_0310 
2025-07-12 11:45:08.246603: PTB_all_energies_1mm_no_background_0310, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.296694: predicting PTB_all_energies_1mm_no_background_0318 
2025-07-12 11:45:08.303036: PTB_all_energies_1mm_no_background_0318, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.349896: predicting PTB_all_energies_1mm_no_background_0331 
2025-07-12 11:45:08.356517: PTB_all_energies_1mm_no_background_0331, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.403517: predicting PTB_all_energies_1mm_no_background_0332 
2025-07-12 11:45:08.409516: PTB_all_energies_1mm_no_background_0332, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.456155: predicting PTB_all_energies_1mm_no_background_0336 
2025-07-12 11:45:08.462835: PTB_all_energies_1mm_no_background_0336, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.510222: predicting PTB_all_energies_1mm_no_background_0340 
2025-07-12 11:45:08.516375: PTB_all_energies_1mm_no_background_0340, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.567422: predicting PTB_all_energies_1mm_no_background_0361 
2025-07-12 11:45:08.573940: PTB_all_energies_1mm_no_background_0361, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.623102: predicting PTB_all_energies_1mm_no_background_0364 
2025-07-12 11:45:08.629454: PTB_all_energies_1mm_no_background_0364, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.676130: predicting PTB_all_energies_1mm_no_background_0370 
2025-07-12 11:45:08.682477: PTB_all_energies_1mm_no_background_0370, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.732598: predicting PTB_all_energies_1mm_no_background_0371 
2025-07-12 11:45:08.738670: PTB_all_energies_1mm_no_background_0371, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.785890: predicting PTB_all_energies_1mm_no_background_0376 
2025-07-12 11:45:08.792787: PTB_all_energies_1mm_no_background_0376, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.839061: predicting PTB_all_energies_1mm_no_background_0384 
2025-07-12 11:45:08.845795: PTB_all_energies_1mm_no_background_0384, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.893497: predicting PTB_all_energies_1mm_no_background_0385 
2025-07-12 11:45:08.899554: PTB_all_energies_1mm_no_background_0385, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:08.946995: predicting PTB_all_energies_1mm_no_background_0389 
2025-07-12 11:45:08.954046: PTB_all_energies_1mm_no_background_0389, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.005573: predicting PTB_all_energies_1mm_no_background_0400 
2025-07-12 11:45:09.011914: PTB_all_energies_1mm_no_background_0400, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.062630: predicting PTB_all_energies_1mm_no_background_0401 
2025-07-12 11:45:09.068838: PTB_all_energies_1mm_no_background_0401, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.116953: predicting PTB_all_energies_1mm_no_background_0408 
2025-07-12 11:45:09.123296: PTB_all_energies_1mm_no_background_0408, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.173205: predicting PTB_all_energies_1mm_no_background_0412 
2025-07-12 11:45:09.179624: PTB_all_energies_1mm_no_background_0412, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.225680: predicting PTB_all_energies_1mm_no_background_0415 
2025-07-12 11:45:09.232342: PTB_all_energies_1mm_no_background_0415, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.278698: predicting PTB_all_energies_1mm_no_background_0420 
2025-07-12 11:45:09.284856: PTB_all_energies_1mm_no_background_0420, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.331660: predicting PTB_all_energies_1mm_no_background_0421 
2025-07-12 11:45:09.338376: PTB_all_energies_1mm_no_background_0421, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.384794: predicting PTB_all_energies_1mm_no_background_0423 
2025-07-12 11:45:09.391841: PTB_all_energies_1mm_no_background_0423, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.441539: predicting PTB_all_energies_1mm_no_background_0429 
2025-07-12 11:45:09.447740: PTB_all_energies_1mm_no_background_0429, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.498559: predicting PTB_all_energies_1mm_no_background_0437 
2025-07-12 11:45:09.504653: PTB_all_energies_1mm_no_background_0437, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.551952: predicting PTB_all_energies_1mm_no_background_0447 
2025-07-12 11:45:09.559207: PTB_all_energies_1mm_no_background_0447, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.608734: predicting PTB_all_energies_1mm_no_background_0448 
2025-07-12 11:45:09.615188: PTB_all_energies_1mm_no_background_0448, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.662513: predicting PTB_all_energies_1mm_no_background_0449 
2025-07-12 11:45:09.668448: PTB_all_energies_1mm_no_background_0449, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.715567: predicting PTB_all_energies_1mm_no_background_0450 
2025-07-12 11:45:09.721616: PTB_all_energies_1mm_no_background_0450, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.768813: predicting PTB_all_energies_1mm_no_background_0456 
2025-07-12 11:45:09.775285: PTB_all_energies_1mm_no_background_0456, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.821430: predicting PTB_all_energies_1mm_no_background_0463 
2025-07-12 11:45:09.828403: PTB_all_energies_1mm_no_background_0463, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.875204: predicting PTB_all_energies_1mm_no_background_0464 
2025-07-12 11:45:09.881471: PTB_all_energies_1mm_no_background_0464, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.929743: predicting PTB_all_energies_1mm_no_background_0466 
2025-07-12 11:45:09.936145: PTB_all_energies_1mm_no_background_0466, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:09.982462: predicting PTB_all_energies_1mm_no_background_0468 
2025-07-12 11:45:09.989821: PTB_all_energies_1mm_no_background_0468, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.037786: predicting PTB_all_energies_1mm_no_background_0472 
2025-07-12 11:45:10.044411: PTB_all_energies_1mm_no_background_0472, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.092918: predicting PTB_all_energies_1mm_no_background_0486 
2025-07-12 11:45:10.099012: PTB_all_energies_1mm_no_background_0486, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.145561: predicting PTB_all_energies_1mm_no_background_0490 
2025-07-12 11:45:10.151746: PTB_all_energies_1mm_no_background_0490, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.197676: predicting PTB_all_energies_1mm_no_background_0494 
2025-07-12 11:45:10.204044: PTB_all_energies_1mm_no_background_0494, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.250757: predicting PTB_all_energies_1mm_no_background_0500 
2025-07-12 11:45:10.257731: PTB_all_energies_1mm_no_background_0500, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.304641: predicting PTB_all_energies_1mm_no_background_0509 
2025-07-12 11:45:10.311683: PTB_all_energies_1mm_no_background_0509, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.360203: predicting PTB_all_energies_1mm_no_background_0511 
2025-07-12 11:45:10.366617: PTB_all_energies_1mm_no_background_0511, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.413043: predicting PTB_all_energies_1mm_no_background_0512 
2025-07-12 11:45:10.419295: PTB_all_energies_1mm_no_background_0512, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.467880: predicting PTB_all_energies_1mm_no_background_0522 
2025-07-12 11:45:10.474331: PTB_all_energies_1mm_no_background_0522, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.520530: predicting PTB_all_energies_1mm_no_background_0537 
2025-07-12 11:45:10.526766: PTB_all_energies_1mm_no_background_0537, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.573575: predicting PTB_all_energies_1mm_no_background_0542 
2025-07-12 11:45:10.579632: PTB_all_energies_1mm_no_background_0542, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.626734: predicting PTB_all_energies_1mm_no_background_0546 
2025-07-12 11:45:10.634297: PTB_all_energies_1mm_no_background_0546, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.680823: predicting PTB_all_energies_1mm_no_background_0550 
2025-07-12 11:45:10.686975: PTB_all_energies_1mm_no_background_0550, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.733991: predicting PTB_all_energies_1mm_no_background_0551 
2025-07-12 11:45:10.741187: PTB_all_energies_1mm_no_background_0551, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.790971: predicting PTB_all_energies_1mm_no_background_0559 
2025-07-12 11:45:10.797192: PTB_all_energies_1mm_no_background_0559, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.844102: predicting PTB_all_energies_1mm_no_background_0581 
2025-07-12 11:45:10.850557: PTB_all_energies_1mm_no_background_0581, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.899985: predicting PTB_all_energies_1mm_no_background_0582 
2025-07-12 11:45:10.907035: PTB_all_energies_1mm_no_background_0582, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:10.953797: predicting PTB_all_energies_1mm_no_background_0585 
2025-07-12 11:45:10.959838: PTB_all_energies_1mm_no_background_0585, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.006802: predicting PTB_all_energies_1mm_no_background_0589 
2025-07-12 11:45:11.013033: PTB_all_energies_1mm_no_background_0589, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.059336: predicting PTB_all_energies_1mm_no_background_0591 
2025-07-12 11:45:11.065736: PTB_all_energies_1mm_no_background_0591, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.111903: predicting PTB_all_energies_1mm_no_background_0594 
2025-07-12 11:45:11.118162: PTB_all_energies_1mm_no_background_0594, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.165186: predicting PTB_all_energies_1mm_no_background_0597 
2025-07-12 11:45:11.172089: PTB_all_energies_1mm_no_background_0597, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.222288: predicting PTB_all_energies_1mm_no_background_0598 
2025-07-12 11:45:11.228398: PTB_all_energies_1mm_no_background_0598, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.274240: predicting PTB_all_energies_1mm_no_background_0600 
2025-07-12 11:45:11.281204: PTB_all_energies_1mm_no_background_0600, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.329878: predicting PTB_all_energies_1mm_no_background_0609 
2025-07-12 11:45:11.336087: PTB_all_energies_1mm_no_background_0609, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.383192: predicting PTB_all_energies_1mm_no_background_0613 
2025-07-12 11:45:11.389441: PTB_all_energies_1mm_no_background_0613, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.435441: predicting PTB_all_energies_1mm_no_background_0618 
2025-07-12 11:45:11.441724: PTB_all_energies_1mm_no_background_0618, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.487927: predicting PTB_all_energies_1mm_no_background_0619 
2025-07-12 11:45:11.494341: PTB_all_energies_1mm_no_background_0619, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.541500: predicting PTB_all_energies_1mm_no_background_0623 
2025-07-12 11:45:11.547662: PTB_all_energies_1mm_no_background_0623, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.594410: predicting PTB_all_energies_1mm_no_background_0635 
2025-07-12 11:45:11.600888: PTB_all_energies_1mm_no_background_0635, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.647741: predicting PTB_all_energies_1mm_no_background_0642 
2025-07-12 11:45:11.654064: PTB_all_energies_1mm_no_background_0642, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.700794: predicting PTB_all_energies_1mm_no_background_0648 
2025-07-12 11:45:11.706920: PTB_all_energies_1mm_no_background_0648, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.757991: predicting PTB_all_energies_1mm_no_background_0653 
2025-07-12 11:45:11.764005: PTB_all_energies_1mm_no_background_0653, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.809907: predicting PTB_all_energies_1mm_no_background_0654 
2025-07-12 11:45:11.816276: PTB_all_energies_1mm_no_background_0654, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.862306: predicting PTB_all_energies_1mm_no_background_0656 
2025-07-12 11:45:11.868835: PTB_all_energies_1mm_no_background_0656, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.916649: predicting PTB_all_energies_1mm_no_background_0663 
2025-07-12 11:45:11.923542: PTB_all_energies_1mm_no_background_0663, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:11.970470: predicting PTB_all_energies_1mm_no_background_0664 
2025-07-12 11:45:11.976781: PTB_all_energies_1mm_no_background_0664, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.023151: predicting PTB_all_energies_1mm_no_background_0665 
2025-07-12 11:45:12.029882: PTB_all_energies_1mm_no_background_0665, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.077100: predicting PTB_all_energies_1mm_no_background_0666 
2025-07-12 11:45:12.083168: PTB_all_energies_1mm_no_background_0666, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.130030: predicting PTB_all_energies_1mm_no_background_0667 
2025-07-12 11:45:12.136065: PTB_all_energies_1mm_no_background_0667, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.185232: predicting PTB_all_energies_1mm_no_background_0668 
2025-07-12 11:45:12.191645: PTB_all_energies_1mm_no_background_0668, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.237890: predicting PTB_all_energies_1mm_no_background_0675 
2025-07-12 11:45:12.244093: PTB_all_energies_1mm_no_background_0675, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.290907: predicting PTB_all_energies_1mm_no_background_0676 
2025-07-12 11:45:12.297316: PTB_all_energies_1mm_no_background_0676, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.343532: predicting PTB_all_energies_1mm_no_background_0681 
2025-07-12 11:45:12.349663: PTB_all_energies_1mm_no_background_0681, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.395619: predicting PTB_all_energies_1mm_no_background_0683 
2025-07-12 11:45:12.402720: PTB_all_energies_1mm_no_background_0683, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.451094: predicting PTB_all_energies_1mm_no_background_0687 
2025-07-12 11:45:12.457137: PTB_all_energies_1mm_no_background_0687, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.504632: predicting PTB_all_energies_1mm_no_background_0697 
2025-07-12 11:45:12.510847: PTB_all_energies_1mm_no_background_0697, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.557099: predicting PTB_all_energies_1mm_no_background_0699 
2025-07-12 11:45:12.564176: PTB_all_energies_1mm_no_background_0699, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.614154: predicting PTB_all_energies_1mm_no_background_0707 
2025-07-12 11:45:12.620362: PTB_all_energies_1mm_no_background_0707, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.667093: predicting PTB_all_energies_1mm_no_background_0709 
2025-07-12 11:45:12.673455: PTB_all_energies_1mm_no_background_0709, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.719964: predicting PTB_all_energies_1mm_no_background_0711 
2025-07-12 11:45:12.726813: PTB_all_energies_1mm_no_background_0711, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.773448: predicting PTB_all_energies_1mm_no_background_0722 
2025-07-12 11:45:12.779886: PTB_all_energies_1mm_no_background_0722, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.826337: predicting PTB_all_energies_1mm_no_background_0739 
2025-07-12 11:45:12.832743: PTB_all_energies_1mm_no_background_0739, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.879230: predicting PTB_all_energies_1mm_no_background_0741 
2025-07-12 11:45:12.885472: PTB_all_energies_1mm_no_background_0741, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.931161: predicting PTB_all_energies_1mm_no_background_0747 
2025-07-12 11:45:12.937467: PTB_all_energies_1mm_no_background_0747, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:12.984997: predicting PTB_all_energies_1mm_no_background_0752 
2025-07-12 11:45:12.991045: PTB_all_energies_1mm_no_background_0752, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.040914: predicting PTB_all_energies_1mm_no_background_0757 
2025-07-12 11:45:13.047474: PTB_all_energies_1mm_no_background_0757, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.094062: predicting PTB_all_energies_1mm_no_background_0758 
2025-07-12 11:45:13.100123: PTB_all_energies_1mm_no_background_0758, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.146958: predicting PTB_all_energies_1mm_no_background_0764 
2025-07-12 11:45:13.153551: PTB_all_energies_1mm_no_background_0764, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.200632: predicting PTB_all_energies_1mm_no_background_0766 
2025-07-12 11:45:13.207210: PTB_all_energies_1mm_no_background_0766, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.253098: predicting PTB_all_energies_1mm_no_background_0768 
2025-07-12 11:45:13.259417: PTB_all_energies_1mm_no_background_0768, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.306111: predicting PTB_all_energies_1mm_no_background_0776 
2025-07-12 11:45:13.311920: PTB_all_energies_1mm_no_background_0776, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.358858: predicting PTB_all_energies_1mm_no_background_0783 
2025-07-12 11:45:13.364793: PTB_all_energies_1mm_no_background_0783, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.410941: predicting PTB_all_energies_1mm_no_background_0788 
2025-07-12 11:45:13.417562: PTB_all_energies_1mm_no_background_0788, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.468224: predicting PTB_all_energies_1mm_no_background_0794 
2025-07-12 11:45:13.474766: PTB_all_energies_1mm_no_background_0794, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.522995: predicting PTB_all_energies_1mm_no_background_0807 
2025-07-12 11:45:13.529189: PTB_all_energies_1mm_no_background_0807, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.578414: predicting PTB_all_energies_1mm_no_background_0813 
2025-07-12 11:45:13.585014: PTB_all_energies_1mm_no_background_0813, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.633651: predicting PTB_all_energies_1mm_no_background_0816 
2025-07-12 11:45:13.639909: PTB_all_energies_1mm_no_background_0816, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.686059: predicting PTB_all_energies_1mm_no_background_0817 
2025-07-12 11:45:13.692574: PTB_all_energies_1mm_no_background_0817, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.739900: predicting PTB_all_energies_1mm_no_background_0824 
2025-07-12 11:45:13.746523: PTB_all_energies_1mm_no_background_0824, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.793786: predicting PTB_all_energies_1mm_no_background_0830 
2025-07-12 11:45:13.800065: PTB_all_energies_1mm_no_background_0830, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.846073: predicting PTB_all_energies_1mm_no_background_0837 
2025-07-12 11:45:13.852570: PTB_all_energies_1mm_no_background_0837, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.902895: predicting PTB_all_energies_1mm_no_background_0841 
2025-07-12 11:45:13.909176: PTB_all_energies_1mm_no_background_0841, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:13.956103: predicting PTB_all_energies_1mm_no_background_0842 
2025-07-12 11:45:13.962638: PTB_all_energies_1mm_no_background_0842, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.008972: predicting PTB_all_energies_1mm_no_background_0850 
2025-07-12 11:45:14.015581: PTB_all_energies_1mm_no_background_0850, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.062326: predicting PTB_all_energies_1mm_no_background_0859 
2025-07-12 11:45:14.068514: PTB_all_energies_1mm_no_background_0859, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.115358: predicting PTB_all_energies_1mm_no_background_0866 
2025-07-12 11:45:14.121770: PTB_all_energies_1mm_no_background_0866, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.168699: predicting PTB_all_energies_1mm_no_background_0871 
2025-07-12 11:45:14.175176: PTB_all_energies_1mm_no_background_0871, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.221961: predicting PTB_all_energies_1mm_no_background_0872 
2025-07-12 11:45:14.228571: PTB_all_energies_1mm_no_background_0872, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.275784: predicting PTB_all_energies_1mm_no_background_0875 
2025-07-12 11:45:14.282322: PTB_all_energies_1mm_no_background_0875, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.332894: predicting PTB_all_energies_1mm_no_background_0895 
2025-07-12 11:45:14.339089: PTB_all_energies_1mm_no_background_0895, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.385512: predicting PTB_all_energies_1mm_no_background_0898 
2025-07-12 11:45:14.391929: PTB_all_energies_1mm_no_background_0898, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.438726: predicting PTB_all_energies_1mm_no_background_0902 
2025-07-12 11:45:14.445193: PTB_all_energies_1mm_no_background_0902, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.492013: predicting PTB_all_energies_1mm_no_background_0906 
2025-07-12 11:45:14.498317: PTB_all_energies_1mm_no_background_0906, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.544616: predicting PTB_all_energies_1mm_no_background_0911 
2025-07-12 11:45:14.551489: PTB_all_energies_1mm_no_background_0911, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.598128: predicting PTB_all_energies_1mm_no_background_0915 
2025-07-12 11:45:14.604282: PTB_all_energies_1mm_no_background_0915, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.651557: predicting PTB_all_energies_1mm_no_background_0916 
2025-07-12 11:45:14.658182: PTB_all_energies_1mm_no_background_0916, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.705297: predicting PTB_all_energies_1mm_no_background_0918 
2025-07-12 11:45:14.711483: PTB_all_energies_1mm_no_background_0918, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.779015: predicting PTB_all_energies_1mm_no_background_0923 
2025-07-12 11:45:14.787369: PTB_all_energies_1mm_no_background_0923, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.834934: predicting PTB_all_energies_1mm_no_background_0927 
2025-07-12 11:45:14.841163: PTB_all_energies_1mm_no_background_0927, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.888130: predicting PTB_all_energies_1mm_no_background_0930 
2025-07-12 11:45:14.894374: PTB_all_energies_1mm_no_background_0930, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:14.941172: predicting PTB_all_energies_1mm_no_background_0941 
2025-07-12 11:45:14.947887: PTB_all_energies_1mm_no_background_0941, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.008801: predicting PTB_all_energies_1mm_no_background_0946 
2025-07-12 11:45:15.015195: PTB_all_energies_1mm_no_background_0946, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.062662: predicting PTB_all_energies_1mm_no_background_0948 
2025-07-12 11:45:15.068622: PTB_all_energies_1mm_no_background_0948, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.116041: predicting PTB_all_energies_1mm_no_background_0954 
2025-07-12 11:45:15.122451: PTB_all_energies_1mm_no_background_0954, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.170078: predicting PTB_all_energies_1mm_no_background_0956 
2025-07-12 11:45:15.176967: PTB_all_energies_1mm_no_background_0956, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.236371: predicting PTB_all_energies_1mm_no_background_0967 
2025-07-12 11:45:15.242554: PTB_all_energies_1mm_no_background_0967, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.290307: predicting PTB_all_energies_1mm_no_background_0968 
2025-07-12 11:45:15.296388: PTB_all_energies_1mm_no_background_0968, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.342892: predicting PTB_all_energies_1mm_no_background_0969 
2025-07-12 11:45:15.349427: PTB_all_energies_1mm_no_background_0969, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.396025: predicting PTB_all_energies_1mm_no_background_0971 
2025-07-12 11:45:15.402300: PTB_all_energies_1mm_no_background_0971, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.455721: predicting PTB_all_energies_1mm_no_background_0976 
2025-07-12 11:45:15.462705: PTB_all_energies_1mm_no_background_0976, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.510071: predicting PTB_all_energies_1mm_no_background_0980 
2025-07-12 11:45:15.516470: PTB_all_energies_1mm_no_background_0980, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.563794: predicting PTB_all_energies_1mm_no_background_0982 
2025-07-12 11:45:15.570443: PTB_all_energies_1mm_no_background_0982, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.617112: predicting PTB_all_energies_1mm_no_background_0983 
2025-07-12 11:45:15.623450: PTB_all_energies_1mm_no_background_0983, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.678540: predicting PTB_all_energies_1mm_no_background_0988 
2025-07-12 11:45:15.685791: PTB_all_energies_1mm_no_background_0988, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.757991: predicting PTB_all_energies_1mm_no_background_0996 
2025-07-12 11:45:15.765573: PTB_all_energies_1mm_no_background_0996, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.829827: predicting PTB_all_energies_1mm_no_background_0998 
2025-07-12 11:45:15.836360: PTB_all_energies_1mm_no_background_0998, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.887756: predicting PTB_all_energies_1mm_no_background_0999 
2025-07-12 11:45:15.894264: PTB_all_energies_1mm_no_background_0999, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.944753: predicting PTB_all_energies_1mm_no_background_1005 
2025-07-12 11:45:15.951095: PTB_all_energies_1mm_no_background_1005, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:15.998992: predicting PTB_all_energies_1mm_no_background_1009 
2025-07-12 11:45:16.005211: PTB_all_energies_1mm_no_background_1009, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.053749: predicting PTB_all_energies_1mm_no_background_1018 
2025-07-12 11:45:16.060304: PTB_all_energies_1mm_no_background_1018, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.113540: predicting PTB_all_energies_1mm_no_background_1019 
2025-07-12 11:45:16.120267: PTB_all_energies_1mm_no_background_1019, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.172809: predicting PTB_all_energies_1mm_no_background_1020 
2025-07-12 11:45:16.179691: PTB_all_energies_1mm_no_background_1020, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.226409: predicting PTB_all_energies_1mm_no_background_1025 
2025-07-12 11:45:16.233223: PTB_all_energies_1mm_no_background_1025, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.279874: predicting PTB_all_energies_1mm_no_background_1029 
2025-07-12 11:45:16.285894: PTB_all_energies_1mm_no_background_1029, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.332687: predicting PTB_all_energies_1mm_no_background_1033 
2025-07-12 11:45:16.338806: PTB_all_energies_1mm_no_background_1033, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.385837: predicting PTB_all_energies_1mm_no_background_1042 
2025-07-12 11:45:16.391872: PTB_all_energies_1mm_no_background_1042, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.438537: predicting PTB_all_energies_1mm_no_background_1053 
2025-07-12 11:45:16.445514: PTB_all_energies_1mm_no_background_1053, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.494667: predicting PTB_all_energies_1mm_no_background_1064 
2025-07-12 11:45:16.502071: PTB_all_energies_1mm_no_background_1064, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.549545: predicting PTB_all_energies_1mm_no_background_1068 
2025-07-12 11:45:16.556554: PTB_all_energies_1mm_no_background_1068, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.605861: predicting PTB_all_energies_1mm_no_background_1072 
2025-07-12 11:45:16.612205: PTB_all_energies_1mm_no_background_1072, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.659089: predicting PTB_all_energies_1mm_no_background_1074 
2025-07-12 11:45:16.665082: PTB_all_energies_1mm_no_background_1074, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.712159: predicting PTB_all_energies_1mm_no_background_1078 
2025-07-12 11:45:16.718145: PTB_all_energies_1mm_no_background_1078, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.764572: predicting PTB_all_energies_1mm_no_background_1079 
2025-07-12 11:45:16.771177: PTB_all_energies_1mm_no_background_1079, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.817229: predicting PTB_all_energies_1mm_no_background_1080 
2025-07-12 11:45:16.823922: PTB_all_energies_1mm_no_background_1080, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.872444: predicting PTB_all_energies_1mm_no_background_1082 
2025-07-12 11:45:16.878873: PTB_all_energies_1mm_no_background_1082, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.926415: predicting PTB_all_energies_1mm_no_background_1085 
2025-07-12 11:45:16.933314: PTB_all_energies_1mm_no_background_1085, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:16.979897: predicting PTB_all_energies_1mm_no_background_1086 
2025-07-12 11:45:16.986460: PTB_all_energies_1mm_no_background_1086, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.035542: predicting PTB_all_energies_1mm_no_background_1090 
2025-07-12 11:45:17.041998: PTB_all_energies_1mm_no_background_1090, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.089477: predicting PTB_all_energies_1mm_no_background_1092 
2025-07-12 11:45:17.095713: PTB_all_energies_1mm_no_background_1092, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.141855: predicting PTB_all_energies_1mm_no_background_1098 
2025-07-12 11:45:17.148031: PTB_all_energies_1mm_no_background_1098, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.193605: predicting PTB_all_energies_1mm_no_background_1104 
2025-07-12 11:45:17.199780: PTB_all_energies_1mm_no_background_1104, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.246864: predicting PTB_all_energies_1mm_no_background_1106 
2025-07-12 11:45:17.254129: PTB_all_energies_1mm_no_background_1106, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.301720: predicting PTB_all_energies_1mm_no_background_1107 
2025-07-12 11:45:17.308370: PTB_all_energies_1mm_no_background_1107, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.355584: predicting PTB_all_energies_1mm_no_background_1110 
2025-07-12 11:45:17.362727: PTB_all_energies_1mm_no_background_1110, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.410427: predicting PTB_all_energies_1mm_no_background_1112 
2025-07-12 11:45:17.416733: PTB_all_energies_1mm_no_background_1112, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.467302: predicting PTB_all_energies_1mm_no_background_1115 
2025-07-12 11:45:17.473894: PTB_all_energies_1mm_no_background_1115, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.520445: predicting PTB_all_energies_1mm_no_background_1118 
2025-07-12 11:45:17.527005: PTB_all_energies_1mm_no_background_1118, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.574685: predicting PTB_all_energies_1mm_no_background_1120 
2025-07-12 11:45:17.581033: PTB_all_energies_1mm_no_background_1120, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.629788: predicting PTB_all_energies_1mm_no_background_1124 
2025-07-12 11:45:17.636153: PTB_all_energies_1mm_no_background_1124, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.684688: predicting PTB_all_energies_1mm_no_background_1129 
2025-07-12 11:45:17.690995: PTB_all_energies_1mm_no_background_1129, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.738590: predicting PTB_all_energies_1mm_no_background_1136 
2025-07-12 11:45:17.745094: PTB_all_energies_1mm_no_background_1136, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.792226: predicting PTB_all_energies_1mm_no_background_1140 
2025-07-12 11:45:17.797976: PTB_all_energies_1mm_no_background_1140, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.845027: predicting PTB_all_energies_1mm_no_background_1148 
2025-07-12 11:45:17.851451: PTB_all_energies_1mm_no_background_1148, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.901022: predicting PTB_all_energies_1mm_no_background_1153 
2025-07-12 11:45:17.907886: PTB_all_energies_1mm_no_background_1153, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:17.954858: predicting PTB_all_energies_1mm_no_background_1155 
2025-07-12 11:45:17.961118: PTB_all_energies_1mm_no_background_1155, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.008207: predicting PTB_all_energies_1mm_no_background_1158 
2025-07-12 11:45:18.014281: PTB_all_energies_1mm_no_background_1158, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.060828: predicting PTB_all_energies_1mm_no_background_1160 
2025-07-12 11:45:18.066946: PTB_all_energies_1mm_no_background_1160, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.112918: predicting PTB_all_energies_1mm_no_background_1161 
2025-07-12 11:45:18.119390: PTB_all_energies_1mm_no_background_1161, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.166387: predicting PTB_all_energies_1mm_no_background_1163 
2025-07-12 11:45:18.172911: PTB_all_energies_1mm_no_background_1163, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.220417: predicting PTB_all_energies_1mm_no_background_1165 
2025-07-12 11:45:18.227125: PTB_all_energies_1mm_no_background_1165, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.273173: predicting PTB_all_energies_1mm_no_background_1169 
2025-07-12 11:45:18.279360: PTB_all_energies_1mm_no_background_1169, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.329335: predicting PTB_all_energies_1mm_no_background_1173 
2025-07-12 11:45:18.335681: PTB_all_energies_1mm_no_background_1173, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.382707: predicting PTB_all_energies_1mm_no_background_1179 
2025-07-12 11:45:18.389230: PTB_all_energies_1mm_no_background_1179, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.435484: predicting PTB_all_energies_1mm_no_background_1181 
2025-07-12 11:45:18.441608: PTB_all_energies_1mm_no_background_1181, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.487992: predicting PTB_all_energies_1mm_no_background_1183 
2025-07-12 11:45:18.494581: PTB_all_energies_1mm_no_background_1183, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.541287: predicting PTB_all_energies_1mm_no_background_1189 
2025-07-12 11:45:18.548181: PTB_all_energies_1mm_no_background_1189, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.595187: predicting PTB_all_energies_1mm_no_background_1200 
2025-07-12 11:45:18.601689: PTB_all_energies_1mm_no_background_1200, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.647909: predicting PTB_all_energies_1mm_no_background_1211 
2025-07-12 11:45:18.654428: PTB_all_energies_1mm_no_background_1211, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.701307: predicting PTB_all_energies_1mm_no_background_1213 
2025-07-12 11:45:18.707507: PTB_all_energies_1mm_no_background_1213, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.756826: predicting PTB_all_energies_1mm_no_background_1214 
2025-07-12 11:45:18.764389: PTB_all_energies_1mm_no_background_1214, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.810558: predicting PTB_all_energies_1mm_no_background_1217 
2025-07-12 11:45:18.817160: PTB_all_energies_1mm_no_background_1217, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.863679: predicting PTB_all_energies_1mm_no_background_1221 
2025-07-12 11:45:18.869974: PTB_all_energies_1mm_no_background_1221, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.916619: predicting PTB_all_energies_1mm_no_background_1227 
2025-07-12 11:45:18.923015: PTB_all_energies_1mm_no_background_1227, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:18.968883: predicting PTB_all_energies_1mm_no_background_1232 
2025-07-12 11:45:18.975959: PTB_all_energies_1mm_no_background_1232, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.024857: predicting PTB_all_energies_1mm_no_background_1235 
2025-07-12 11:45:19.032099: PTB_all_energies_1mm_no_background_1235, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.080366: predicting PTB_all_energies_1mm_no_background_1238 
2025-07-12 11:45:19.086748: PTB_all_energies_1mm_no_background_1238, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.133493: predicting PTB_all_energies_1mm_no_background_1241 
2025-07-12 11:45:19.139818: PTB_all_energies_1mm_no_background_1241, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.185955: predicting PTB_all_energies_1mm_no_background_1245 
2025-07-12 11:45:19.192833: PTB_all_energies_1mm_no_background_1245, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.240003: predicting PTB_all_energies_1mm_no_background_1248 
2025-07-12 11:45:19.246096: PTB_all_energies_1mm_no_background_1248, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.292871: predicting PTB_all_energies_1mm_no_background_1249 
2025-07-12 11:45:19.299528: PTB_all_energies_1mm_no_background_1249, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.345186: predicting PTB_all_energies_1mm_no_background_1250 
2025-07-12 11:45:19.351336: PTB_all_energies_1mm_no_background_1250, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.398123: predicting PTB_all_energies_1mm_no_background_1256 
2025-07-12 11:45:19.404511: PTB_all_energies_1mm_no_background_1256, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.456141: predicting PTB_all_energies_1mm_no_background_1258 
2025-07-12 11:45:19.462633: PTB_all_energies_1mm_no_background_1258, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.508976: predicting PTB_all_energies_1mm_no_background_1263 
2025-07-12 11:45:19.515170: PTB_all_energies_1mm_no_background_1263, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.562100: predicting PTB_all_energies_1mm_no_background_1269 
2025-07-12 11:45:19.568441: PTB_all_energies_1mm_no_background_1269, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.615751: predicting PTB_all_energies_1mm_no_background_1291 
2025-07-12 11:45:19.621763: PTB_all_energies_1mm_no_background_1291, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.670130: predicting PTB_all_energies_1mm_no_background_1300 
2025-07-12 11:45:19.676924: PTB_all_energies_1mm_no_background_1300, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.723416: predicting PTB_all_energies_1mm_no_background_1302 
2025-07-12 11:45:19.729491: PTB_all_energies_1mm_no_background_1302, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.775603: predicting PTB_all_energies_1mm_no_background_1303 
2025-07-12 11:45:19.781538: PTB_all_energies_1mm_no_background_1303, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.828199: predicting PTB_all_energies_1mm_no_background_1304 
2025-07-12 11:45:19.834954: PTB_all_energies_1mm_no_background_1304, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.885817: predicting PTB_all_energies_1mm_no_background_1305 
2025-07-12 11:45:19.892806: PTB_all_energies_1mm_no_background_1305, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.939725: predicting PTB_all_energies_1mm_no_background_1306 
2025-07-12 11:45:19.946598: PTB_all_energies_1mm_no_background_1306, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:19.994186: predicting PTB_all_energies_1mm_no_background_1316 
2025-07-12 11:45:20.000658: PTB_all_energies_1mm_no_background_1316, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.047881: predicting PTB_all_energies_1mm_no_background_1320 
2025-07-12 11:45:20.054255: PTB_all_energies_1mm_no_background_1320, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.100074: predicting PTB_all_energies_1mm_no_background_1326 
2025-07-12 11:45:20.106292: PTB_all_energies_1mm_no_background_1326, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.152977: predicting PTB_all_energies_1mm_no_background_1333 
2025-07-12 11:45:20.159440: PTB_all_energies_1mm_no_background_1333, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.206128: predicting PTB_all_energies_1mm_no_background_1335 
2025-07-12 11:45:20.212321: PTB_all_energies_1mm_no_background_1335, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.257862: predicting PTB_all_energies_1mm_no_background_1343 
2025-07-12 11:45:20.263997: PTB_all_energies_1mm_no_background_1343, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.313666: predicting PTB_all_energies_1mm_no_background_1345 
2025-07-12 11:45:20.319300: PTB_all_energies_1mm_no_background_1345, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.366483: predicting PTB_all_energies_1mm_no_background_1352 
2025-07-12 11:45:20.373126: PTB_all_energies_1mm_no_background_1352, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.419411: predicting PTB_all_energies_1mm_no_background_1358 
2025-07-12 11:45:20.426021: PTB_all_energies_1mm_no_background_1358, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.476642: predicting PTB_all_energies_1mm_no_background_1359 
2025-07-12 11:45:20.483066: PTB_all_energies_1mm_no_background_1359, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.530400: predicting PTB_all_energies_1mm_no_background_1361 
2025-07-12 11:45:20.536549: PTB_all_energies_1mm_no_background_1361, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.583091: predicting PTB_all_energies_1mm_no_background_1362 
2025-07-12 11:45:20.590329: PTB_all_energies_1mm_no_background_1362, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.636252: predicting PTB_all_energies_1mm_no_background_1366 
2025-07-12 11:45:20.642462: PTB_all_energies_1mm_no_background_1366, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.688918: predicting PTB_all_energies_1mm_no_background_1373 
2025-07-12 11:45:20.695299: PTB_all_energies_1mm_no_background_1373, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.743117: predicting PTB_all_energies_1mm_no_background_1378 
2025-07-12 11:45:20.749848: PTB_all_energies_1mm_no_background_1378, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.796437: predicting PTB_all_energies_1mm_no_background_1383 
2025-07-12 11:45:20.802655: PTB_all_energies_1mm_no_background_1383, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.849826: predicting PTB_all_energies_1mm_no_background_1386 
2025-07-12 11:45:20.855969: PTB_all_energies_1mm_no_background_1386, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.905986: predicting PTB_all_energies_1mm_no_background_1391 
2025-07-12 11:45:20.912197: PTB_all_energies_1mm_no_background_1391, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:20.959511: predicting PTB_all_energies_1mm_no_background_1392 
2025-07-12 11:45:20.965742: PTB_all_energies_1mm_no_background_1392, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.012058: predicting PTB_all_energies_1mm_no_background_1398 
2025-07-12 11:45:21.018069: PTB_all_energies_1mm_no_background_1398, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.065980: predicting PTB_all_energies_1mm_no_background_1403 
2025-07-12 11:45:21.072658: PTB_all_energies_1mm_no_background_1403, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.119538: predicting PTB_all_energies_1mm_no_background_1407 
2025-07-12 11:45:21.125940: PTB_all_energies_1mm_no_background_1407, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.173141: predicting PTB_all_energies_1mm_no_background_1410 
2025-07-12 11:45:21.179602: PTB_all_energies_1mm_no_background_1410, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.226686: predicting PTB_all_energies_1mm_no_background_1416 
2025-07-12 11:45:21.233702: PTB_all_energies_1mm_no_background_1416, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.282549: predicting PTB_all_energies_1mm_no_background_1438 
2025-07-12 11:45:21.289026: PTB_all_energies_1mm_no_background_1438, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.340312: predicting PTB_all_energies_1mm_no_background_1439 
2025-07-12 11:45:21.346888: PTB_all_energies_1mm_no_background_1439, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.393368: predicting PTB_all_energies_1mm_no_background_1451 
2025-07-12 11:45:21.399638: PTB_all_energies_1mm_no_background_1451, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.448445: predicting PTB_all_energies_1mm_no_background_1455 
2025-07-12 11:45:21.454275: PTB_all_energies_1mm_no_background_1455, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.500709: predicting PTB_all_energies_1mm_no_background_1461 
2025-07-12 11:45:21.506786: PTB_all_energies_1mm_no_background_1461, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.552972: predicting PTB_all_energies_1mm_no_background_1467 
2025-07-12 11:45:21.559485: PTB_all_energies_1mm_no_background_1467, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.606094: predicting PTB_all_energies_1mm_no_background_1489 
2025-07-12 11:45:21.612802: PTB_all_energies_1mm_no_background_1489, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.659682: predicting PTB_all_energies_1mm_no_background_1509 
2025-07-12 11:45:21.665789: PTB_all_energies_1mm_no_background_1509, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.712265: predicting PTB_all_energies_1mm_no_background_1511 
2025-07-12 11:45:21.718776: PTB_all_energies_1mm_no_background_1511, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.770648: predicting PTB_all_energies_1mm_no_background_1515 
2025-07-12 11:45:21.776929: PTB_all_energies_1mm_no_background_1515, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.824762: predicting PTB_all_energies_1mm_no_background_1518 
2025-07-12 11:45:21.830891: PTB_all_energies_1mm_no_background_1518, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.877659: predicting PTB_all_energies_1mm_no_background_1520 
2025-07-12 11:45:21.883740: PTB_all_energies_1mm_no_background_1520, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.932339: predicting PTB_all_energies_1mm_no_background_1541 
2025-07-12 11:45:21.938619: PTB_all_energies_1mm_no_background_1541, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:21.986725: predicting PTB_all_energies_1mm_no_background_1543 
2025-07-12 11:45:21.992780: PTB_all_energies_1mm_no_background_1543, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.042046: predicting PTB_all_energies_1mm_no_background_1567 
2025-07-12 11:45:22.048594: PTB_all_energies_1mm_no_background_1567, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.095594: predicting PTB_all_energies_1mm_no_background_1571 
2025-07-12 11:45:22.101887: PTB_all_energies_1mm_no_background_1571, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.148417: predicting PTB_all_energies_1mm_no_background_1576 
2025-07-12 11:45:22.155608: PTB_all_energies_1mm_no_background_1576, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.206357: predicting PTB_all_energies_1mm_no_background_1587 
2025-07-12 11:45:22.212516: PTB_all_energies_1mm_no_background_1587, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.260019: predicting PTB_all_energies_1mm_no_background_1590 
2025-07-12 11:45:22.266440: PTB_all_energies_1mm_no_background_1590, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.316086: predicting PTB_all_energies_1mm_no_background_1591 
2025-07-12 11:45:22.322038: PTB_all_energies_1mm_no_background_1591, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.370372: predicting PTB_all_energies_1mm_no_background_1592 
2025-07-12 11:45:22.376514: PTB_all_energies_1mm_no_background_1592, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.433843: predicting PTB_all_energies_1mm_no_background_1602 
2025-07-12 11:45:22.440521: PTB_all_energies_1mm_no_background_1602, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.491031: predicting PTB_all_energies_1mm_no_background_1604 
2025-07-12 11:45:22.497322: PTB_all_energies_1mm_no_background_1604, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.544771: predicting PTB_all_energies_1mm_no_background_1607 
2025-07-12 11:45:22.551199: PTB_all_energies_1mm_no_background_1607, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.598241: predicting PTB_all_energies_1mm_no_background_1609 
2025-07-12 11:45:22.604682: PTB_all_energies_1mm_no_background_1609, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.673214: predicting PTB_all_energies_1mm_no_background_1610 
2025-07-12 11:45:22.679625: PTB_all_energies_1mm_no_background_1610, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.732072: predicting PTB_all_energies_1mm_no_background_1615 
2025-07-12 11:45:22.738449: PTB_all_energies_1mm_no_background_1615, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.785470: predicting PTB_all_energies_1mm_no_background_1618 
2025-07-12 11:45:22.791659: PTB_all_energies_1mm_no_background_1618, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.838593: predicting PTB_all_energies_1mm_no_background_1620 
2025-07-12 11:45:22.844811: PTB_all_energies_1mm_no_background_1620, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.892729: predicting PTB_all_energies_1mm_no_background_1622 
2025-07-12 11:45:22.899207: PTB_all_energies_1mm_no_background_1622, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:22.948402: predicting PTB_all_energies_1mm_no_background_1623 
2025-07-12 11:45:22.954618: PTB_all_energies_1mm_no_background_1623, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.000900: predicting PTB_all_energies_1mm_no_background_1624 
2025-07-12 11:45:23.007496: PTB_all_energies_1mm_no_background_1624, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.054833: predicting PTB_all_energies_1mm_no_background_1627 
2025-07-12 11:45:23.061604: PTB_all_energies_1mm_no_background_1627, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.113269: predicting PTB_all_energies_1mm_no_background_1639 
2025-07-12 11:45:23.119633: PTB_all_energies_1mm_no_background_1639, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.167088: predicting PTB_all_energies_1mm_no_background_1643 
2025-07-12 11:45:23.173492: PTB_all_energies_1mm_no_background_1643, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.220228: predicting PTB_all_energies_1mm_no_background_1644 
2025-07-12 11:45:23.226129: PTB_all_energies_1mm_no_background_1644, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.273410: predicting PTB_all_energies_1mm_no_background_1646 
2025-07-12 11:45:23.279526: PTB_all_energies_1mm_no_background_1646, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.326275: predicting PTB_all_energies_1mm_no_background_1648 
2025-07-12 11:45:23.333408: PTB_all_energies_1mm_no_background_1648, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.381112: predicting PTB_all_energies_1mm_no_background_1661 
2025-07-12 11:45:23.387544: PTB_all_energies_1mm_no_background_1661, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.436027: predicting PTB_all_energies_1mm_no_background_1680 
2025-07-12 11:45:23.442504: PTB_all_energies_1mm_no_background_1680, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.491799: predicting PTB_all_energies_1mm_no_background_1681 
2025-07-12 11:45:23.498518: PTB_all_energies_1mm_no_background_1681, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.549425: predicting PTB_all_energies_1mm_no_background_1695 
2025-07-12 11:45:23.555876: PTB_all_energies_1mm_no_background_1695, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.604913: predicting PTB_all_energies_1mm_no_background_1700 
2025-07-12 11:45:23.611326: PTB_all_energies_1mm_no_background_1700, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.659502: predicting PTB_all_energies_1mm_no_background_1702 
2025-07-12 11:45:23.665468: PTB_all_energies_1mm_no_background_1702, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.713452: predicting PTB_all_energies_1mm_no_background_1709 
2025-07-12 11:45:23.719611: PTB_all_energies_1mm_no_background_1709, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.765384: predicting PTB_all_energies_1mm_no_background_1710 
2025-07-12 11:45:23.772229: PTB_all_energies_1mm_no_background_1710, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.819570: predicting PTB_all_energies_1mm_no_background_1713 
2025-07-12 11:45:23.825668: PTB_all_energies_1mm_no_background_1713, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.872566: predicting PTB_all_energies_1mm_no_background_1717 
2025-07-12 11:45:23.879348: PTB_all_energies_1mm_no_background_1717, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.925629: predicting PTB_all_energies_1mm_no_background_1719 
2025-07-12 11:45:23.932154: PTB_all_energies_1mm_no_background_1719, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:23.983154: predicting PTB_all_energies_1mm_no_background_1720 
2025-07-12 11:45:23.989673: PTB_all_energies_1mm_no_background_1720, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.038226: predicting PTB_all_energies_1mm_no_background_1721 
2025-07-12 11:45:24.044540: PTB_all_energies_1mm_no_background_1721, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.091653: predicting PTB_all_energies_1mm_no_background_1727 
2025-07-12 11:45:24.097887: PTB_all_energies_1mm_no_background_1727, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.144427: predicting PTB_all_energies_1mm_no_background_1744 
2025-07-12 11:45:24.150686: PTB_all_energies_1mm_no_background_1744, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.198619: predicting PTB_all_energies_1mm_no_background_1752 
2025-07-12 11:45:24.205135: PTB_all_energies_1mm_no_background_1752, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.253379: predicting PTB_all_energies_1mm_no_background_1753 
2025-07-12 11:45:24.259455: PTB_all_energies_1mm_no_background_1753, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.306181: predicting PTB_all_energies_1mm_no_background_1754 
2025-07-12 11:45:24.312238: PTB_all_energies_1mm_no_background_1754, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.359205: predicting PTB_all_energies_1mm_no_background_1761 
2025-07-12 11:45:24.365989: PTB_all_energies_1mm_no_background_1761, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.416919: predicting PTB_all_energies_1mm_no_background_1763 
2025-07-12 11:45:24.423752: PTB_all_energies_1mm_no_background_1763, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.471146: predicting PTB_all_energies_1mm_no_background_1765 
2025-07-12 11:45:24.477610: PTB_all_energies_1mm_no_background_1765, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.523813: predicting PTB_all_energies_1mm_no_background_1774 
2025-07-12 11:45:24.529642: PTB_all_energies_1mm_no_background_1774, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.577686: predicting PTB_all_energies_1mm_no_background_1785 
2025-07-12 11:45:24.583763: PTB_all_energies_1mm_no_background_1785, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.631029: predicting PTB_all_energies_1mm_no_background_1786 
2025-07-12 11:45:24.637377: PTB_all_energies_1mm_no_background_1786, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.684046: predicting PTB_all_energies_1mm_no_background_1791 
2025-07-12 11:45:24.690577: PTB_all_energies_1mm_no_background_1791, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:24.738214: predicting PTB_all_energies_1mm_no_background_1799 
2025-07-12 11:45:24.744754: PTB_all_energies_1mm_no_background_1799, shape torch.Size([1, 1, 504, 504]), rank 0 
2025-07-12 11:45:29.994203: Validation complete 
2025-07-12 11:45:29.995483: Mean Validation Dice:  0.9322686101444275 
